{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In the upper right corner, select from the dropdown menu 'Change Runtime Type', and select a GPU as available - this will allow you to run your neural network training on accelerated hardware and run everything faster."
      ],
      "metadata": {
        "id": "9YUH3WeLBcJh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fKUVG_yVxys",
        "outputId": "474ecf7c-9001-48bf-fd85-4b79b5760ef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This indicates that a GPU has been detected and can be used - the device is saved to 'device' so that we can direct data and models to access memory on that device."
      ],
      "metadata": {
        "id": "GOkdVdGsCPNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "train_X = torch.Tensor( trainset.data/255.0 - 0.5 )\n",
        "train_X = train_X.permute( 0, 3, 1, 2 )\n",
        "\n",
        "train_X = train_X.to( device ) # This line is different from the previous CIFAR code - it transfers the tensor to the GPU memory\n",
        "\n",
        "test_X = torch.Tensor( testset.data/255.0 - 0.5 )\n",
        "test_X = test_X.permute( 0, 3, 1, 2 )\n",
        "\n",
        "test_X = test_X.to( device ) # Again, transfering the tensor to GPU memory.\n",
        "\n",
        "train_Y = torch.Tensor( np.asarray( trainset.targets ) ).long()\n",
        "train_Y = train_Y.to( device )\n",
        "test_Y = torch.Tensor( np.asarray( testset.targets ) ).long()\n",
        "test_Y = test_Y.to( device )\n",
        "\n",
        "# All the data needs to be loaded into the GPU, as that is where the model processing will occur.\n",
        "\n",
        "def get_batch(x, y, batch_size):\n",
        "  n = x.shape[0]\n",
        "\n",
        "  batch_indices = random.sample( [ i for i in range(n) ], k = batch_size )\n",
        "\n",
        "  x_batch = x[ batch_indices ]\n",
        "  y_batch = y[ batch_indices ]\n",
        "\n",
        "  return x_batch, y_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnRW_4dyWG12",
        "outputId": "7f23810c-9631-4478-8723-09b5d82cead3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 28661553.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CIFARModel, self).__init__()\n",
        "\n",
        "    self.conv_layer_1 = nn.Conv2d(in_channels = 3, out_channels = 5, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.conv_layer_2 = nn.Conv2d(in_channels = 5, out_channels = 10, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.conv_layer_3 = nn.Conv2d(in_channels = 10, out_channels = 15, kernel_size = 3, stride = 1, bias=True)\n",
        "\n",
        "    self.linear_layer = torch.nn.Linear( in_features = 15*26*26, out_features = 10, bias=True )\n",
        "    # Note that the output of the last convolutional layer will be 15x26x16 - why?\n",
        "    # So we want to input 15*26*26 values into the last layer, and get 10 output values out (for the class probabilities)\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    output = self.conv_layer_1( input_tensor )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.conv_layer_2( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.conv_layer_3( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "\n",
        "    # At this point, the block of node values from the convolutional layer is flattened\n",
        "    # So that it can be passed into a standard linear layer\n",
        "    output = nn.Flatten()( output )\n",
        "    output = self.linear_layer( output )\n",
        "    return output"
      ],
      "metadata": {
        "id": "stRuThG-WK5a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix( model, x, y ):\n",
        "  identification_counts = np.zeros( shape = (10,10), dtype = np.int32 )\n",
        "\n",
        "  logits = model( x )\n",
        "  predicted_classes = torch.argmax( logits, dim = 1 )\n",
        "\n",
        "  n = x.shape[0]\n",
        "\n",
        "  for i in range(n):\n",
        "    actual_class = int( y[i].item() )\n",
        "    predicted_class = predicted_classes[i].item()\n",
        "    identification_counts[actual_class, predicted_class] += 1\n",
        "\n",
        "  return identification_counts"
      ],
      "metadata": {
        "id": "WHQ91OrnWYE0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_model = CIFARModel()\n",
        "\n",
        "cifar_model.to( device ) # The only change is that we also send the model to the GPU\n",
        "\n",
        "print( cifar_model )\n",
        "confusion_matrix( cifar_model, test_X, test_Y )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pHjBz3GWihM",
        "outputId": "6d5d6a04-6835-4317-ce08-b52ecc2969ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFARModel(\n",
            "  (conv_layer_1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv_layer_2): Conv2d(5, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv_layer_3): Conv2d(10, 15, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (linear_layer): Linear(in_features=10140, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0, 1000,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   0,    0, 1000,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   0,    0, 1000,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   0,    0, 1000,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   0,    0, 1000,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   0,    0, 1000,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   0,    0, 1000,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   0,    0, 1000,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   0,    0, 1000,    0,    0,    0,    0,    0,    0,    0],\n",
              "       [   0,    0, 1000,    0,    0,    0,    0,    0,    0,    0]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, everything else runs as before - just faster. I also increased the bach size, which I might could have done previously. Nevertheless - faster."
      ],
      "metadata": {
        "id": "GHMoVFMXC5vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_optimizer = optim.Adam(cifar_model.parameters(), lr = 0.01 )\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "print(\"Initial Test Loss:\", loss_function( cifar_model( test_X ), test_Y ).item() )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlptfBlvWlxk",
        "outputId": "bebcd9b3-8996-40a1-fd8e-593fd0dffa8f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Test Loss: 2.3417484760284424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "for epochs in range(100):\n",
        "  total_loss = 0\n",
        "  for batch in range( train_X.shape[0] // batch_size ):\n",
        "    x_batch, y_batch = get_batch(train_X, train_Y, batch_size)\n",
        "\n",
        "    cnn_optimizer.zero_grad()\n",
        "    logits = cifar_model( x_batch )\n",
        "    loss = loss_function( logits, y_batch )\n",
        "\n",
        "    loss.backward()\n",
        "    cnn_optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print( \"Average Total Loss over Batches:\", total_loss / ( train_X.shape[0] // batch_size ) )\n",
        "  print( confusion_matrix( cifar_model, test_X, test_Y ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uaShVxtDWq4W",
        "outputId": "b01eb0eb-b1f0-44f5-fbc9-1022347ba5fd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Total Loss over Batches: 2.236274775695801\n",
            "[[144  47 205  14  39  31  52  13 206 249]\n",
            " [  8 308  72  13  89  82 130  25  44 229]\n",
            " [ 17  47 225  21 101  72 431  16  16  54]\n",
            " [  4  32 167  48  98 150 411  28  11  51]\n",
            " [  5  11 109  14 101  49 617  22  17  55]\n",
            " [  8  55 143  29 127 190 369  31  10  38]\n",
            " [  1  19  63  24  57  31 760  20   2  23]\n",
            " [  2  81 109  23 124  40 307 120   6 188]\n",
            " [ 50  55 143  16  32  64  31  21 246 342]\n",
            " [  3  99  58  25  76  26 118  45  37 513]]\n",
            "Average Total Loss over Batches: 1.9371692210006715\n",
            "[[500  81  71  29   9   8  46  67 119  70]\n",
            " [ 38 547  55  23  21  34  47  67  65 103]\n",
            " [ 67  38 227  35  32  40 440  88  20  13]\n",
            " [ 28  39 148 102  16  80 450 106  15  16]\n",
            " [ 42  18 123  26  41  25 608  90  19   8]\n",
            " [ 23  59 175  68  35 105 419  94  13   9]\n",
            " [  2  16  61  32  15  17 769  81   5   2]\n",
            " [ 16  57  97  41  45  18 306 377  11  32]\n",
            " [294  88  64  31   3  30  18  59 345  68]\n",
            " [ 58 224  70  31  20  18  60 163  82 274]]\n",
            "Average Total Loss over Batches: 1.6826918815803529\n",
            "[[429  48 105  51  35  27  41  56 131  77]\n",
            " [ 11 580  21  31   8  21  80  42  46 160]\n",
            " [ 38  11 293  74 156  78 254  69  17  10]\n",
            " [ 11  12 127 191  73 129 358  86   5   8]\n",
            " [ 31   3 126  54 348  40 293  88   9   8]\n",
            " [ 10  12 147 143  87 231 273  91   5   1]\n",
            " [  2   4  62  74  91  20 687  54   1   5]\n",
            " [ 11  15  92  97 102  55 145 455   4  24]\n",
            " [182 109  62  39   8  12  47  29 403 109]\n",
            " [ 26 183  41  67   9  20  78 103  45 428]]\n",
            "Average Total Loss over Batches: 1.4798363474178313\n",
            "[[374  38  47  38  55  24  11  32 303  78]\n",
            " [ 14 658  10  18   7  19  15  17  82 160]\n",
            " [ 44  17 222  82 276 128  87  75  45  24]\n",
            " [ 15  20  66 285 114 266  95  94  21  24]\n",
            " [ 29   9  59  56 521 101 103  90  22  10]\n",
            " [ 12   9  63 155 110 516  33  79  16   7]\n",
            " [  5  15  40 107 152 108 501  40  12  20]\n",
            " [ 14  16  28  94 125 134  27 497  16  49]\n",
            " [ 91  74  13  19  20  15   8  16 685  59]\n",
            " [ 35 227  12  37  16  27  18  53  92 483]]\n",
            "Average Total Loss over Batches: 1.3155183734035492\n",
            "[[592  48  49  14  25  24  22  21 171  34]\n",
            " [ 33 712   5  15   1  20  15  22  77 100]\n",
            " [105  16 305  51 146 139 117  79  31  11]\n",
            " [ 33  20  69 228  67 312 135  95  22  19]\n",
            " [ 70   7  95  41 375 117 127 140  20   8]\n",
            " [ 26  14  80 103  41 576  49  93  10   8]\n",
            " [ 13  23  44  66  69 102 608  43  18  14]\n",
            " [ 30  13  37  47  67 143  35 584  11  33]\n",
            " [160  66  15  11  10  15  15  17 662  29]\n",
            " [ 66 247  18  27  14  24  23  52  99 430]]\n",
            "Average Total Loss over Batches: 1.191925664215088\n",
            "[[470  53  87  47  52   3  15  50 160  63]\n",
            " [ 16 742  10  16  10   2  15  19  45 125]\n",
            " [ 58   8 311 142 222  31 101  80  28  19]\n",
            " [ 14  15  74 474 124  64  88  98  16  33]\n",
            " [ 23   7  82  98 523  18  95 133  16   5]\n",
            " [  9  14  84 290  94 281  49 152  14  13]\n",
            " [  3  20  61 124 110  14 595  44  10  19]\n",
            " [  9  18  27  91 118  35  15 657   4  26]\n",
            " [105 102  12  31  17   1  10  23 652  47]\n",
            " [ 36 241  13  34  21   5  22  59  58 511]]\n",
            "Average Total Loss over Batches: 1.0967366614627838\n",
            "[[672  20  62  12  18   5  37  15 125  34]\n",
            " [ 43 682  15  10   5   3  38  15  84 105]\n",
            " [108  12 383  39 138  57 188  39  29   7]\n",
            " [ 52  12 125 235  84 153 246  45  22  26]\n",
            " [ 62   8 141  40 383  41 230  66  26   3]\n",
            " [ 32  11 131 131  62 387 156  68  17   5]\n",
            " [ 14  13  67  32  38  24 778  11  17   6]\n",
            " [ 53   8  77  51 105  67  88 514   7  30]\n",
            " [185  60  25  10  10   6  19  12 644  29]\n",
            " [ 86 189  23  12  10  12  60  31  97 480]]\n",
            "Average Total Loss over Batches: 1.0098959679222106\n",
            "[[639  16  28  39  22  21   9  19 139  68]\n",
            " [ 43 601   4  15   8   7  14  21  74 213]\n",
            " [132   6 300 129 139 110  83  56  28  17]\n",
            " [ 49   9  66 429  63 232  69  39  13  31]\n",
            " [ 73   5 110 128 400  81  87  88  23   5]\n",
            " [ 25   5  54 218  49 523  31  70  16   9]\n",
            " [ 15   9  63 143  80  85 546  28  16  15]\n",
            " [ 39   7  39  95  68 135  21 542  11  43]\n",
            " [163  47   4  26  11  14   8   7 675  45]\n",
            " [ 63 119   7  56   8  23  14  40  57 613]]\n",
            "Average Total Loss over Batches: 0.9392299672269822\n",
            "[[479  16 100  41  37   8  18  22 215  64]\n",
            " [ 31 664  13  15  12  10  27  15  90 123]\n",
            " [ 65   9 404  93 137  99  99  46  37  11]\n",
            " [ 30   8 129 340  74 203 119  45  23  29]\n",
            " [ 35   7 141  81 420  71 124  90  25   6]\n",
            " [ 17   9 110 183  51 468  65  65  21  11]\n",
            " [ 14   8  78  92  61  60 642  19  13  13]\n",
            " [ 18  10  76  78  92 107  30 539  10  40]\n",
            " [ 84  57  28  19  13  11  13   8 727  40]\n",
            " [ 52 170  18  33   9  20  26  34  89 549]]\n",
            "Average Total Loss over Batches: 0.8657716878986359\n",
            "[[640  25  48  31  18  19   8  21 110  80]\n",
            " [ 29 652   4  24   6   6   7  14  56 202]\n",
            " [125  14 322 148  97 120  56  75  22  21]\n",
            " [ 48  15  71 409  44 243  43  69  21  37]\n",
            " [ 59   9 110 141 352  98  58 132  28  13]\n",
            " [ 24  10  59 234  42 488  24  90  17  12]\n",
            " [ 17  24  70 180  61 105 459  39  16  29]\n",
            " [ 29   8  38 105  60 101   4 594   5  56]\n",
            " [167  77   9  21  14   9   2  10 628  63]\n",
            " [ 54 149  11  33   7  22   5  38  48 633]]\n",
            "Average Total Loss over Batches: 0.8123726144218445\n",
            "[[610  18  71  40  21  20  14  13 132  61]\n",
            " [ 37 641   7  23  10  14  15  14  67 172]\n",
            " [ 96   7 379 105 120 129  78  49  23  14]\n",
            " [ 43  13 107 343  60 270  71  54  14  25]\n",
            " [ 56   5 153  87 386 110  91  84  20   8]\n",
            " [ 16   4  88 164  46 542  43  73  10  14]\n",
            " [ 19  14  89 126  68  99 539  21  12  13]\n",
            " [ 31   7  74  74  96 151  11 519   5  32]\n",
            " [163  67  25  20  15  16   8   9 633  44]\n",
            " [ 60 149  19  39  11  28  12  40  67 575]]\n",
            "Average Total Loss over Batches: 0.7666696970343589\n",
            "[[544  33  90  25  39  10  27  25 152  55]\n",
            " [ 34 718   8   8  15   3  39  10  49 116]\n",
            " [ 69  16 352  65 179  66 156  60  25  12]\n",
            " [ 31  20  81 280 129 152 191  73  15  28]\n",
            " [ 34  10  90  53 479  45 164  93  22  10]\n",
            " [ 17  14 112 177 100 366  92  92  17  13]\n",
            " [ 11  15  67  55  62  37 720  11  12  10]\n",
            " [ 29  14  66  56 140  72  54 512   6  51]\n",
            " [144  83  17  11  28   8  26  12 632  39]\n",
            " [ 54 234  15  23  13  13  31  29  53 535]]\n",
            "Average Total Loss over Batches: 0.7191421049237251\n",
            "[[540  17  88  15  38  23  40  22 148  69]\n",
            " [ 42 618  18   8  15   7  54  12  58 168]\n",
            " [ 64   1 396  60 141  85 165  44  29  15]\n",
            " [ 29  10 126 212 100 215 210  63  15  20]\n",
            " [ 37   5 145  51 402  60 200  73  21   6]\n",
            " [ 12   9 124 120  72 451 116  77  11   8]\n",
            " [  9   8  81  39  51  43 738  14  10   7]\n",
            " [ 27   6  86  66 127 103  66 483   4  32]\n",
            " [133  53  29  12  26  14  29   8 634  62]\n",
            " [ 53 144  24  27  14  25  50  44  59 560]]\n",
            "Average Total Loss over Batches: 0.6756907133793831\n",
            "[[438  24  78  32  67  26  26  18 226  65]\n",
            " [ 27 624  13  13  15  11  26  17  77 177]\n",
            " [ 54  10 303  97 204 112 103  63  37  17]\n",
            " [ 25  17  76 300 123 238 112  66  21  22]\n",
            " [ 24  10 109  75 497  60 113  73  26  13]\n",
            " [ 12   6  77 163 101 470  60  82  18  11]\n",
            " [  8  16  62  84  91  63 607  28  20  21]\n",
            " [ 23  11  61  71 152 111  28 493  12  38]\n",
            " [ 96  59  12  20  34  18  23   8 687  43]\n",
            " [ 46 159  15  39  24  28  18  46  75 550]]\n",
            "Average Total Loss over Batches: 0.6356683392381668\n",
            "[[618  33  68  30  20  29  25  20 109  48]\n",
            " [ 47 675   8  14  13  10  26  16  61 130]\n",
            " [110  12 320  99  99 131 145  49  23  12]\n",
            " [ 44  18  79 314  60 243 139  61  16  26]\n",
            " [ 49  11 141  76 342 100 161  86  20  14]\n",
            " [ 19  12  84 179  47 491  76  73  11   8]\n",
            " [ 17  17  72  76  49  63 654  17  14  21]\n",
            " [ 43  16  68  86  89 127  39 483   7  42]\n",
            " [219  67  17  25  17  17  21   6 567  44]\n",
            " [ 73 205  19  31  14  25  24  33  59 517]]\n",
            "Average Total Loss over Batches: 0.6071665745067596\n",
            "[[455  31  69  27  40  17  11  32 240  78]\n",
            " [ 29 644   7   8  13   6  16  20  84 173]\n",
            " [ 58  12 324  85 188  86  90  81  60  16]\n",
            " [ 30  16  79 281 146 201  95  77  38  37]\n",
            " [ 30   8 119  67 479  60  98  91  38  10]\n",
            " [ 17  11  87 169  93 423  53 103  27  17]\n",
            " [  8  20  85  92 101  60 559  27  27  21]\n",
            " [ 32  13  62  74 149  81  24 511  20  34]\n",
            " [111  69   8  13  20   8   9  14 700  48]\n",
            " [ 42 168  15  25  24  19  12  52  83 560]]\n",
            "Average Total Loss over Batches: 0.5678109636068344\n",
            "[[529  37  81  41  37  14   9  27 164  61]\n",
            " [ 47 674   9  18  12   3  14  22  46 155]\n",
            " [ 66  11 344 140 140  94  83  84  27  11]\n",
            " [ 31  18  87 367 110 175  81  84  17  30]\n",
            " [ 30  19 123 109 445  66  81  98  20   9]\n",
            " [ 18  14  79 249  76 383  42 108  12  19]\n",
            " [ 16  19  81 135  98  59 512  43  16  21]\n",
            " [ 25   8  63  96 109  82  11 560   9  37]\n",
            " [154  62  21  27  26   9   9  10 632  50]\n",
            " [ 50 183  16  38  24  14  14  51  61 549]]\n",
            "Average Total Loss over Batches: 0.5405386604130268\n",
            "[[568  23 112  51  33  18  15  22 100  58]\n",
            " [ 56 602  15  24  11  12  27  24  56 173]\n",
            " [ 70   6 371 129 110 114  91  77  18  14]\n",
            " [ 31  11  90 365  77 226  80  83  13  24]\n",
            " [ 42  10 154 110 376  85  96 101  12  14]\n",
            " [ 16   6  96 231  54 444  47  91   8   7]\n",
            " [ 11  13  97 137  71  90 515  35  13  18]\n",
            " [ 23   7  79 100  90 121  22 520   6  32]\n",
            " [221  62  35  40  19  11  21   9 531  51]\n",
            " [ 68 146  25  58  17  33  20  62  47 524]]\n",
            "Average Total Loss over Batches: 0.522458398090601\n",
            "[[556  21  76  28  48  24  13  28 174  32]\n",
            " [ 59 656  13  13  15  13  26  19  82 104]\n",
            " [ 74   7 352 112 168  91  85  70  35   6]\n",
            " [ 38  12 102 304 115 209 114  71  21  14]\n",
            " [ 34   8 146  72 452  71  91  88  31   7]\n",
            " [ 16   7 109 189  77 418  59 100  19   6]\n",
            " [ 14  15 108  92  92  64 554  29  20  12]\n",
            " [ 34   8  79  85 123 101  26 511  13  20]\n",
            " [148  58  20  24  30  13  19  10 652  26]\n",
            " [ 76 195  34  43  25  22  23  63  91 428]]\n",
            "Average Total Loss over Batches: 0.4948185848760605\n",
            "[[487  26  72  54  45  33   9  27 155  92]\n",
            " [ 33 598  11  26  14  12  12  24  45 225]\n",
            " [ 61   9 296 128 149 157  59  90  27  24]\n",
            " [ 20  12  56 355  95 256  55  88  16  47]\n",
            " [ 24  10 107 116 398 122  56 122  20  25]\n",
            " [ 10   3  56 208  66 474  28 120  14  21]\n",
            " [  9  16  71 147  95 114 440  54  13  41]\n",
            " [ 20   9  38  92  87 139   9 546   7  53]\n",
            " [136  60  25  30  29  20  13  12 586  89]\n",
            " [ 40 128  14  44  20  31  11  45  37 630]]\n",
            "Average Total Loss over Batches: 0.46269778821408747\n",
            "[[464  36  67  49  53  29   8  27 170  97]\n",
            " [ 37 632  10  15  13   6   9  22  47 209]\n",
            " [ 66  18 304 152 153 114  56  81  34  22]\n",
            " [ 32  24  61 371 108 193  49  88  19  55]\n",
            " [ 32  18 105 111 454  67  42 112  25  34]\n",
            " [ 10   9  61 257  81 374  30 124  23  31]\n",
            " [ 19  28  87 131 112  86 418  52  16  51]\n",
            " [ 29  11  40  95 110 103  12 525   9  66]\n",
            " [128  70  16  28  19   9   7  12 627  84]\n",
            " [ 40 158  16  31  15  20   6  41  54 619]]\n",
            "Average Total Loss over Batches: 0.4497262230885029\n",
            "[[487  21  93  39  47  30  26  34 150  73]\n",
            " [ 45 534  15  25  17  15  40  30  47 232]\n",
            " [ 63   9 339 120 153 107  99  68  24  18]\n",
            " [ 24  13 100 291  90 216 124  89  14  39]\n",
            " [ 28   4 138  98 394  75 110 110  16  27]\n",
            " [  6   6  91 214  72 399  61 118  16  17]\n",
            " [  8  14  80  92  77  80 558  51  10  30]\n",
            " [ 19   7  69  88 119 101  25 520   7  45]\n",
            " [135  49  33  31  28  19  26  14 590  75]\n",
            " [ 50 110  23  34  21  28  32  61  58 583]]\n",
            "Average Total Loss over Batches: 0.4247857352387905\n",
            "[[542  37  70  21  35  17  10  20 193  55]\n",
            " [ 57 653  10  15  13   6   9  20  71 146]\n",
            " [ 89  14 350 117 127  91  57  89  46  20]\n",
            " [ 51  28  91 315 104 179  62 103  29  38]\n",
            " [ 48  16 138  89 403  55  60 133  31  27]\n",
            " [ 21  16  94 231  76 335  38 133  32  24]\n",
            " [ 25  27 117 124  86  70 433  58  24  36]\n",
            " [ 32  17  59  73 107  75  13 558  17  49]\n",
            " [149  59  17  18  19   6   6  11 667  48]\n",
            " [ 59 198  20  37  11  17   6  54  87 511]]\n",
            "Average Total Loss over Batches: 0.4147654309351742\n",
            "[[556  42  83  31  39  24  15  26 127  57]\n",
            " [ 51 674  10  11  12  11  17  24  50 140]\n",
            " [ 78  15 302  99 156 110 106  89  31  14]\n",
            " [ 36  20  91 267 100 219 128  86  18  35]\n",
            " [ 39  13 132  77 422  67 112 101  19  18]\n",
            " [ 19  15  78 186  75 424  73 104  10  16]\n",
            " [ 13  22  78  89  79  71 567  45  14  22]\n",
            " [ 37  16  55  89 116  94  24 521  11  37]\n",
            " [186  83  22  21  20  11  18  10 580  49]\n",
            " [ 66 232  15  35  14  28  20  49  60 481]]\n",
            "Average Total Loss over Batches: 0.392635604981184\n",
            "[[553  33  66  35  34  16  17  21 142  83]\n",
            " [ 40 666   6  13  12   7  22  19  57 158]\n",
            " [ 92  20 307  95 140  88 129  75  32  22]\n",
            " [ 38  25  86 278  97 169 152  81  26  48]\n",
            " [ 45  15 112  80 401  55 146 102  18  26]\n",
            " [ 26  22  88 194  78 343  92 105  21  31]\n",
            " [ 14  26  81  78  69  45 608  36  12  31]\n",
            " [ 35  23  48  75 114  90  34 503  12  66]\n",
            " [177  65  24  20  13   8  20   8 591  74]\n",
            " [ 56 189  17  27  11  21  21  52  61 545]]\n",
            "Average Total Loss over Batches: 0.3770846032643318\n",
            "[[447  33 114  53  51  31  19  21 159  72]\n",
            " [ 37 617  12  24  15  12  30  24  59 170]\n",
            " [ 52   6 349 137 151 118  89  56  26  16]\n",
            " [ 27  12  80 343 106 221 106  64  14  27]\n",
            " [ 28   7 150 121 391  78 100  85  20  20]\n",
            " [  9  11  85 233  66 419  59  88  14  16]\n",
            " [  6  16  96 116  79  75 522  40  20  30]\n",
            " [ 15  13  61 112 118 104  33 490  11  43]\n",
            " [123  73  31  37  25  16  20   9 608  58]\n",
            " [ 41 187  23  43  16  27  26  54  59 524]]\n",
            "Average Total Loss over Batches: 0.37274463581204414\n",
            "[[487  32 112  39  53  16  21  20 156  64]\n",
            " [ 50 628  12  22  16   7  30  18  67 150]\n",
            " [ 69  13 343 116 142  69 129  73  32  14]\n",
            " [ 34  20  96 287 113 157 160  90  20  23]\n",
            " [ 34  12 140  94 406  47 143  88  19  17]\n",
            " [ 16  12 111 201  88 320 103 113  19  17]\n",
            " [ 14  17  87  88  89  33 606  30  15  21]\n",
            " [ 33  12  75  92 145  68  44 477  11  43]\n",
            " [148  70  30  24  26   7  28  10 614  43]\n",
            " [ 67 183  23  34  15  16  28  46  74 514]]\n",
            "Average Total Loss over Batches: 0.35293681895554063\n",
            "[[558  28 108  41  33  20  12  23 106  71]\n",
            " [ 66 582  10  24  12  10  18  31  57 190]\n",
            " [ 85   9 384 124 116 100  74  75  17  16]\n",
            " [ 42  15 108 290  98 221  81  90  21  34]\n",
            " [ 51   7 181 102 377  63  73 110  10  26]\n",
            " [ 22  10 113 214  72 370  47 117  18  17]\n",
            " [ 20  13 132 126 102  77 435  54  12  29]\n",
            " [ 45   9  74  88 108  84  12 537  10  33]\n",
            " [200  68  35  26  18   9  12  18 543  71]\n",
            " [ 59 147  26  35  14  27  10  65  63 554]]\n",
            "Average Total Loss over Batches: 0.34435697070151566\n",
            "[[501  24  41  45  65  28  18  25 163  90]\n",
            " [ 38 577   4  27  22   8  17  27  58 222]\n",
            " [ 75  11 241 139 192 118  83  81  37  23]\n",
            " [ 29  14  53 321 117 199 102  92  19  54]\n",
            " [ 35  12  86 102 427  65  94 128  20  31]\n",
            " [ 15  12  56 217  96 392  44 117  16  35]\n",
            " [ 12  14  70 106  97  76 507  55  17  46]\n",
            " [ 22  11  41  88 123  90  15 540   9  61]\n",
            " [133  62  11  31  31   7  16  11 618  80]\n",
            " [ 47 142   9  30  14  23  15  54  59 607]]\n",
            "Average Total Loss over Batches: 0.33181566143393515\n",
            "[[517  26  76  28  51  29  11  18 173  71]\n",
            " [ 60 580   9  13  19   8  19  23  79 190]\n",
            " [ 81   9 305 110 186 119  70  58  39  23]\n",
            " [ 45  14  84 264 133 227  80  74  30  49]\n",
            " [ 46  12 120  83 446  70  70  98  20  35]\n",
            " [ 22  11  82 172  95 424  44 102  19  29]\n",
            " [ 16  17  91 101 106  94 457  44  25  49]\n",
            " [ 36  16  62  88 140  99  21 457  20  61]\n",
            " [152  60  26  19  24  11  15   7 618  68]\n",
            " [ 65 149  18  24  16  20  12  47  88 561]]\n",
            "Average Total Loss over Batches: 0.3214240390983224\n",
            "[[562  30  85  40  29  29  12  19 142  52]\n",
            " [ 62 610  11  26  15  13  14  26  62 161]\n",
            " [ 90  12 338 123 131 133  59  68  31  15]\n",
            " [ 46  16  90 322  92 235  64  86  22  27]\n",
            " [ 53   9 140 103 370  94  71 116  21  23]\n",
            " [ 27  15  92 201  65 440  38  93  12  17]\n",
            " [ 24  23 105 139  99 128 391  46  19  26]\n",
            " [ 39   9  66  97  98 110  12 508  15  46]\n",
            " [190  81  38  28  17  10   8  10 564  54]\n",
            " [ 81 183  24  36  13  35  13  50  71 494]]\n",
            "Average Total Loss over Batches: 0.3139684078961611\n",
            "[[493  32 104  31  35  22  18  12 187  66]\n",
            " [ 49 624  10  16  10  10  22  17  84 158]\n",
            " [ 70  15 369 110 117  88 116  54  45  16]\n",
            " [ 46  28 107 277  95 187 137  63  28  32]\n",
            " [ 39  13 165  89 361  54 138  88  29  24]\n",
            " [ 23  22 133 205  78 333  80  79  26  21]\n",
            " [ 17  29 106  99  71  59 531  31  27  30]\n",
            " [ 48  24  99 107 121  89  35 412  19  46]\n",
            " [146  73  31  21  19   5  11   5 630  59]\n",
            " [ 72 208  28  24  12  23  16  33  91 493]]\n",
            "Average Total Loss over Batches: 0.30412037832096217\n",
            "[[484  38 111  52  38  19  18  22 160  58]\n",
            " [ 44 612  13  16  13  10  32  24  69 167]\n",
            " [ 66   7 340 123 132  91 123  68  34  16]\n",
            " [ 23  25  98 310  97 177 150  81  17  22]\n",
            " [ 37  13 142 107 362  54 149  95  21  20]\n",
            " [ 13  16 108 224  71 366  87  88  16  11]\n",
            " [ 13  20  95 100  71  55 572  38  15  21]\n",
            " [ 27  12  81 107 108  86  46 477  15  41]\n",
            " [148  74  37  28  24   9  22  13 585  60]\n",
            " [ 63 209  25  31  12  24  24  52  80 480]]\n",
            "Average Total Loss over Batches: 0.2926829950518906\n",
            "[[491  32 121  33  38  26  14  18 174  53]\n",
            " [ 47 640  13  16  19  16  17  17  78 137]\n",
            " [ 80   7 351 104 151 113  79  65  36  14]\n",
            " [ 42  16 108 280 105 213 108  79  25  24]\n",
            " [ 43  10 146  83 392  81  92 103  27  23]\n",
            " [ 18  16 110 171  79 403  67  96  20  20]\n",
            " [ 10  22 104 102  95  87 481  47  26  26]\n",
            " [ 31  14  79  89 133  99  29 470  20  36]\n",
            " [153  71  38  26  16  13  15   9 610  49]\n",
            " [ 63 198  24  27  21  26  16  48  88 489]]\n",
            "Average Total Loss over Batches: 0.2940995018543303\n",
            "[[524  38 122  29  36  35  11  23 133  49]\n",
            " [ 57 621  17  19  19  13  23  23  65 143]\n",
            " [ 79  12 363 112 137 122  87  51  25  12]\n",
            " [ 48  21 116 270  96 239  95  81  18  16]\n",
            " [ 44  10 177  94 385  80  85  87  18  20]\n",
            " [ 23  12 119 175  81 412  62  92  13  11]\n",
            " [ 13  25 142 109  92  94 453  40  11  21]\n",
            " [ 27  21  96 104 134 112  27 441  15  23]\n",
            " [192  92  46  25  22  14  18  11 535  45]\n",
            " [ 70 200  35  37  19  34  25  44  74 462]]\n",
            "Average Total Loss over Batches: 0.2839558259743452\n",
            "[[489  30  89  44  54  19  11  17 178  69]\n",
            " [ 53 584  11  30  24   8  13  20  67 190]\n",
            " [ 71  10 329 134 176  88  73  69  32  18]\n",
            " [ 36  15  97 328 130 166  83  87  22  36]\n",
            " [ 39  11 136  97 448  53  68  96  24  28]\n",
            " [ 15  14  90 233 108 338  43 115  17  27]\n",
            " [ 12  18  99 130 123  79 435  54  17  33]\n",
            " [ 24  13  65 105 145  82  15 496  16  39]\n",
            " [151  60  27  30  31   9  19  16 590  67]\n",
            " [ 55 167  25  38  28  31  16  49  77 514]]\n",
            "Average Total Loss over Batches: 0.2748649749465287\n",
            "[[583  30  85  34  37  21  13  17 119  61]\n",
            " [ 61 622  10  22  20   8  22  19  57 159]\n",
            " [ 96  15 338 120 147  99  86  58  28  13]\n",
            " [ 46  25  93 299  99 196 107  86  22  27]\n",
            " [ 55  10 130 113 383  67  88 107  22  25]\n",
            " [ 23  15 109 210  73 346  65 116  20  23]\n",
            " [ 19  24 100 127  77  69 486  53  14  31]\n",
            " [ 40  23  71  99 107  86  22 500  11  41]\n",
            " [194  72  32  29  24   8  18   9 555  59]\n",
            " [ 74 197  24  30  15  25  16  56  71 492]]\n",
            "Average Total Loss over Batches: 0.2687944213941693\n",
            "[[551  31  51  36  30  21  10  23 173  74]\n",
            " [ 43 615   9  24  15   7   8  22  65 192]\n",
            " [111  16 280 124 139 106  65  88  45  26]\n",
            " [ 57  23  69 308  94 184  76 106  35  48]\n",
            " [ 53  12 108 101 383  63  61 154  28  37]\n",
            " [ 29  20  68 213  75 342  47 150  24  32]\n",
            " [ 21  30  87 142  92  84 386  77  26  55]\n",
            " [ 40  23  48  82  86  75  13 556  17  60]\n",
            " [147  70  18  26  16   9   8  14 628  64]\n",
            " [ 59 178  13  29  10  23   6  55  76 551]]\n",
            "Average Total Loss over Batches: 0.26569281042724846\n",
            "[[457  34  95  49  66  24  13  38 149  75]\n",
            " [ 39 584  12  26  21  17  22  22  61 196]\n",
            " [ 58  12 307 124 170 128  74  83  26  18]\n",
            " [ 27  19  73 310 105 229  96  92  19  30]\n",
            " [ 27   8 117 107 407  84  78 128  19  25]\n",
            " [ 14  13  72 218  85 392  59 114  12  21]\n",
            " [ 16  18  86 142  88  87 458  58  14  33]\n",
            " [ 20  11  60  96 114 110  22 514  13  40]\n",
            " [144  82  25  32  31  13  21  15 566  71]\n",
            " [ 42 174  20  35  22  32  17  64  68 526]]\n",
            "Average Total Loss over Batches: 0.2547032658842951\n",
            "[[493  33  81  42  47  20  17  27 195  45]\n",
            " [ 56 625  15  18  15   9  27  20  82 133]\n",
            " [ 81  13 339 122 137  75 104  72  46  11]\n",
            " [ 40  22 102 284 115 167 122  95  24  29]\n",
            " [ 45  11 138  99 392  47 101 113  29  25]\n",
            " [ 24  15 110 217  83 309  88 115  21  18]\n",
            " [ 19  21  88 119  89  56 507  46  26  29]\n",
            " [ 37  18  82  95 119  75  35 483  17  39]\n",
            " [164  65  28  27  19   8  24   8 614  43]\n",
            " [ 67 202  27  31  19  24  21  50 103 456]]\n",
            "Average Total Loss over Batches: 0.253597197342217\n",
            "[[529  28  96  44  45  16  11  21 138  72]\n",
            " [ 53 630  10  17  18  10  24  19  49 170]\n",
            " [ 74  16 326 104 149  96 118  71  28  18]\n",
            " [ 45  26  94 250 124 170 147  92  19  33]\n",
            " [ 41   9 126  92 419  47 116 108  15  27]\n",
            " [ 22  17 114 195  94 303  87 130  19  19]\n",
            " [ 15  20  90 106  88  55 524  54  19  29]\n",
            " [ 34  17  60  93 121  86  35 491  11  52]\n",
            " [179  74  34  19  23   8  26  15 560  62]\n",
            " [ 58 181  19  21  23  19  23  54  68 534]]\n",
            "Average Total Loss over Batches: 0.24902271088927985\n",
            "[[505  30  92  38  56  20  14  18 158  69]\n",
            " [ 63 562  10  23  21  11  26  23  68 193]\n",
            " [ 78  10 302 103 183 101 105  60  37  21]\n",
            " [ 50  19  93 240 137 205 132  75  21  28]\n",
            " [ 38   8 116  84 437  60 130  84  20  23]\n",
            " [ 19  13  99 187 103 350  92 102  15  20]\n",
            " [ 21  16  80  92 100  71 533  38  21  28]\n",
            " [ 38  14  70  81 135  99  39 458  12  54]\n",
            " [160  60  30  23  28  13  23  11 575  77]\n",
            " [ 66 154  21  31  27  25  23  49  83 521]]\n",
            "Average Total Loss over Batches: 0.23644824927143754\n",
            "[[523  35  97  36  47  24   8  29 146  55]\n",
            " [ 68 628   9  22  16  11  15  23  56 152]\n",
            " [ 80  15 326 117 154 117  62  86  30  13]\n",
            " [ 48  22 100 295 109 209  72  99  20  26]\n",
            " [ 45  10 127  88 410  74  66 133  21  26]\n",
            " [ 28  15 106 204  85 354  46 120  19  23]\n",
            " [ 24  21 102 145 111  85 391  73  17  31]\n",
            " [ 40  15  53  90 106  98  17 528  10  43]\n",
            " [179  74  26  26  26  10  12  15 580  52]\n",
            " [ 71 212  17  36  16  29  12  57  76 474]]\n",
            "Average Total Loss over Batches: 0.24493468619145453\n",
            "[[489  32  91  46  47  32  19  27 164  53]\n",
            " [ 58 580  14  26  26  15  29  27  68 157]\n",
            " [ 68  10 302 110 173 131 100  60  30  16]\n",
            " [ 38  17  97 281 107 224 116  81  22  17]\n",
            " [ 40   6 130 101 405  75 103  99  21  20]\n",
            " [ 18  11  96 206  83 399  69  89  14  15]\n",
            " [ 14   9  94 117  90  90 501  48  15  22]\n",
            " [ 28  10  76  99 118 120  23 484  15  27]\n",
            " [152  67  33  27  28  13  21  16 589  54]\n",
            " [ 64 175  26  34  24  38  26  67  76 470]]\n",
            "Average Total Loss over Batches: 0.2361574965468049\n",
            "[[470  26  84  52  36  32  13  27 195  65]\n",
            " [ 50 570  12  24  16  15  19  26  72 196]\n",
            " [ 71  10 312 118 138 130  94  67  42  18]\n",
            " [ 41  13  87 297  99 237  97  72  25  32]\n",
            " [ 34   8 147 108 357  88  97 106  26  29]\n",
            " [ 10   7  95 210  55 420  64 100  17  22]\n",
            " [ 17  21 102 128  85 100 437  57  19  34]\n",
            " [ 28  12  70 103  98 120  26 467  16  60]\n",
            " [133  67  23  29  23  10  16  13 609  77]\n",
            " [ 60 153  16  35  12  29  18  49  78 550]]\n",
            "Average Total Loss over Batches: 0.23663004897829146\n",
            "[[502  50 114  31  50  24  19  20 119  71]\n",
            " [ 54 644  16  13  21  10  24  19  44 155]\n",
            " [ 67  10 358  91 152 104 116  58  22  22]\n",
            " [ 41  30 121 225 126 165 144  86  24  38]\n",
            " [ 34  10 156  74 391  56 132 106  15  26]\n",
            " [ 22  16 129 167  94 310  94 123  18  27]\n",
            " [ 15  25  97  79  91  59 538  48  20  28]\n",
            " [ 28  17  82  79 119  87  36 480  11  61]\n",
            " [200  82  39  22  33  14  23  15 500  72]\n",
            " [ 59 194  25  33  18  20  24  55  55 517]]\n",
            "Average Total Loss over Batches: 0.235411427038759\n",
            "[[527  35  98  48  47  20  18  13 146  48]\n",
            " [ 61 616  17  24  17  11  31  18  70 135]\n",
            " [ 82  11 322 110 159  99 105  61  38  13]\n",
            " [ 43  20  97 272 122 201 130  63  24  28]\n",
            " [ 41  11 146  91 416  59 105  85  25  21]\n",
            " [ 20  14 113 205  92 338  89  90  22  17]\n",
            " [ 16  21 102 107  97  64 509  36  23  25]\n",
            " [ 40  13  84 107 120 107  37 444  15  33]\n",
            " [172  83  31  28  28  11  21  11 573  42]\n",
            " [ 76 199  21  36  24  26  37  41  91 449]]\n",
            "Average Total Loss over Batches: 0.2254429419065267\n",
            "[[522  35  89  39  31  22  19  20 169  54]\n",
            " [ 55 600  14  16  22  13  18  21  75 166]\n",
            " [ 86  13 311 128 159 101  98  53  38  13]\n",
            " [ 43  22  99 287 117 194 114  70  25  29]\n",
            " [ 43  10 132  97 410  63  87 109  23  26]\n",
            " [ 24  16 121 214  89 338  67  96  18  17]\n",
            " [ 24  21  98 121 106  80 447  51  20  32]\n",
            " [ 36  15  82 100 117  94  22 473  17  44]\n",
            " [172  71  27  27  25  12  15  13 587  51]\n",
            " [ 78 190  19  36  16  27  18  51  91 474]]\n",
            "Average Total Loss over Batches: 0.22672315018974246\n",
            "[[489  37  91  48  45  20  16  29 165  60]\n",
            " [ 58 583   8  19  14   9  30  23  66 190]\n",
            " [ 78  13 324 113 142 101 111  71  28  19]\n",
            " [ 47  17 107 258 111 175 135  88  24  38]\n",
            " [ 43  12 144 102 363  60 120 106  22  28]\n",
            " [ 22  14 116 204  78 331  80 114  18  23]\n",
            " [ 24  23  92 105  81  65 500  54  23  33]\n",
            " [ 37  20  75 100 103  82  32 487  14  50]\n",
            " [151  80  30  23  28  10  20  19 577  62]\n",
            " [ 65 186  19  34  13  25  19  56  74 509]]\n",
            "Average Total Loss over Batches: 0.22816335700206458\n",
            "[[507  45  83  35  43  20  15  28 157  67]\n",
            " [ 49 656   8  15  13  10  13  21  53 162]\n",
            " [ 88  17 323 108 154 106  64  84  35  21]\n",
            " [ 38  34  96 280 111 189  94  92  27  39]\n",
            " [ 47  15 133  82 386  64  81 140  23  29]\n",
            " [ 23  23  92 194  90 346  55 122  25  30]\n",
            " [ 16  42 102 113 108  76 407  63  26  47]\n",
            " [ 24  24  57  78 113  85  20 527  12  60]\n",
            " [154  91  21  21  23   9  14  12 577  78]\n",
            " [ 52 195  16  25  17  23  11  52  72 537]]\n",
            "Average Total Loss over Batches: 0.2185923476244323\n",
            "[[506  35  62  52  37  30  16  24 150  88]\n",
            " [ 53 579   8  15  14   9  25  20  55 222]\n",
            " [ 82  13 254 128 147 122 121  74  35  24]\n",
            " [ 38  14  68 298 108 200 125  80  19  50]\n",
            " [ 46  11 100 110 368  68 118 123  22  34]\n",
            " [ 16  17  65 225  73 364  75 113  16  36]\n",
            " [ 19  20  69  92  86  69 532  48  18  47]\n",
            " [ 31  15  48  94  90  89  32 513  15  73]\n",
            " [165  73  14  23  24  10  19  14 576  82]\n",
            " [ 52 172  15  26  14  21  18  50  67 565]]\n",
            "Average Total Loss over Batches: 0.2203148600673303\n",
            "[[523  34  83  41  44  28  10  12 163  62]\n",
            " [ 71 599  11  19  13  14  18  18  65 172]\n",
            " [ 88  11 331 119 149 110  79  56  42  15]\n",
            " [ 52  19 107 285 100 215  91  70  33  28]\n",
            " [ 51  10 154 105 386  76  87  83  25  23]\n",
            " [ 24  14 104 214  78 381  57  88  18  22]\n",
            " [ 23  20 105 118 103 102 427  44  26  32]\n",
            " [ 36  19  77 101 107 123  28 442  22  45]\n",
            " [164  71  25  18  32   8  16   7 589  70]\n",
            " [ 69 189  20  32  19  22  15  47  91 496]]\n",
            "Average Total Loss over Batches: 0.20543633947890252\n",
            "[[572  18  85  47  39  17  19  15 141  47]\n",
            " [ 82 544  21  27  16  16  36  24  75 159]\n",
            " [ 99   6 325 116 131 103 112  56  39  13]\n",
            " [ 51  13 122 271  97 188 133  70  33  22]\n",
            " [ 52   7 172 102 350  69 115  96  22  15]\n",
            " [ 28  12 139 227  75 304  88  88  23  16]\n",
            " [ 28  11 116 106  86  70 507  41  18  17]\n",
            " [ 51  14  92 101 109 101  37 451  19  25]\n",
            " [192  61  36  24  23  13  19  16 568  48]\n",
            " [ 88 156  32  39  24  31  35  53  94 448]]\n",
            "Average Total Loss over Batches: 0.21736369385868312\n",
            "[[503  33  83  39  52  30  16  22 151  71]\n",
            " [ 60 594   9  19  18  12  27  22  58 181]\n",
            " [ 71  13 301 114 166 123  84  76  34  18]\n",
            " [ 39  15  93 269 112 227 101  85  27  32]\n",
            " [ 40   7 132 107 390  76  84 115  21  28]\n",
            " [ 19  19  90 200  81 382  66 104  14  25]\n",
            " [ 21  25  88 118 101  99 445  49  21  33]\n",
            " [ 34  18  63  92 113 100  24 487  16  53]\n",
            " [140  78  29  22  35   9  15  12 586  74]\n",
            " [ 62 181  21  33  16  23  16  47  73 528]]\n",
            "Average Total Loss over Batches: 0.20435421326547862\n",
            "[[533  40  87  46  30  14  11  14 179  46]\n",
            " [ 59 634  15  16  11   6  20  17  93 129]\n",
            " [ 88  12 376 116 136  78  80  51  54   9]\n",
            " [ 56  21 135 306 103 170  72  67  42  28]\n",
            " [ 46   9 209 105 354  57  74  89  30  27]\n",
            " [ 25  16 145 239  91 288  66  84  26  20]\n",
            " [ 27  30 148 127 107  72 379  52  36  22]\n",
            " [ 47  18  95 109 114  79  25 443  20  50]\n",
            " [167  79  28  16  19   7   9   9 615  51]\n",
            " [ 77 205  25  30  15  14  16  46 108 464]]\n",
            "Average Total Loss over Batches: 0.21356311929300428\n",
            "[[528  45  72  43  28  25  10  13 173  63]\n",
            " [ 63 625   8  16  13  10  13  19  70 163]\n",
            " [100  13 311 141 121 119  70  63  44  18]\n",
            " [ 64  24  85 316  85 208  78  73  32  35]\n",
            " [ 56  15 133 114 351  72  86 118  25  30]\n",
            " [ 24  18 101 224  69 356  56 100  23  29]\n",
            " [ 29  25 105 141  78  86 419  55  27  35]\n",
            " [ 42  21  58 117  88  91  18 492  15  58]\n",
            " [161  88  19  19  20   9   8  13 599  64]\n",
            " [ 76 195  14  34  10  19  10  41  91 510]]\n",
            "Average Total Loss over Batches: 0.20784762503810228\n",
            "[[477  45 120  44  30  33  13  24 161  53]\n",
            " [ 45 640  12  16  11  12  30  20  62 152]\n",
            " [ 68  13 365 119 110 126  96  56  35  12]\n",
            " [ 38  21 107 298  68 228 114  75  22  29]\n",
            " [ 36  13 171  97 318  87 111 120  23  24]\n",
            " [ 13  16 113 215  61 391  70  88  16  17]\n",
            " [ 21  25 100 120  71  81 493  45  16  28]\n",
            " [ 30  27  78 114  88 114  26 466  12  45]\n",
            " [122  89  34  25  20  15  20  16 614  45]\n",
            " [ 61 210  23  37  12  26  23  44  75 489]]\n",
            "Average Total Loss over Batches: 0.2016201051302254\n",
            "[[476  25  91  43  49  26  16  24 184  66]\n",
            " [ 64 536  11  27  23  20  30  29  62 198]\n",
            " [ 73   8 316 116 140 130  88  76  37  16]\n",
            " [ 37  12  88 288 112 236  90  92  21  24]\n",
            " [ 34   4 143 101 365  86 101 118  25  23]\n",
            " [ 21   8  82 209  78 390  63 111  15  23]\n",
            " [ 16  13  92 120 107  93 453  60  22  24]\n",
            " [ 27  14  64 103 106 115  23 501  10  37]\n",
            " [138  62  29  25  32  19  16  19 590  70]\n",
            " [ 58 145  18  48  18  31  27  75  79 501]]\n",
            "Average Total Loss over Batches: 0.2001379395790957\n",
            "[[488  32  91  49  60  29  18  17 162  54]\n",
            " [ 50 607   6  29  21  17  31  21  69 149]\n",
            " [ 69  11 291 122 168 133 102  54  35  15]\n",
            " [ 35  22  77 297 107 234 116  62  27  23]\n",
            " [ 38   7 117 104 391  93 110  94  22  24]\n",
            " [ 17  11  77 223  83 394  73  86  17  19]\n",
            " [ 19  17  80 123 105  93 477  38  19  29]\n",
            " [ 29  14  59 115 123 126  23 458  15  38]\n",
            " [135  66  24  31  39  18  21   9 597  60]\n",
            " [ 59 188  19  50  18  34  25  51  84 472]]\n",
            "Average Total Loss over Batches: 0.20197052788380535\n",
            "[[475  41  94  48  33  32  11  28 182  56]\n",
            " [ 50 616  14  30  13  15  14  25  71 152]\n",
            " [ 73  11 327 138 123 127  66  85  39  11]\n",
            " [ 37  18  97 323  91 224  74  90  21  25]\n",
            " [ 38  10 158 129 332  74  63 145  27  24]\n",
            " [ 17  16  95 244  70 368  45 107  19  19]\n",
            " [ 17  22 104 152  95 106 382  62  30  30]\n",
            " [ 27  18  68 113  89 110  14 512  16  33]\n",
            " [127  74  25  28  21  17  17  20 616  55]\n",
            " [ 60 174  20  45  14  34  13  66  86 488]]\n",
            "Average Total Loss over Batches: 0.19152344946957192\n",
            "[[452  30 138  44  45  18  21  19 191  42]\n",
            " [ 54 589  18  30  23  12  38  19  82 135]\n",
            " [ 62  10 374 126 127  80 116  52  41  12]\n",
            " [ 36  20 132 286 109 170 126  69  30  22]\n",
            " [ 36   8 175 105 370  57 121  87  23  18]\n",
            " [ 20  12 145 231  76 309  78  97  17  15]\n",
            " [ 18  15 111 103  88  63 528  33  21  20]\n",
            " [ 30  17 106 109 127  86  41 440  16  28]\n",
            " [139  68  43  26  26   8  24   9 614  43]\n",
            " [ 68 183  33  49  18  22  42  53  87 445]]\n",
            "Average Total Loss over Batches: 0.20208184822648764\n",
            "[[504  55  67  45  32  20  16  19 148  94]\n",
            " [ 41 639   6  19  17  11  14  21  49 183]\n",
            " [ 82  30 267 117 144 115 100  82  36  27]\n",
            " [ 39  34  70 284 107 181 121  85  25  54]\n",
            " [ 43  25  99 105 364  73  98 132  22  39]\n",
            " [ 21  25  80 211  85 322  78 119  19  40]\n",
            " [ 23  37  65  98  94  72 483  49  21  58]\n",
            " [ 32  33  50  94 100  79  24 491  12  85]\n",
            " [151  85  18  25  23  13  18  11 567  89]\n",
            " [ 43 208  12  27  13  23  11  49  65 549]]\n",
            "Average Total Loss over Batches: 0.1967626636453904\n",
            "[[541  36  67  38  46  25  14  16 161  56]\n",
            " [ 71 591   7  19  13  12  22  20  82 163]\n",
            " [ 97  12 312 118 157  94  85  65  43  17]\n",
            " [ 54  17  92 288 104 204 112  70  34  25]\n",
            " [ 53  10 135 103 393  64  96  94  28  24]\n",
            " [ 33  11 110 223  85 330  68  97  21  22]\n",
            " [ 26  30 108 123 100  79 424  55  27  28]\n",
            " [ 47  28  71 108 122  88  29 453  15  39]\n",
            " [186  72  23  19  26  11  15  10 575  63]\n",
            " [ 75 189  17  34  18  21  19  50  98 479]]\n",
            "Average Total Loss over Batches: 0.1915798739678599\n",
            "[[471  44  99  43  36  15  14  20 201  57]\n",
            " [ 51 652   9  24  15   8  16  19  71 135]\n",
            " [ 86  23 308 127 132  94  93  74  46  17]\n",
            " [ 54  31  99 300  98 172 100  82  35  29]\n",
            " [ 36  18 145 106 348  74  89 124  32  28]\n",
            " [ 26  27  99 217  90 278  69 135  25  34]\n",
            " [ 19  44 108 110  81  56 458  57  31  36]\n",
            " [ 29  35  68  99 106  72  22 495  19  55]\n",
            " [136  83  27  24  16   8  13   7 631  55]\n",
            " [ 55 225  17  33  18  17  16  46  95 478]]\n",
            "Average Total Loss over Batches: 0.1906761960842181\n",
            "[[516  25 107  67  47  32  19  16 118  53]\n",
            " [ 49 527  13  40  25  25  50  24  60 187]\n",
            " [ 77   4 324 143 140 126  93  54  24  15]\n",
            " [ 35  15 104 328  84 215 108  73  19  19]\n",
            " [ 37   6 149 121 356  93 121  86  13  18]\n",
            " [ 17   4  99 230  71 397  69  82  11  20]\n",
            " [ 21  18  90 115  90 101 476  41  18  30]\n",
            " [ 23  12  81 123 119 129  31 439  10  33]\n",
            " [189  60  45  38  31  21  30  13 509  64]\n",
            " [ 59 134  30  57  21  40  43  50  74 492]]\n",
            "Average Total Loss over Batches: 0.19327893031087703\n",
            "[[528  35  77  55  44  26  17  20 153  45]\n",
            " [ 65 571  13  29  22  15  27  20  85 153]\n",
            " [ 76   9 308 140 151 106 107  54  33  16]\n",
            " [ 44  13  96 298 113 176 138  69  27  26]\n",
            " [ 40   7 130 116 408  55 114  82  25  23]\n",
            " [ 21  13 103 251  97 312  76  88  15  24]\n",
            " [ 19  18  83 119 103  58 518  42  18  22]\n",
            " [ 43  21  83 117 137  91  39 423  16  30]\n",
            " [173  71  29  28  30  10  22   9 565  63]\n",
            " [ 88 188  24  43  20  21  24  52  84 456]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4bd2d8d3b151>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mcnn_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                             )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    161\u001b[0m                 state_steps)\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    164\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    312\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_div_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq_sqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq_sqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_addcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_exp_avgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg_sq_sqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar results - but much faster."
      ],
      "metadata": {
        "id": "RlMvskNtGWDq"
      }
    }
  ]
}