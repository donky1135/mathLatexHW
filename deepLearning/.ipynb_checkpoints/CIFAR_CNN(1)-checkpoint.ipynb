{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MxHWiEHnYqW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivIpm9qZ3V9y",
        "outputId": "bfaea4f8-83d3-425c-ce61-a5bfa541bbd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaxI5PV_4K_x"
      },
      "source": [
        "In the above, I'm downloading the CIFAR data set. This is a set of 32x32 pixel RGB images, belonging to the 10 classes indicated. The ToTensor() transformation here is necessary to convert the original data from image objects to numerical arrays that we can process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMwhxL9bo74s",
        "outputId": "1689a9eb-41e9-4428-8f73-4ab0c8ecf330"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainset.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKDU8rSm4ZPN"
      },
      "source": [
        "The training data set consists of 50,000 32x32 images, each pixel consisting of three values (RGB). The three values are generally referred to as the `channels' of the data. A black and white image could be thought of as having a single channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXGkny55pG_1",
        "outputId": "0d9c1894-a56b-4931-9696-c22d72652822"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[6, 9, 9, 4, 1, 1, 2, 7, 8, 3]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainset.targets[0:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63i86Q2B4nsS"
      },
      "source": [
        "The targets, the first 10 of which are shown here, are class labels, 0 through 9, corresponding to the indices in the classes list above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BgIVy_x1pMhZ",
        "outputId": "b7f1cc3d-f405-41b1-9697-151bcca4c0fe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'frog'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classes[ trainset.targets[0] ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOH2fPpP4tw4"
      },
      "source": [
        "We see that the first image in the training set is labeled as a frog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "AaUxu6WrpUH1",
        "outputId": "3ed0fce2-709f-4628-d502-3642594d8275"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-6b1c95bc-3a74-4231-9eb1-63d322180f81\" class=\"ndarray_repr\"><pre>ndarray (32, 32, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,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\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[ 59,  62,  63],\n",
              "        [ 43,  46,  45],\n",
              "        [ 50,  48,  43],\n",
              "        ...,\n",
              "        [158, 132, 108],\n",
              "        [152, 125, 102],\n",
              "        [148, 124, 103]],\n",
              "\n",
              "       [[ 16,  20,  20],\n",
              "        [  0,   0,   0],\n",
              "        [ 18,   8,   0],\n",
              "        ...,\n",
              "        [123,  88,  55],\n",
              "        [119,  83,  50],\n",
              "        [122,  87,  57]],\n",
              "\n",
              "       [[ 25,  24,  21],\n",
              "        [ 16,   7,   0],\n",
              "        [ 49,  27,   8],\n",
              "        ...,\n",
              "        [118,  84,  50],\n",
              "        [120,  84,  50],\n",
              "        [109,  73,  42]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[208, 170,  96],\n",
              "        [201, 153,  34],\n",
              "        [198, 161,  26],\n",
              "        ...,\n",
              "        [160, 133,  70],\n",
              "        [ 56,  31,   7],\n",
              "        [ 53,  34,  20]],\n",
              "\n",
              "       [[180, 139,  96],\n",
              "        [173, 123,  42],\n",
              "        [186, 144,  30],\n",
              "        ...,\n",
              "        [184, 148,  94],\n",
              "        [ 97,  62,  34],\n",
              "        [ 83,  53,  34]],\n",
              "\n",
              "       [[177, 144, 116],\n",
              "        [168, 129,  94],\n",
              "        [179, 142,  87],\n",
              "        ...,\n",
              "        [216, 184, 140],\n",
              "        [151, 118,  84],\n",
              "        [123,  92,  72]]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-6b1c95bc-3a74-4231-9eb1-63d322180f81 button').onclick = (e) => {\n",
              "        document.querySelector('#id-6b1c95bc-3a74-4231-9eb1-63d322180f81').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-6b1c95bc-3a74-4231-9eb1-63d322180f81 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ],
            "text/plain": [
              "array([[[ 59,  62,  63],\n",
              "        [ 43,  46,  45],\n",
              "        [ 50,  48,  43],\n",
              "        ...,\n",
              "        [158, 132, 108],\n",
              "        [152, 125, 102],\n",
              "        [148, 124, 103]],\n",
              "\n",
              "       [[ 16,  20,  20],\n",
              "        [  0,   0,   0],\n",
              "        [ 18,   8,   0],\n",
              "        ...,\n",
              "        [123,  88,  55],\n",
              "        [119,  83,  50],\n",
              "        [122,  87,  57]],\n",
              "\n",
              "       [[ 25,  24,  21],\n",
              "        [ 16,   7,   0],\n",
              "        [ 49,  27,   8],\n",
              "        ...,\n",
              "        [118,  84,  50],\n",
              "        [120,  84,  50],\n",
              "        [109,  73,  42]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[208, 170,  96],\n",
              "        [201, 153,  34],\n",
              "        [198, 161,  26],\n",
              "        ...,\n",
              "        [160, 133,  70],\n",
              "        [ 56,  31,   7],\n",
              "        [ 53,  34,  20]],\n",
              "\n",
              "       [[180, 139,  96],\n",
              "        [173, 123,  42],\n",
              "        [186, 144,  30],\n",
              "        ...,\n",
              "        [184, 148,  94],\n",
              "        [ 97,  62,  34],\n",
              "        [ 83,  53,  34]],\n",
              "\n",
              "       [[177, 144, 116],\n",
              "        [168, 129,  94],\n",
              "        [179, 142,  87],\n",
              "        ...,\n",
              "        [216, 184, 140],\n",
              "        [151, 118,  84],\n",
              "        [123,  92,  72]]], dtype=uint8)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainset.data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BniQOZ764yx1"
      },
      "source": [
        "COLAB prints out a tiny thumbnail here, which I think is interesting, but more importantly we see that the image data is stored as a numpy array, 32x32x3, where each value is an integer between 0 and 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "7AlD1gPHpVZ-",
        "outputId": "97054520-8c90-4491-a57a-1e697db1b0dd"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow( trainset.data[0] / 256 )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bamkT4q5CBc"
      },
      "source": [
        "In the following code block, I prepare the data for use - the first step is to scale all the pixel values to be between 0 and 1, by dividing by 255. Next, I subtract off 0.5, so that pixel values are centered [-0.5, 0.5] rather than [0, 255] as before.\n",
        "\n",
        "Two important technical notes here:\n",
        "\n",
        "First, the implementation of convolutional layers in PyTorch expects the data to be presented 'channel first', that is a tensor of data should be of the form [ batch_size, num channels, height, width ]. As such, I use the permute function here, to make the channel dimension the second dimension (placing dimension 3 in dimension 1), and bump over the remaining dimensions. The result of this will be, in the case of the training data, a block of size [50000,3,32,32].\n",
        "\n",
        "Second, apparently the crossEntropyLoss method expects class labels to come in the form of long data types, so I transform the data labels as appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3l8wu6ZleJo",
        "outputId": "568b6ff1-f51b-42fd-ea27-f36c8a316f0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([50000, 3, 32, 32])\n",
            "torch.Size([10000, 3, 32, 32])\n",
            "torch.Size([50000])\n",
            "torch.Size([10000])\n"
          ]
        }
      ],
      "source": [
        "train_X = torch.Tensor( trainset.data/255.0 - 0.5 )\n",
        "train_X = train_X.permute( 0, 3, 1, 2 )\n",
        "test_X = torch.Tensor( testset.data/255.0 - 0.5 )\n",
        "test_X = test_X.permute( 0, 3, 1, 2 )\n",
        "\n",
        "train_Y = torch.Tensor( np.asarray( trainset.targets ) ).long()\n",
        "#train_Y = train_Y.reshape( (-1,1) )\n",
        "test_Y = torch.Tensor( np.asarray( testset.targets ) ).long()\n",
        "#test_Y = test_Y.reshape( (-1,1) )\n",
        "\n",
        "print( train_X.shape )\n",
        "print( test_X.shape )\n",
        "print( train_Y.shape )\n",
        "print( test_Y.shape )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDj36Is157TQ"
      },
      "source": [
        "The following code simply serves to remove a random batch from the specified data set, as we've seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iHA-uU_znf2",
        "outputId": "3bc89dfb-61ff-47b7-fd9b-515af9494ad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 3, 32, 32])\n",
            "torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "def get_batch(x, y, batch_size):\n",
        "  n = x.shape[0]\n",
        "\n",
        "  batch_indices = random.sample( [ i for i in range(n) ], k = batch_size )\n",
        "\n",
        "  x_batch = x[ batch_indices ]\n",
        "  y_batch = y[ batch_indices ]\n",
        "\n",
        "  return x_batch, y_batch\n",
        "\n",
        "batch_x, batch_y = get_batch( train_X, train_Y, batch_size = 4 )\n",
        "print( batch_x.shape )\n",
        "print( batch_y.shape )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-XnMf4s6DCL"
      },
      "source": [
        "To demonstrate a convolutional layer, I'm going to create a 2D convolutional layer object. I'm specifying that the input will have 3 channels (RGB). The output will have 10 channels (that is, I will be calculating 10 features every time the convolutional window is applied). The local feature will be of size 3x3 (it doesn't have to be square, but the default assumption is square), and the stride is 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpJ39WOE0Rvj"
      },
      "outputs": [],
      "source": [
        "conv_test_layer = nn.Conv2d(in_channels = 3, out_channels = 10, kernel_size = 3, stride = 1, bias=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU52U4Er6XIs"
      },
      "source": [
        "Note that applying this convolutional layer to a input batch of 32x32, there are 30 places that a 3x3 feature window can be applied horizontally, at stride 1.\n",
        "\n",
        "Similarly, there are 30 places a 3x3 feature window can be applied horizontally, at stride 1.\n",
        "\n",
        "At each of these 30x30 locations, this layer will be computing 10 features. So the output of this layer should be of size 10x30x30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo5Fmk-k00mF"
      },
      "outputs": [],
      "source": [
        "convolved_batch = conv_test_layer( batch_x )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICnINTiD04Hr",
        "outputId": "52275512-d61a-47fd-bc58-94d1fbb1e75d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 10, 30, 30])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convolved_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVnxcxGb6y-l"
      },
      "source": [
        "This is exactly as we expect.\n",
        "\n",
        "What do the internal parameters of a convolutional layer look like?\n",
        "\n",
        "Note that when we compute a feature of a 3x3 window, that is of 9 pixels, each one with three values. So a single feature will require 3\\*3\\*3 weights, with a single bias value.\n",
        "\n",
        "If we want to do this 10 times (for 10 total features), we need an arrangement of 10x3x3x3 weights, and 10 bias values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WZv6YmJ05hM",
        "outputId": "e5086f03-9cc5-483b-b235-f31a3ccd874e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 3, 3, 3])\n",
            "torch.Size([10])\n"
          ]
        }
      ],
      "source": [
        "print( conv_test_layer.weight.shape )\n",
        "print( conv_test_layer.bias.shape )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3umk3hDD7XkS"
      },
      "source": [
        "This is exactly as we expect.\n",
        "\n",
        "Note that `under the hood', the convolutional layer is applying this block of weights iteratively, at each location it is computing a local feature (according to the stride)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP6BjfEQ7mGg"
      },
      "source": [
        "We can in fact stack convolutional layers.\n",
        "\n",
        "The output of the first test layer is 10x30x30. We could pass this through another convolutional layer. Here I specify 10 input features or channels at each location, we want to compute 5 output features, and the kernel or feature window will again be 3x3, with a stride of 1.\n",
        "\n",
        "For a 30x30 field of view, and a 3x3 feature window at stride 1, there are 28x28 possible locations to apply that feature window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcYNfIGaGx_m"
      },
      "outputs": [],
      "source": [
        "conv_test_layer_2 = nn.Conv2d(in_channels = 10, out_channels = 5, kernel_size = 3, stride = 1, bias=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vgciZnLG3cw",
        "outputId": "09443b3b-ebbc-4a62-aec0-af76157bfc28"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 5, 28, 28])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conv_test_layer_2( convolved_batch ).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv1NzsFQ8AF0"
      },
      "source": [
        "We see that for each of the 4 images in the batch, we are computing an output of size 5 (features) x 28 (local field height) x 28 (local field width)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkQclHkF1Dig"
      },
      "outputs": [],
      "source": [
        "class CIFARModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CIFARModel, self).__init__()\n",
        "\n",
        "    self.conv_layer_1 = nn.Conv2d(in_channels = 3, out_channels = 5, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.conv_layer_2 = nn.Conv2d(in_channels = 5, out_channels = 10, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.conv_layer_3 = nn.Conv2d(in_channels = 10, out_channels = 15, kernel_size = 3, stride = 1, bias=True)\n",
        "\n",
        "    self.linear_layer = torch.nn.Linear( in_features = 15*26*26, out_features = 10, bias=True )\n",
        "    # Note that the output of the last convolutional layer will be 15x26x16 - why?\n",
        "    # So we want to input 15*26*26 values into the last layer, and get 10 output values out (for the class probabilities)\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    output = self.conv_layer_1( input_tensor )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.conv_layer_2( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.conv_layer_3( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "\n",
        "    # At this point, the block of node values from the convolutional layer is flattened\n",
        "    # So that it can be passed into a standard linear layer\n",
        "    output = nn.Flatten()( output )\n",
        "    output = self.linear_layer( output )\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8HdWU1B88zo"
      },
      "source": [
        "We can initialize and view this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VgmewIe1lSp",
        "outputId": "67a29238-63af-46ab-e499-63ab6d87430d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CIFARModel(\n",
            "  (conv_layer_1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv_layer_2): Conv2d(5, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv_layer_3): Conv2d(10, 15, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (linear_layer): Linear(in_features=10140, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "cifar_model = CIFARModel()\n",
        "print( cifar_model )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKYeu3Ri9Meb"
      },
      "source": [
        "We can use the standard confusion matrix to get a sense of how good the initial random model is, and see how that changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpGDcyOz9Q-U"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix( model, x, y ):\n",
        "  identification_counts = np.zeros( shape = (10,10), dtype = np.int32 )\n",
        "\n",
        "  logits = model( x )\n",
        "  predicted_classes = torch.argmax( logits, dim = 1 )\n",
        "\n",
        "  n = x.shape[0]\n",
        "\n",
        "  for i in range(n):\n",
        "    actual_class = int( y[i].item() )\n",
        "    predicted_class = predicted_classes[i].item()\n",
        "    identification_counts[actual_class, predicted_class] += 1\n",
        "\n",
        "  return identification_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwNlcq7_-ZSu",
        "outputId": "1e481252-4e0c-4179-bf98-47db88d0573d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
              "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
              "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
              "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
              "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
              "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
              "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
              "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
              "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
              "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0]],\n",
              "      dtype=int32)"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confusion_matrix( cifar_model, test_X, test_Y )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdB75lOI1sVg"
      },
      "outputs": [],
      "source": [
        "cnn_optimizer = optim.Adam(cifar_model.parameters(), lr = 0.01 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_uWsb1V1xZg"
      },
      "outputs": [],
      "source": [
        "loss_function = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya7iliGm136x",
        "outputId": "62fa21e3-ee6c-48c6-fbcf-b7be80cdc656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Test Loss: 2.3205106258392334\n"
          ]
        }
      ],
      "source": [
        "print(\"Initial Test Loss:\", loss_function( cifar_model( test_X ), test_Y ).item() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-dEK9Sr2BL6",
        "outputId": "3d527766-7950-4052-8724-9bb6ddf829fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Total Loss over Batches: 2.0620238712072374\n",
            "[[517  51  70  28   7  10  41  46 111 119]\n",
            " [ 53 465  52  32  35  46  47  51  46 173]\n",
            " [ 75  37 208  60  21  40 405  91  18  45]\n",
            " [ 44  36 116 153  33  78 396  76  15  53]\n",
            " [ 63  12 105  53  36  22 572  75  17  45]\n",
            " [ 35  60 131 127  33  93 393  83   8  37]\n",
            " [ 11  12  37  63  10  20 737  86   5  19]\n",
            " [ 30  44  87  53  53  24 297 297  13 102]\n",
            " [367  52  64  28   4  25  11  32 296 121]\n",
            " [ 74 148  81  45  31  12  59  87  58 405]]\n",
            "Average Total Loss over Batches: 1.7356353501462936\n",
            "[[505 135  72  19  30   7  19  22 129  62]\n",
            " [ 24 809   6   8   6   4   7  17  54  65]\n",
            " [ 73  65 214  54 141  51 256  84  22  40]\n",
            " [ 27  72  89 151 108  96 254 100  16  87]\n",
            " [ 38  39 115  40 254  34 320 113  18  29]\n",
            " [ 13  47 120 106 123 214 212 131   3  31]\n",
            " [  6  41  53  77  70  24 597  79   5  48]\n",
            " [ 20  54  61  37 124  40 104 471  10  79]\n",
            " [233 173  32  15   7   8  17  28 413  74]\n",
            " [ 40 394  12  27  16   6  23  29  86 367]]\n",
            "Average Total Loss over Batches: 1.515681339149475\n",
            "[[486  65  72  41  36   4  20  38 194  44]\n",
            " [ 21 664   6  38  20   4  23  21  78 125]\n",
            " [ 58  15 300 133 173  23 184  83  20  11]\n",
            " [ 21  18  61 383  93  47 250  64  23  40]\n",
            " [ 32   8 103 121 364  14 246  93  14   5]\n",
            " [ 11  15  79 292  78 185 194 128   8  10]\n",
            " [  5  14  31  98  61   8 723  39   4  17]\n",
            " [ 16  16  42 116 103  43 100 518  10  36]\n",
            " [126  91  27  33  12   4  20  24 602  61]\n",
            " [ 36 201  13  59  23   4  31  63  85 485]]\n",
            "Average Total Loss over Batches: 1.3500463881462812\n",
            "[[482  37  95  22  68   8  18  24 212  34]\n",
            " [ 38 658  13  20  31   9  12  19 144  56]\n",
            " [ 58   6 408  58 233  52  92  58  30   5]\n",
            " [ 15  19 116 251 191 144 154  66  27  17]\n",
            " [ 26   6 160  45 495  34 112  98  20   4]\n",
            " [  8   9 110 135 145 347 104 126  12   4]\n",
            " [  5   8  83  57 168  29 601  33  10   6]\n",
            " [ 16   6  69  44 142  87  38 566  15  17]\n",
            " [125  48  40  16  23  12  11  13 690  22]\n",
            " [ 72 202  19  36  49   9  26  58 116 413]]\n",
            "Average Total Loss over Batches: 1.237147360203266\n",
            "[[457  41 144  18  26  20  16  18 177  83]\n",
            " [ 23 644  14  23   9  20  16  10  55 186]\n",
            " [ 55  13 493  67  92 105  87  51  22  15]\n",
            " [ 16  13 149 328  70 192 114  47  15  56]\n",
            " [ 29  12 224  73 292  82 156  96  18  18]\n",
            " [  3   7 118 165  44 481  82  78   5  17]\n",
            " [  4  12  88  71  55  58 662  22   4  24]\n",
            " [ 13   7  75  68  46 143  50 517  12  69]\n",
            " [123  66  30  15   7  30  17  15 618  79]\n",
            " [ 52 140  21  30  12  13  26  24  29 653]]\n",
            "Average Total Loss over Batches: 1.1408498464325816\n",
            "[[580  34  47  20  12  23   5  10 192  77]\n",
            " [ 37 611   3  11   8   8   4   7  86 225]\n",
            " [113  24 330  78  91 163  65  72  34  30]\n",
            " [ 46  25  68 347  50 241  48  70  32  73]\n",
            " [ 74  22 121 104 286 109  92 124  40  28]\n",
            " [ 20  13  50 177  36 528  30  98  20  28]\n",
            " [ 24  23  49 101  61  99 541  41  11  50]\n",
            " [ 26   8  33  68  42 148  12 549  22  92]\n",
            " [116  63   9  12   4  17   5   9 696  69]\n",
            " [ 47 138   4  24   6  11  12  20  65 673]]\n",
            "Average Total Loss over Batches: 1.0539692684578896\n",
            "[[559  43  91  29  44  22   7  25 137  43]\n",
            " [ 37 674  11  17   7  15  12  14  68 145]\n",
            " [ 64  12 403  84 125 139  55  85  22  11]\n",
            " [ 28  20 110 353  61 221  60  88  21  38]\n",
            " [ 32   7 161 104 343 103  85 139  19   7]\n",
            " [ 11  14  77 183  47 503  39 103   9  14]\n",
            " [ 11  18  59  90  93  77 574  50  10  18]\n",
            " [ 21   9  61  69  61 137  15 580  12  35]\n",
            " [140  67  26  14  14  27   9   9 651  43]\n",
            " [ 61 151  12  29  14  19  15  41  76 582]]\n",
            "Average Total Loss over Batches: 0.9844606054903566\n",
            "[[658  18  65  15  52  10  22   5 112  43]\n",
            " [ 66 636  17  14  18   9  23   8  69 140]\n",
            " [104  10 399  36 251  45 106  27  15   7]\n",
            " [ 46  15 134 214 184 122 196  41  21  27]\n",
            " [ 50   7 111  35 568  30 142  37  15   5]\n",
            " [ 25   9 140 134 137 352 122  64  10   7]\n",
            " [ 13   6  55  24 143  15 723  11   7   3]\n",
            " [ 43   6  83  47 202  82  55 444   8  30]\n",
            " [193  47  26  12  23   8  20   3 624  44]\n",
            " [ 83 142  23  19  36   8  47  22  56 564]]\n",
            "Average Total Loss over Batches: 0.9290413379179686\n",
            "[[614  31  79  44  31  11  19  16  93  62]\n",
            " [ 52 652   9  26  11  12  17  12  57 152]\n",
            " [ 85  19 398 105 131  65 100  65  14  18]\n",
            " [ 34  26 100 398  79 107 134  72  16  34]\n",
            " [ 58  12 134  96 390  36 156  97  16   5]\n",
            " [ 19  15  99 228  63 370  80 111   6   9]\n",
            " [ 14  18  65  73  69  35 684  29   5   8]\n",
            " [ 31   8  60  80  84  84  40 567   6  40]\n",
            " [195  64  20  29  13  13  17  16 573  60]\n",
            " [ 74 161  16  31  12  16  24  40  50 576]]\n",
            "Average Total Loss over Batches: 0.8706873266878724\n",
            "[[590  24  82  40  31  11  11  13 134  64]\n",
            " [ 41 606  15  19  12  14  12  12  85 184]\n",
            " [ 84   8 397 127 139  90  58  60  21  16]\n",
            " [ 23  16  89 462  84 136  71  63  25  31]\n",
            " [ 36   5 166 126 395  60  95  86  24   7]\n",
            " [ 14  10  90 259  62 407  31 100  13  14]\n",
            " [ 11   8  64 100 122  57 586  33   9  10]\n",
            " [ 22   3  54 103  93 102  23 548  12  40]\n",
            " [133  51  31  26  14  12  11   7 662  53]\n",
            " [ 55 125  19  37  15  23  20  35  61 610]]\n",
            "Average Total Loss over Batches: 0.8323294417085685\n",
            "[[530  43  80  42  42  14  14  16 161  58]\n",
            " [ 47 672  11  20   8   9  12  10  73 138]\n",
            " [ 66  13 350 126 175  74  83  68  24  21]\n",
            " [ 19  21  83 417 113 122  86  82  17  40]\n",
            " [ 32   9 105  97 439  50 130  98  27  13]\n",
            " [ 16  15  76 258  85 359  49 111   9  22]\n",
            " [  9  15  41 116 112  42 614  30   6  15]\n",
            " [ 23  11  47 105 131  74  31 528   8  42]\n",
            " [108  68  22  27  24  13  11  13 650  64]\n",
            " [ 44 149  16  35  21   9  24  42  60 600]]\n",
            "Average Total Loss over Batches: 0.7884573312246055\n",
            "[[583  33  49  28  45  32  14  17 146  53]\n",
            " [ 40 636   8  26  12  21  13  12  60 172]\n",
            " [ 80  11 266 102 167 168  94  70  20  22]\n",
            " [ 30  18  52 355  93 235  92  63  19  43]\n",
            " [ 40  10  71 106 406 114 126  94  17  16]\n",
            " [ 26  12  46 201  72 490  37  90  11  15]\n",
            " [  8  12  22 103 100  86 607  31  11  20]\n",
            " [ 23  11  27  77 112 136  34 511  12  57]\n",
            " [140  78   6  27  21  19   9  16 608  76]\n",
            " [ 51 156   5  25  18  19  21  34  62 609]]\n",
            "Average Total Loss over Batches: 0.752612078763172\n",
            "[[578  32 112  25  28  24  11  12 156  22]\n",
            " [ 60 667  20  23   9  16  12  13  81  99]\n",
            " [ 79   7 444  72  97 122  88  58  23  10]\n",
            " [ 26  21 146 318  66 226  97  50  23  27]\n",
            " [ 38  16 203  73 320 107 130  86  22   5]\n",
            " [ 17  11 116 182  48 471  51  83  15   6]\n",
            " [ 10  18  81  80  61  64 640  33   7   6]\n",
            " [ 31  14  84  72  84 145  41 493   9  27]\n",
            " [143  66  45  22   9  13  13  13 646  30]\n",
            " [ 84 194  38  30  13  30  27  39  80 465]]\n",
            "Average Total Loss over Batches: 0.7094468093735352\n",
            "[[571  36  75  31  31  14  11  13 185  33]\n",
            " [ 60 605  12  26  13  12  13  17 102 140]\n",
            " [ 76  10 384  79 188  76  74  58  35  20]\n",
            " [ 28  15 117 330 131 141 101  55  49  33]\n",
            " [ 41   6 132  67 493  45 100  73  36   7]\n",
            " [ 22   8 105 197  88 380  50 103  32  15]\n",
            " [ 13  14  57  91 156  38 576  24  17  14]\n",
            " [ 28   7  82  73 140  99  33 495  15  28]\n",
            " [119  59  24  18  25   8   9  11 693  34]\n",
            " [ 73 144  27  28  24  28  22  35 103 516]]\n",
            "Average Total Loss over Batches: 0.6849050540210213\n",
            "[[484  55 120  28  55  20  11  18 157  52]\n",
            " [ 34 652  15  15   9   8  11   8  82 166]\n",
            " [ 51  15 400  73 185  80  74  73  28  21]\n",
            " [ 18  28 114 276 135 175  91  78  36  49]\n",
            " [ 26  12 168  61 439  60 106  87  27  14]\n",
            " [ 15  11 104 170  98 386  42 118  20  36]\n",
            " [ 10  15  60  82 155  42 560  39  12  25]\n",
            " [ 22  14  74  62 130  84  35 512  14  53]\n",
            " [ 85  71  34  18  27  11   9  11 678  56]\n",
            " [ 46 178  13  13  18  16  15  30  60 611]]\n",
            "Average Total Loss over Batches: 0.6640509117172286\n",
            "[[653  33  56  44  27  27   7  14 100  39]\n",
            " [ 60 577  16  33  18  24  14  14  85 159]\n",
            " [ 99   7 335 118 126 152  62  63  24  14]\n",
            " [ 33  16  94 407  73 212  77  46  21  21]\n",
            " [ 44   8 137 129 367 103  99  90  17   6]\n",
            " [ 13   8  86 210  60 476  35  83  17  12]\n",
            " [ 11  11  59 139 108  72 549  30  11  10]\n",
            " [ 29   8  64 104 104 140  29 482  10  30]\n",
            " [212  65  15  33  17  20   8  14 570  46]\n",
            " [ 75 135  22  38  20  37  21  37  61 554]]\n",
            "Average Total Loss over Batches: 0.6335089702185616\n",
            "[[552  35  86  25  27  19   8  19 182  47]\n",
            " [ 50 619  21  12  13  11  15   9  95 155]\n",
            " [ 80  13 410  73 139  86  65  76  37  21]\n",
            " [ 40  28 138 279  99 169  90  80  44  33]\n",
            " [ 41  17 189  57 379  56  98 113  40  10]\n",
            " [ 21  11 129 161  74 394  49 119  19  23]\n",
            " [ 11  17  84  87 108  54 556  46  18  19]\n",
            " [ 29   8  83  56 122  87  35 516  13  51]\n",
            " [112  71  20  11  17  10  10  13 679  57]\n",
            " [ 65 158  21  13  16  14  18  37  73 585]]\n",
            "Average Total Loss over Batches: 0.6235797348084534\n",
            "[[589  39  52  47  48  20   4  18 134  49]\n",
            " [ 42 572  11  24  27  22   8  20  61 213]\n",
            " [ 81   6 302  99 219 144  41  73  16  19]\n",
            " [ 32  13  91 368 132 214  38  64  16  32]\n",
            " [ 35   7  92 106 513  80  46  94  19   8]\n",
            " [ 15   7  84 216 105 437  18  87  13  18]\n",
            " [  8  18  54 159 211  92 391  44  10  13]\n",
            " [ 23   5  52  92 152 124  14 494  10  34]\n",
            " [169  73  15  28  42  16   6  23 550  78]\n",
            " [ 62 143  20  37  32  24  10  55  60 557]]\n",
            "Average Total Loss over Batches: 0.5956593477013568\n",
            "[[605  26  96  35  35  13  16  11 132  31]\n",
            " [ 80 567  24  24  25  15  32  13  92 128]\n",
            " [ 89   6 388  80 125  95 146  50  14   7]\n",
            " [ 36  17 139 293  86 182 166  44  21  16]\n",
            " [ 39   6 193  74 340  53 193  83  12   7]\n",
            " [ 26   6 131 183  63 390 105  72  17   7]\n",
            " [  9   8  67  47  73  54 711  18   9   4]\n",
            " [ 38   5 105  62 113 113  75 456   9  24]\n",
            " [212  56  34  21  23  11  23  14 568  38]\n",
            " [100 144  26  31  15  33  48  47  82 474]]\n",
            "Average Total Loss over Batches: 0.5678827269640752\n",
            "[[539  45  85  23  31  19   6  18 187  47]\n",
            " [ 31 610  14  18  12  15   8  12  85 195]\n",
            " [ 72  13 395  83 150  86  64  86  32  19]\n",
            " [ 25  26 127 325  99 182  76  68  35  37]\n",
            " [ 38  13 157  81 391  67  97 113  28  15]\n",
            " [ 23  13 123 187  69 396  42 105  21  21]\n",
            " [ 10  22  79 111 126  55 529  36  16  16]\n",
            " [ 32  13  75  69 118 100  26 503  16  48]\n",
            " [108  74  28  14  20  15   7  15 646  73]\n",
            " [ 56 167  19  20  15  20  20  33  73 577]]\n",
            "Average Total Loss over Batches: 0.5606566603068309\n",
            "[[568  37  82  21  25  15  17  13 178  44]\n",
            " [ 43 620  16  14   8  14  24   5  96 160]\n",
            " [ 86  11 399  72  97 105 117  62  34  17]\n",
            " [ 29  19 137 294  70 199 138  46  41  27]\n",
            " [ 45  17 191  84 287  74 180  83  29  10]\n",
            " [ 23  12 130 177  53 414  85  77  18  11]\n",
            " [ 11   9  62  73  56  53 689  21  12  14]\n",
            " [ 34  14  84  77  87 116  70 450  15  53]\n",
            " [137  67  17  16  19  14  19  13 639  59]\n",
            " [ 60 166  15  17  10  24  36  20  86 566]]\n",
            "Average Total Loss over Batches: 0.5468341836264636\n",
            "[[550  39  92  47  43  24  13  28 124  40]\n",
            " [ 45 593  22  30  29  23  16  15  65 162]\n",
            " [ 60   7 400 104 162 106  64  72  11  14]\n",
            " [ 17  19 147 343  95 200  80  60  11  28]\n",
            " [ 26   9 168  84 426  78  99  89  13   8]\n",
            " [ 11   5 117 218  86 398  37 102  12  14]\n",
            " [  6  11  89 115 117  69 541  34   6  12]\n",
            " [ 24   9  81  88 134 122  28 471   4  39]\n",
            " [149  71  43  32  35  19  17  16 551  67]\n",
            " [ 61 151  25  32  20  36  25  45  46 559]]\n",
            "Average Total Loss over Batches: 0.5272674289518315\n",
            "[[612  48  96  27  20   9   9  22 110  47]\n",
            " [ 56 673  15  15  11  12  16  16  59 127]\n",
            " [ 96  15 401  76 124  79  99  78  18  14]\n",
            " [ 41  24 155 311  86 158 101  78  21  25]\n",
            " [ 49  23 160  77 338  64 142 122  20   5]\n",
            " [ 35  15 140 211  67 321  63 109  20  19]\n",
            " [ 14  27  85  83  82  49 597  41   8  14]\n",
            " [ 37  20  95  73  89  90  35 508  10  43]\n",
            " [185 110  41  18  17  10   7  16 535  61]\n",
            " [ 80 196  32  33  11  15  19  52  48 514]]\n",
            "Average Total Loss over Batches: 0.5173126440334367\n",
            "[[642  51  73  16  20  12  11  10 129  36]\n",
            " [ 56 670  18  10   6  14  12   7  71 136]\n",
            " [122  24 401  68  99  96  69  57  38  26]\n",
            " [ 52  43 158 269  82 182  70  65  42  37]\n",
            " [ 83  29 205  70 293  83  98  99  23  17]\n",
            " [ 49  23 147 135  52 402  42  97  28  25]\n",
            " [ 26  54 108  95  78  81 475  42  19  22]\n",
            " [ 74  24  95  70  85 108  35 428  18  63]\n",
            " [227  88  29   7  11  12   9  11 544  62]\n",
            " [ 92 229  17  14   7  24  21  22  78 496]]\n",
            "Average Total Loss over Batches: 0.49817696274404416\n",
            "[[530  36  82  65  34  26   9  27 129  62]\n",
            " [ 52 569  23  43  17  32  14  23  73 154]\n",
            " [ 64   7 321 164 126 149  54  85  16  14]\n",
            " [ 21  19 104 413  68 220  48  74   9  24]\n",
            " [ 33  10 121 169 319 118  93 116  14   7]\n",
            " [ 13   6  79 243  51 471  21  97   8  11]\n",
            " [  8   8  52 204  97 101 473  44   4   9]\n",
            " [ 25  12  53 122  78 153  23 490   6  38]\n",
            " [152  69  28  52  19  23  10  23 536  88]\n",
            " [ 58 138  19  56  10  38  32  58  47 544]]\n",
            "Average Total Loss over Batches: 0.4856424541596533\n",
            "[[526  35  81  33  31  17  15  20 176  66]\n",
            " [ 53 548  24  17  22  19  14  14  96 193]\n",
            " [ 80  11 390  74 145 101  78  67  31  23]\n",
            " [ 31  10 155 298 112 177  92  60  32  33]\n",
            " [ 36  11 163  72 404  70 107 102  23  12]\n",
            " [ 14   7 127 197  84 381  51  99  23  17]\n",
            " [  8  12  90  85 129  56 544  45  15  16]\n",
            " [ 20   8  84  91 121 106  37 479  13  41]\n",
            " [117  53  30  14  23  15   9  14 652  73]\n",
            " [ 50 134  20  22  15  25  23  42  77 592]]\n",
            "Average Total Loss over Batches: 0.47609716006200525\n",
            "[[553  49  71  32  35  21  15  16 161  47]\n",
            " [ 58 636  14  20  15  17  15  10  71 144]\n",
            " [ 81  13 342  84 184 105  82  63  29  17]\n",
            " [ 27  24 124 290 107 196  95  64  37  36]\n",
            " [ 41  13 119  67 452  80 115  79  26   8]\n",
            " [ 20  10 113 188  88 395  55  90  20  21]\n",
            " [ 10  17  66  92 140  73 551  31   5  15]\n",
            " [ 36  14  64  67 127 117  41 485  15  34]\n",
            " [138  72  26  17  24  15  14  11 627  56]\n",
            " [ 74 172  18  24  18  22  26  33  77 536]]\n",
            "Average Total Loss over Batches: 0.46317585070680944\n",
            "[[520  50  76  33  29  20  27  12 190  43]\n",
            " [ 56 623  28  15  15  17  23   6  88 129]\n",
            " [ 84  16 354  83 150 119 112  41  25  16]\n",
            " [ 31  22 121 293 101 226 108  38  29  31]\n",
            " [ 47  14 122  75 371  98 185  60  21   7]\n",
            " [ 19  13 110 192  71 436  63  60  19  17]\n",
            " [  6  15  70  95  89  79 613  19   7   7]\n",
            " [ 37  12  77  76 130 138  58 418  18  36]\n",
            " [126  76  22  14  27  21  21   8 635  50]\n",
            " [ 74 161  16  24  15  30  48  30  88 514]]\n",
            "Average Total Loss over Batches: 0.4586766338126187\n",
            "[[552  40  62  45  32  31  15  16 162  45]\n",
            " [ 46 593  21  25  20  22  21  14 100 138]\n",
            " [ 69  11 318 102 164 131  86  76  26  17]\n",
            " [ 21  23 114 334  72 213  97  72  27  27]\n",
            " [ 39  13 118  81 390  93 135 104  20   7]\n",
            " [ 14   8  99 196  72 439  49  96  15  12]\n",
            " [  8  19  58 102 101  72 591  32   8   9]\n",
            " [ 31  11  56  82 100 138  36 497  14  35]\n",
            " [133  72  22  24  31  26  11  13 616  52]\n",
            " [ 68 165  14  44  16  35  35  43  76 504]]\n",
            "Average Total Loss over Batches: 0.4456144446272834\n",
            "[[523  44  75  35  31  22   8  16 188  58]\n",
            " [ 38 595  10  15  18   9  13  18  81 203]\n",
            " [ 81  17 299 113 200  82  60  96  30  22]\n",
            " [ 29  27 108 321 113 162  75  88  31  46]\n",
            " [ 34  15 101  77 466  57  92 123  24  11]\n",
            " [ 22  11  88 218  98 356  36 125  19  27]\n",
            " [  9  16  57 109 148  70 515  48   9  19]\n",
            " [ 23  17  59  77 114 105  22 522  13  48]\n",
            " [101  71  23  22  26  12   8  21 640  76]\n",
            " [ 52 148  10  26  16  21  18  40  62 607]]\n",
            "Average Total Loss over Batches: 0.44018194000564514\n",
            "[[499  32  77  32  49  19  17  24 196  55]\n",
            " [ 38 559  21  16  29  22  15  20  95 185]\n",
            " [ 63  10 345  89 187  96  75  86  27  22]\n",
            " [ 22  14 120 288 120 193  97  76  35  35]\n",
            " [ 26  10 122  65 417  69 130 121  27  13]\n",
            " [ 13   6 102 193  81 401  49 115  22  18]\n",
            " [  5  18  76  77 119  65 571  41  12  16]\n",
            " [ 21   9  62  63 121 130  39 500  14  41]\n",
            " [110  56  25  19  42  16  13  16 641  62]\n",
            " [ 59 149  21  25  22  26  28  46  70 554]]\n",
            "Average Total Loss over Batches: 0.42799650131597883\n",
            "[[500  51  81  31  28  19  13  12 203  62]\n",
            " [ 39 618  19  25  10  17  17  11  94 150]\n",
            " [ 70  14 346 123 123 111  83  77  32  21]\n",
            " [ 23  23 124 340  73 190  78  68  32  49]\n",
            " [ 50  19 133 116 319  93 123 102  28  17]\n",
            " [ 22  15  99 226  61 398  39  95  23  22]\n",
            " [  8  21  81 145  90  62 532  37  12  12]\n",
            " [ 28  10  59  86  99 118  38 487  20  55]\n",
            " [ 94  83  26  25  25  16   9  11 650  61]\n",
            " [ 51 164  14  27   8  31  34  44  72 555]]\n",
            "Average Total Loss over Batches: 0.4239482973549864\n",
            "[[527  35 102  49  45  22  22  19 147  32]\n",
            " [ 64 580  34  29  32  16  38   9  77 121]\n",
            " [ 63   7 374  77 161  98 137  54  21   8]\n",
            " [ 18  21 143 285 122 155 162  62  21  11]\n",
            " [ 33  13 140  57 387  61 220  67  17   5]\n",
            " [ 21  10 136 194 114 336  97  74  10   8]\n",
            " [  6   6  65  51 105  35 700  20   6   6]\n",
            " [ 22   7  79  83 161  98  82 437   6  25]\n",
            " [151  62  41  25  37  20  26  14 579  45]\n",
            " [ 76 156  26  40  32  35  55  47  69 464]]\n",
            "Average Total Loss over Batches: 0.40996652247451476\n",
            "[[543  47  77  34  35  23   7  21 154  59]\n",
            " [ 39 582  15  19  25  14  15  14  74 203]\n",
            " [ 78  12 307 113 160 117  68  91  27  27]\n",
            " [ 37  19  96 322  96 201  78  75  31  45]\n",
            " [ 38  14 114 102 368  93 107 121  25  18]\n",
            " [ 24   9  94 208  78 394  37 107  25  24]\n",
            " [  4  20  65 112 122  79 514  46  13  25]\n",
            " [ 29  20  55  77 111 121  27 495  18  47]\n",
            " [144  89  24  16  32  17   9  16 574  79]\n",
            " [ 54 165  14  23  19  21  20  43  67 574]]\n",
            "Average Total Loss over Batches: 0.4075290298316366\n",
            "[[511  39  74  26  35  31  12  20 206  46]\n",
            " [ 51 588  19  17  23  15  15  18  86 168]\n",
            " [ 81  14 328  66 168 120  89  79  38  17]\n",
            " [ 28  26 127 241 111 217 108  75  44  23]\n",
            " [ 44  15 136  50 392  86 136 109  25   7]\n",
            " [ 24  12 104 157  80 415  54 108  32  14]\n",
            " [  8  15  85  75 131  71 553  37  14  11]\n",
            " [ 31  13  65  61 129 123  38 492  17  31]\n",
            " [122  74  26  14  28  19  17  10 633  57]\n",
            " [ 66 160  21  31  22  28  30  47  85 510]]\n",
            "Average Total Loss over Batches: 0.40225220636280895\n",
            "[[620  49  67  31  22  17   8  13 134  39]\n",
            " [ 81 599  16  22  14  12  11  15  84 146]\n",
            " [105  13 329 133 143  90  53  78  34  22]\n",
            " [ 64  24 117 342  90 168  54  72  33  36]\n",
            " [ 65  17 118 119 365  76  94 106  27  13]\n",
            " [ 33  16 100 237  75 374  24  98  25  18]\n",
            " [ 22  25  74 145 142  61 456  40  15  20]\n",
            " [ 48  18  55 106 103 109  23 483  16  39]\n",
            " [188  81  22  19  18  15   6  16 582  53]\n",
            " [ 95 178  16  31  11  31  22  37  62 517]]\n",
            "Average Total Loss over Batches: 0.39036305899612606\n",
            "[[503  36  79  33  39  20   9  15 207  59]\n",
            " [ 43 585  15  25  19  17  20  16  79 181]\n",
            " [ 77   7 327 120 154 117  63  75  38  22]\n",
            " [ 37  23  97 324  99 197  76  72  39  36]\n",
            " [ 35  17 123 115 358  91 101 123  27  10]\n",
            " [ 22  10  97 222  76 397  32  94  31  19]\n",
            " [  8  19  67 127 127  75 502  43  17  15]\n",
            " [ 30  15  53  91 119 116  29 481  17  49]\n",
            " [115  76  23  14  29  17   8  13 634  71]\n",
            " [ 58 133  12  33  17  23  21  45  77 581]]\n",
            "Average Total Loss over Batches: 0.39095018498080664\n",
            "[[491  68  66  37  41  22  17  22 168  68]\n",
            " [ 37 612  12  17  18  17  12  22  78 175]\n",
            " [ 79  24 287  99 140 125  85  95  36  30]\n",
            " [ 25  35 106 323  84 199  77  87  22  42]\n",
            " [ 39  28 101 102 318 101 128 135  31  17]\n",
            " [ 23  20  86 183  59 430  40 115  18  26]\n",
            " [ 10  33  56 110  80  74 543  57  12  25]\n",
            " [ 32  22  43  86  92 127  41 486  14  57]\n",
            " [138  89  18  23  30  22  13  18 564  85]\n",
            " [ 49 176  11  25   6  36  28  44  60 565]]\n",
            "Average Total Loss over Batches: 0.38535476689969944\n",
            "[[477  38 103  43  50  35  19  28 143  64]\n",
            " [ 44 550  19  25  22  23  14  23  64 216]\n",
            " [ 54   9 347 108 180 109  65  89  22  17]\n",
            " [ 22  18 130 332 106 183  78  79  17  35]\n",
            " [ 32  11 134  87 417  79 107 113  13   7]\n",
            " [ 17   8 117 211  74 402  34 111  12  14]\n",
            " [  8  15  84 112 149  68 501  38   9  16]\n",
            " [ 19   8  62  93 137 114  31 498   7  31]\n",
            " [127  73  43  37  34  35  13  17 531  90]\n",
            " [ 57 123  14  33  27  32  27  44  54 589]]\n",
            "Average Total Loss over Batches: 0.37797348918741014\n",
            "[[546  35  65  34  33  35   8  18 162  64]\n",
            " [ 54 552  18  19  21  16  14  16  70 220]\n",
            " [ 84  14 339  90 157 130  62  73  29  22]\n",
            " [ 32  20 118 293 107 209  76  74  36  35]\n",
            " [ 39  20 142  82 386  86  90 125  17  13]\n",
            " [ 23  13 110 194  74 412  35 107  18  14]\n",
            " [ 13  19  84 104 140  87 467  51  17  18]\n",
            " [ 32  16  64  73 118 123  28 488  15  43]\n",
            " [144  72  30  16  31  24  13  14 569  87]\n",
            " [ 64 151  16  28  20  28  23  39  65 566]]\n",
            "Average Total Loss over Batches: 0.37186214225053205\n",
            "[[513  40  96  43  20  18  11  18 189  52]\n",
            " [ 47 587  19  24  28  15  18  13  78 171]\n",
            " [ 77  11 344 117 166  98  70  67  35  15]\n",
            " [ 28  22 134 331  91 186  88  45  44  31]\n",
            " [ 39  18 148  91 381  72 119  93  26  13]\n",
            " [ 24   9 126 218  71 380  53  74  33  12]\n",
            " [  7  21  85  93 134  64 530  36  16  14]\n",
            " [ 30  18  80 102 127 107  41 452  12  31]\n",
            " [121  70  31  16  28  15  15  10 630  64]\n",
            " [ 61 174  19  33  16  29  24  30  78 536]]\n",
            "Average Total Loss over Batches: 0.3683549063643732\n",
            "[[513  58  62  31  42  16  13  21 179  65]\n",
            " [ 33 620  14  12  21   9  14  10  71 196]\n",
            " [ 82  17 257  93 218  92  68 103  38  32]\n",
            " [ 28  37  90 272 131 157  80  94  38  73]\n",
            " [ 34  27  71  63 462  59 109 117  28  30]\n",
            " [ 25  22  81 182 110 339  38 128  31  44]\n",
            " [  8  46  46  93 167  61 470  51  16  42]\n",
            " [ 35  34  34  70 148  74  32 491  17  65]\n",
            " [128  93  16  16  31  14   8  11 595  88]\n",
            " [ 50 173  10  16  15  12  17  33  60 614]]\n",
            "Average Total Loss over Batches: 0.35830499648266384\n",
            "[[576  42  86  40  29  35   3  11 141  37]\n",
            " [ 66 577  23  32  21  20  11  12  85 153]\n",
            " [ 83  11 352 115 137 131  54  71  28  18]\n",
            " [ 34  17 121 347  94 222  60  61  23  21]\n",
            " [ 50  13 155 121 349 109  85  93  19   6]\n",
            " [ 27   9 105 233  61 436  27  75  17  10]\n",
            " [  9  17  84 155 114 111 436  45  13  16]\n",
            " [ 43  21  69  98 117 135  30 444  13  30]\n",
            " [191  58  36  22  26  25   9   8 568  57]\n",
            " [ 88 179  20  30  15  30  19  32  76 511]]\n",
            "Average Total Loss over Batches: 0.36024263348004143\n",
            "[[462  40  83  40  48  32  34  22 171  68]\n",
            " [ 39 595  15  18  23  16  31   8  71 184]\n",
            " [ 60  11 324  68 163 110 141  70  29  24]\n",
            " [ 24  27 120 261  96 173 178  52  31  38]\n",
            " [ 32  16 134  56 365  64 209  89  15  20]\n",
            " [ 21  17 117 167  82 359 111  85  20  21]\n",
            " [  6  13  63  63  88  44 670  25  11  17]\n",
            " [ 23  16  69  56 140 110  76 452  14  44]\n",
            " [120  87  27  16  40  19  24  10 568  89]\n",
            " [ 54 154  15  23  16  28  43  23  58 586]]\n",
            "Average Total Loss over Batches: 0.3593408929267913\n",
            "[[560  36  70  29  26  17   6  19 200  37]\n",
            " [ 78 546  26  18  30  17  12  22 138 113]\n",
            " [102  11 324  97 171  88  49  94  52  12]\n",
            " [ 50  19 130 292 106 180  60  91  51  21]\n",
            " [ 65  12 129  81 389  70  80 131  35   8]\n",
            " [ 37  10 112 213  80 355  27 120  34  12]\n",
            " [ 22  16  96 120 149  80 434  47  25  11]\n",
            " [ 45  17  66  55 124  98  18 528  24  25]\n",
            " [144  58  23  18  30  12   5  17 662  31]\n",
            " [ 99 156  21  27  20  33  20  58 129 437]]\n",
            "Average Total Loss over Batches: 0.34725357603253915\n",
            "[[539  34  69  33  52  33  15  18 154  53]\n",
            " [ 51 562  14  24  38  17  21  18  73 182]\n",
            " [ 84   9 287 104 196 119  74  87  22  18]\n",
            " [ 27  20 111 283 133 206  97  76  21  26]\n",
            " [ 37  13  98  74 436  89 122 110  13   8]\n",
            " [ 24   5  96 185 101 420  41  95  18  15]\n",
            " [ 10  14  64  88 135  76 555  38   8  12]\n",
            " [ 26  16  58  68 142 126  37 485  11  31]\n",
            " [156  68  19  22  38  25  10  20 575  67]\n",
            " [ 72 127  17  31  25  33  33  45  72 545]]\n",
            "Average Total Loss over Batches: 0.34329252795097737\n",
            "[[546  49  79  34  20  37   6  24 144  61]\n",
            " [ 55 532  19  32  21  21  12  25  78 205]\n",
            " [ 77  11 307 145 110 159  32 105  25  29]\n",
            " [ 27  14 107 350  59 245  47  91  25  35]\n",
            " [ 46  11 128 147 269 137  63 165  18  16]\n",
            " [ 24  10  76 230  39 446  21 123  18  13]\n",
            " [ 10  22  86 186 107 129 347  68  12  33]\n",
            " [ 28  14  47  84  74 146  19 526  17  45]\n",
            " [162  69  27  31  15  26   8  22 554  86]\n",
            " [ 60 146  14  31  10  49  14  45  68 563]]\n",
            "Average Total Loss over Batches: 0.3367560525835489\n",
            "[[525  27 115  45  30  22  17  15 155  49]\n",
            " [ 56 531  32  41  35  16  24  15  95 155]\n",
            " [ 62   8 365 129 147  86  95  74  18  16]\n",
            " [ 28  14 147 349  92 163  99  55  29  24]\n",
            " [ 35  11 138 115 351  72 140 117  13   8]\n",
            " [ 21   8 127 230  67 360  61  90  21  15]\n",
            " [  8   7  99 102  98  58 578  27  11  12]\n",
            " [ 27   6  85  96 122  99  45 474  13  33]\n",
            " [165  47  38  36  33  22  18  14 566  61]\n",
            " [ 87 136  27  41  18  30  36  44  71 510]]\n",
            "Average Total Loss over Batches: 0.3405771225663304\n",
            "[[540  40  75  43  35  23  23  19 159  43]\n",
            " [ 51 564  25  34  30  10  33  27  78 148]\n",
            " [ 73  10 307 119 140 123 111  74  28  15]\n",
            " [ 31  13 121 327  75 180 122  76  31  24]\n",
            " [ 38  12 114  86 327  89 190 118  18   8]\n",
            " [ 20   7 101 211  62 390  71  96  24  18]\n",
            " [  9   7  83  92  82  62 615  34   9   7]\n",
            " [ 30  15  69  80  97 117  54 498  12  28]\n",
            " [155  64  34  26  31  23  20  15 578  54]\n",
            " [ 81 159  22  40  14  35  39  39  70 501]]\n",
            "Average Total Loss over Batches: 0.3308187267679626\n",
            "[[533  43  87  28  34  21  20  31 132  71]\n",
            " [ 38 580  18  21  21   9  17  14  60 222]\n",
            " [ 76  10 330  88 160  87 104  96  24  25]\n",
            " [ 32  20 124 259 104 162 130  93  26  50]\n",
            " [ 37  15 131  64 383  54 158 118  19  21]\n",
            " [ 27  13 117 183  87 351  72 109  16  25]\n",
            " [ 10  11  74  75 119  49 587  42  11  22]\n",
            " [ 34  16  62  59 134  89  43 505  11  47]\n",
            " [141  95  39  11  34  20  15  21 535  89]\n",
            " [ 60 160  12  19  16  19  27  37  46 604]]\n",
            "Average Total Loss over Batches: 0.32806763285991486\n",
            "[[555  49  90  31  39  19   8  14 131  64]\n",
            " [ 41 584  19  25  22  11  17  11  59 211]\n",
            " [ 94  15 360  90 169  88  66  66  21  31]\n",
            " [ 31  23 155 298 112 161  78  62  26  54]\n",
            " [ 43  19 152  99 372  70 110  97  15  23]\n",
            " [ 25  11 138 213  90 350  44  73  21  35]\n",
            " [ 11  19 103 105 127  64 503  30   9  29]\n",
            " [ 32  25  76  80 138 100  30 445  12  62]\n",
            " [165  95  34  13  34  16  11   8 538  86]\n",
            " [ 66 155  16  24  14  20  19  26  47 613]]\n",
            "Average Total Loss over Batches: 0.33233933119244413\n",
            "[[523  56 101  33  32  38  14  20 137  46]\n",
            " [ 46 607  23  30  26  15  21  20  73 139]\n",
            " [ 70  13 329 104 144 115  97  89  20  19]\n",
            " [ 27  26 128 294  90 197 103  85  27  23]\n",
            " [ 33  14 120  96 338  95 149 134  12   9]\n",
            " [ 26  10 104 200  58 406  59 107  19  11]\n",
            " [  8  21  85 100 108  56 564  40  11   7]\n",
            " [ 26  18  72  74 105 119  38 509  12  27]\n",
            " [148  98  42  21  30  23  12  15 553  58]\n",
            " [ 71 186  23  32  19  28  32  48  68 493]]\n",
            "Average Total Loss over Batches: 0.31859924435463277\n",
            "[[570  31  94  33  24  28   9  19 130  62]\n",
            " [ 72 525  27  30  23  14  21  22  69 197]\n",
            " [ 87   8 325 121 153 125  57  85  19  20]\n",
            " [ 39  11 123 308  94 214  71  81  24  35]\n",
            " [ 47   9 130 109 353 105  99 121  13  14]\n",
            " [ 28   7 116 209  58 414  37  97  21  13]\n",
            " [ 13  10  99 124 133  92 448  51   9  21]\n",
            " [ 34   9  71  73 105 124  30 504  12  38]\n",
            " [200  57  43  21  27  21  11  16 521  83]\n",
            " [ 81 126  25  30  16  28  23  51  73 547]]\n",
            "Average Total Loss over Batches: 0.31982530189616454\n",
            "[[559  37  88  27  17  22   8  20 171  51]\n",
            " [ 54 585  22  24  20   8  17  22  79 169]\n",
            " [ 99  13 417  71 119  87  68  82  29  15]\n",
            " [ 45  19 176 257  85 184  85  91  33  25]\n",
            " [ 52  18 211  68 289  71 119 143  19  10]\n",
            " [ 31  13 161 169  55 359  51 119  26  16]\n",
            " [ 11  14 125  90  90  74 525  42  14  15]\n",
            " [ 34  16  91  57  94 103  35 510  17  43]\n",
            " [163  68  47  12  19  15   9  19 574  74]\n",
            " [ 64 163  26  22  11  31  23  50  87 523]]\n",
            "Average Total Loss over Batches: 0.32177053103883607\n",
            "[[561  38 106  21  28  27  10  16 142  51]\n",
            " [ 50 554  16  27  30  15  13  27  77 191]\n",
            " [ 98  13 379  87 151  84  53  82  23  30]\n",
            " [ 39  19 177 258 101 178  81  87  27  33]\n",
            " [ 56  13 154  69 375  71 106 124  16  16]\n",
            " [ 29  17 146 178  73 370  33 111  20  23]\n",
            " [ 15  26  97  98 122  74 475  55  16  22]\n",
            " [ 35  17  81  63 127  96  31 500  13  37]\n",
            " [177  68  49  13  31  16   9  19 543  75]\n",
            " [ 73 170  27  16  16  25  18  45  71 539]]\n",
            "Average Total Loss over Batches: 0.31086061250868813\n",
            "[[521  67  86  30  23  24  13  17 173  46]\n",
            " [ 60 642  18  20  12  13  14  14  69 138]\n",
            " [ 92  20 314 112 136 102  67  97  37  23]\n",
            " [ 41  38 111 312  84 168  79  99  38  30]\n",
            " [ 52  21 147 102 315  68 122 138  21  14]\n",
            " [ 38  19 113 204  50 358  39 128  29  22]\n",
            " [ 12  36  74 119 104  82 484  54  13  22]\n",
            " [ 40  25  61  76  89  98  32 516  14  49]\n",
            " [143 106  33  16  16  18  11  14 585  58]\n",
            " [ 79 203  12  31  11  30  20  44  67 503]]\n",
            "Average Total Loss over Batches: 0.30425573252855914\n",
            "[[573  52  87  23  34  25   8  11 142  45]\n",
            " [ 59 614  21  23  30  11  18  17  75 132]\n",
            " [ 95  14 335  83 164  98  83  80  30  18]\n",
            " [ 40  24 153 261 117 182  94  73  29  27]\n",
            " [ 52  18 139  64 397  75 125  99  18  13]\n",
            " [ 24  10 126 163  96 378  59 102  24  18]\n",
            " [ 17  20  96  83 112  62 552  38  12   8]\n",
            " [ 35  15  80  61 134 104  44 479  13  35]\n",
            " [168  78  34  14  33  18  12  14 577  52]\n",
            " [ 94 187  19  27  18  28  26  48  69 484]]\n",
            "Average Total Loss over Batches: 0.30117787640731764\n",
            "[[492  42 118  37  35  37  13  15 160  51]\n",
            " [ 44 546  23  31  30  24  21  18  85 178]\n",
            " [ 57   9 387  97 148 124  72  67  24  15]\n",
            " [ 26  18 157 286  79 228  76  73  29  28]\n",
            " [ 38  13 155 102 334 103 113 118  12  12]\n",
            " [ 22   5 120 187  49 459  39  89  17  13]\n",
            " [  9  13  98 106 113  98 498  35  11  19]\n",
            " [ 26  14  85  69 108 146  34 466  13  39]\n",
            " [133  64  37  23  33  28  16  21 582  63]\n",
            " [ 59 148  19  35  18  42  30  36  73 540]]\n",
            "Average Total Loss over Batches: 0.3047224717433964\n",
            "[[443  48  95  58  42  39  18  22 176  59]\n",
            " [ 45 569  14  42  25  16  30  21  82 156]\n",
            " [ 54  14 317 124 142 143  85  78  27  16]\n",
            " [ 18  15 118 356  75 208  88  74  20  28]\n",
            " [ 35  11 129 128 305 124 138 102  15  13]\n",
            " [ 17   8  92 238  54 417  42 110  11  11]\n",
            " [  6  19  76 126  89  86 534  35  13  16]\n",
            " [ 18  12  69  93 102 136  44 474  13  39]\n",
            " [103  85  34  33  40  33  14  19 566  73]\n",
            " [ 68 154  20  49  17  36  41  43  57 515]]\n",
            "Average Total Loss over Batches: 0.3062742485783655\n",
            "[[518  66  64  37  26  31   9  15 165  69]\n",
            " [ 53 561  14  28  20  14  17  21  86 186]\n",
            " [ 85  20 315 110 151 120  57  78  33  31]\n",
            " [ 38  22 122 305  89 190  67  86  37  44]\n",
            " [ 49  19 124 121 328 103  99 118  19  20]\n",
            " [ 30  20  96 205  66 393  34 107  27  22]\n",
            " [ 16  28  74 139 110 102 425  57  19  30]\n",
            " [ 36  21  65  84 115 116  31 459  17  56]\n",
            " [151  86  29  17  32  19  10  18 543  95]\n",
            " [ 62 160  16  26  18  28  19  47  78 546]]\n",
            "Average Total Loss over Batches: 0.30086429866611625\n",
            "[[509  25  93  44  40  32   9  18 185  45]\n",
            " [ 65 483  27  39  41  20  26  19 107 173]\n",
            " [ 68   6 342 104 170 127  66  71  32  14]\n",
            " [ 30  11 150 301  97 218  81  65  31  16]\n",
            " [ 42  10 155 100 356  98 112 108  15   4]\n",
            " [ 22   5 126 218  70 407  40  82  22   8]\n",
            " [  8   7 104 116 114  81 507  37  15  11]\n",
            " [ 30  11  81  86 129 113  33 478  15  24]\n",
            " [134  44  37  30  44  24  14  15 601  57]\n",
            " [ 78 117  26  37  21  39  35  52 100 495]]\n",
            "Average Total Loss over Batches: 0.2981489441001085\n",
            "[[564  58  77  19  25  26  14  17 161  39]\n",
            " [ 59 597  22  15  20  12  16  14  87 158]\n",
            " [107  21 331  70 158  84  82  82  38  27]\n",
            " [ 51  34 136 223 121 169  91  76  55  44]\n",
            " [ 60  15 144  55 391  57 121 112  27  18]\n",
            " [ 39  20 109 159  87 361  60 103  36  26]\n",
            " [ 15  27  87  78 131  61 512  41  19  29]\n",
            " [ 53  26  67  57 134  83  52 468  19  41]\n",
            " [171  78  19  12  32  17  10   9 599  53]\n",
            " [ 90 195  16  22  20  24  19  32  81 501]]\n",
            "Average Total Loss over Batches: 0.29126461844374696\n",
            "[[543  48  82  27  34  24   8  19 165  50]\n",
            " [ 41 574  14  28  16  15  13  22  79 198]\n",
            " [ 80  15 296 134 152 111  57  99  27  29]\n",
            " [ 34  22  96 326  89 196  67  99  30  41]\n",
            " [ 44  17 109 110 336 107  89 145  21  22]\n",
            " [ 30  15  84 223  63 391  27 124  20  23]\n",
            " [ 15  24  60 156 123 102 417  57  18  28]\n",
            " [ 32  22  51  79 102 126  25 492  15  56]\n",
            " [135  77  29  16  28  24   9  16 596  70]\n",
            " [ 67 164  10  22  13  24  14  42  76 568]]\n",
            "Average Total Loss over Batches: 0.2873275695090354\n",
            "[[461  73 106  38  25  32  11  24 170  60]\n",
            " [ 41 580  17  34  14  20  17  27  80 170]\n",
            " [ 69  14 301 123 127 132  72 107  27  28]\n",
            " [ 30  28  98 301  67 221  80 106  36  33]\n",
            " [ 44  18 115 108 297 110 115 156  20  17]\n",
            " [ 22  16  79 195  54 414  41 141  22  16]\n",
            " [  6  24  78 132  73 103 488  66  16  14]\n",
            " [ 28  19  62  79  86 115  38 513  16  44]\n",
            " [123  91  34  14  21  27  11  24 584  71]\n",
            " [ 61 192  13  30  14  37  25  56  70 502]]\n",
            "Average Total Loss over Batches: 0.2909490903703717\n",
            "[[506  74  76  37  33  23  10  23 167  51]\n",
            " [ 35 631  12  21  17  15  17  12  77 163]\n",
            " [ 86  22 321 102 141 101  74  90  39  24]\n",
            " [ 33  40 118 300  88 187  82  81  35  36]\n",
            " [ 47  22 132  96 320  82 121 142  21  17]\n",
            " [ 29  22 104 190  66 375  48 128  23  15]\n",
            " [ 11  35  81 115 108  81 495  40  12  22]\n",
            " [ 38  29  55  69 101 108  38 498  13  51]\n",
            " [138 103  32  14  30  21  11  13 578  60]\n",
            " [ 70 206  14  24   9  24  20  43  76 514]]\n",
            "Average Total Loss over Batches: 0.28530013560011214\n",
            "[[520  36 108  48  29  23  12  21 147  56]\n",
            " [ 46 503  24  31  34  18  23  19  87 215]\n",
            " [ 74   8 354 117 167  89  65  86  22  18]\n",
            " [ 27  10 147 313  94 184  79  89  27  30]\n",
            " [ 40   9 160  97 348  85 114 120  17  10]\n",
            " [ 28   8 116 210  78 384  43  95  20  18]\n",
            " [ 11  10  99 120 124  62 505  39  15  15]\n",
            " [ 32   8  75  80 123 107  38 482  14  41]\n",
            " [148  55  41  23  35  26  12  17 566  77]\n",
            " [ 66 130  26  33  16  26  28  45  71 559]]\n",
            "Average Total Loss over Batches: 0.28898377517564294\n",
            "[[478  50  85  50  44  30  11  15 170  67]\n",
            " [ 31 537   8  33  29  21  18  21  83 219]\n",
            " [ 68  19 281 136 167 108  83  85  32  21]\n",
            " [ 25  21 118 323  90 202  87  74  24  36]\n",
            " [ 39  15 108 110 369  88 124 114  17  16]\n",
            " [ 22  19  93 209  73 394  40 112  16  22]\n",
            " [  9  27  71 129 118  73 503  37  14  19]\n",
            " [ 30  15  54  86 130 114  34 475  13  49]\n",
            " [111  89  24  27  31  21  12  22 587  76]\n",
            " [ 57 153   8  27  14  30  28  41  63 579]]\n",
            "Average Total Loss over Batches: 0.30504878793855\n",
            "[[455  49 106  49  41  21  15  19 187  58]\n",
            " [ 31 574  14  32  22  20  25  20  76 186]\n",
            " [ 60  16 316 124 137 121  85  89  29  23]\n",
            " [ 16  17 120 337  72 206  89  78  29  36]\n",
            " [ 31  19 134 113 300 102 131 136  20  14]\n",
            " [ 14  13 103 231  48 400  48 108  14  21]\n",
            " [  6  23  72 132  87  82 511  49  11  27]\n",
            " [ 20  20  62  98  95 114  41 489  13  48]\n",
            " [108  77  31  21  37  24  17  19 580  86]\n",
            " [ 52 167  17  33  11  27  29  45  76 543]]\n",
            "Average Total Loss over Batches: 0.281867181993537\n",
            "[[480  57  83  38  38  32  17  20 155  80]\n",
            " [ 36 555  14  23  22  15  17  17  73 228]\n",
            " [ 70  16 301 107 175 107  77  89  30  28]\n",
            " [ 32  26 113 290 112 187  83  81  25  51]\n",
            " [ 40  16 117  78 386  84 121 122  16  20]\n",
            " [ 27  12  90 182  85 400  45 105  20  34]\n",
            " [  7  24  67 122 137  75 488  41  13  26]\n",
            " [ 26  19  56  63 131 102  43 492  12  56]\n",
            " [134  82  25  15  34  22  14  18 571  85]\n",
            " [ 58 154   9  25  16  25  25  40  58 590]]\n",
            "Average Total Loss over Batches: 0.28149460029060314\n",
            "[[487  38  90  47  30  28  10  20 164  86]\n",
            " [ 31 518  14  28  21  17  23  17  64 267]\n",
            " [ 66  14 293 133 167 114  68  87  28  30]\n",
            " [ 25  19  94 326  93 209  74  75  26  59]\n",
            " [ 33  16 109 104 356  94 115 131  20  22]\n",
            " [ 20  15  96 219  74 397  34 101  20  24]\n",
            " [  6  19  60 124 132  84 483  48  15  29]\n",
            " [ 22  15  61  79 119 120  32 471  13  68]\n",
            " [124  74  25  25  27  19  11  20 564 111]\n",
            " [ 51 131   7  24  12  22  22  38  60 633]]\n",
            "Average Total Loss over Batches: 0.27263806596825235\n",
            "[[510  47 104  25  33  27  12  17 159  66]\n",
            " [ 47 546  25  19  25  15  19  15  78 211]\n",
            " [ 75  14 341  83 180  99  77  78  28  25]\n",
            " [ 29  25 146 243 117 198  86  78  32  46]\n",
            " [ 36  17 148  68 386  83 120 112  13  17]\n",
            " [ 27  14 117 166  88 383  54 105  22  24]\n",
            " [  8  23  87  87 139  78 493  37  17  31]\n",
            " [ 32  18  71  52 138 118  42 465  11  53]\n",
            " [148  73  34  19  37  17  11  16 567  78]\n",
            " [ 65 148  20  20  17  24  27  34  70 575]]\n",
            "Average Total Loss over Batches: 0.27154165393324015\n",
            "[[478  41  97  35  62  20  15  17 177  58]\n",
            " [ 52 502  30  44  36  20  36  20  87 173]\n",
            " [ 64  11 364  91 188  78  91  65  33  15]\n",
            " [ 27  13 164 293 110 174  97  67  25  30]\n",
            " [ 41  11 152  86 377  62 144  99  18  10]\n",
            " [ 24   9 136 207  85 357  58  83  18  23]\n",
            " [ 10  13  95 126 115  73 525  23  10  10]\n",
            " [ 31   9  83  75 137 102  47 455  14  47]\n",
            " [124  74  36  21  48  22  18  17 572  68]\n",
            " [ 61 136  21  39  21  35  44  44  80 519]]\n",
            "Average Total Loss over Batches: 0.2683923954961883\n",
            "[[459  34  61  45  57  19  16  20 192  97]\n",
            " [ 28 504   7  24  20  10  13  16  75 303]\n",
            " [ 77  17 239 125 211  79  76  93  39  44]\n",
            " [ 23  19  85 313 110 154  88  89  39  80]\n",
            " [ 38  23  82 110 403  54 109 121  22  38]\n",
            " [ 23  18  78 231  96 318  43 119  18  56]\n",
            " [  9  27  48 124 141  58 480  41  19  53]\n",
            " [ 23  18  45  84 134  73  42 464  16 101]\n",
            " [110  52  14  27  29  13   9  21 611 114]\n",
            " [ 43 131   4  21  12  17  18  24  62 668]]\n",
            "Average Total Loss over Batches: 0.27407459894267244\n",
            "[[463  42  74  41  56  35  16  30 175  68]\n",
            " [ 40 515  17  34  25  19  26  27  86 211]\n",
            " [ 63  15 279 104 188 116  74 107  29  25]\n",
            " [ 25  19 120 269 108 206  75 102  29  47]\n",
            " [ 37  16 121  76 364  90 113 147  20  16]\n",
            " [ 23  12 102 168  69 421  37 120  21  27]\n",
            " [  9  26  82 121 121  80 459  59  14  29]\n",
            " [ 24  13  58  62 120 104  31 517  14  57]\n",
            " [125  66  25  30  34  25  10  23 581  81]\n",
            " [ 59 138  12  21  13  25  28  49  70 585]]\n",
            "Average Total Loss over Batches: 0.2647375599925737\n",
            "[[573  54  64  26  17  22   7  13 172  52]\n",
            " [ 59 594  19  28  19  14  16  18  83 150]\n",
            " [111  22 317 101 154 100  77  61  39  18]\n",
            " [ 53  29 125 293  79 196  84  61  42  38]\n",
            " [ 61  21 136  92 333  95 105 113  26  18]\n",
            " [ 41  18 125 190  65 387  48  77  27  22]\n",
            " [ 17  26 100 135 101  85 461  42  15  18]\n",
            " [ 54  22  67  86 105 126  32 441  13  54]\n",
            " [195  91  25  17  22  19   5  13 549  64]\n",
            " [ 83 181  15  22  15  26  24  38  79 517]]\n",
            "Average Total Loss over Batches: 0.27263174737610874\n",
            "[[507  28 108  37  59  30   9  29 164  29]\n",
            " [ 67 532  34  36  43  22  27  30  95 114]\n",
            " [ 68  12 315  78 206  99  95  89  27  11]\n",
            " [ 22  13 142 256 138 192 106  90  28  13]\n",
            " [ 38  10 126  60 418  68 145 114  17   4]\n",
            " [ 25   5 112 177 102 364  69 121  15  10]\n",
            " [  7   8  87  86 156  63 532  47   8   6]\n",
            " [ 25  10  71  64 156 105  45 494  13  17]\n",
            " [130  63  31  21  56  23  18  19 601  38]\n",
            " [ 90 138  20  33  38  44  46  87  85 419]]\n",
            "Average Total Loss over Batches: 0.26557121357413843\n",
            "[[562  60  60  21  28  17   7  20 180  45]\n",
            " [ 64 582  18  22  20  11  15  24  89 155]\n",
            " [120  15 315  91 147 100  59  84  48  21]\n",
            " [ 53  33 134 246  94 195  65  95  52  33]\n",
            " [ 63  19 142  77 321  87  91 147  29  24]\n",
            " [ 39  15 127 189  59 373  30 108  35  25]\n",
            " [ 21  28  90 136 112 101 399  63  21  29]\n",
            " [ 58  25  67  71  97  98  26 502  22  34]\n",
            " [167  84  26   8  26  14   5  17 593  60]\n",
            " [ 98 179  18  20  15  23  17  52  92 486]]\n",
            "Average Total Loss over Batches: 0.2675324157866021\n",
            "[[533  53  55  39  43  31  14  26 150  56]\n",
            " [ 44 579   7  31  24  13  20  20  68 194]\n",
            " [ 90  18 257 107 192 102  83  94  34  23]\n",
            " [ 35  29  90 306 107 191  92  73  33  44]\n",
            " [ 48  20  92  98 392  80 108 122  17  23]\n",
            " [ 32  17  70 202  86 387  56 107  17  26]\n",
            " [ 11  24  48 133 131  82 475  53  15  28]\n",
            " [ 43  24  38  80 128  98  36 481  12  60]\n",
            " [169 100  14  18  38  23  15  17 532  74]\n",
            " [ 64 177   5  28  12  23  22  35  64 570]]\n",
            "Average Total Loss over Batches: 0.258448237818618\n",
            "[[548  53  77  26  26  25  13  26 153  53]\n",
            " [ 45 606  14  26  17  14  22  19  75 162]\n",
            " [ 89  18 337  92 141  90  86  92  36  19]\n",
            " [ 38  28 125 279  79 181 113  93  31  33]\n",
            " [ 54  22 148  79 319  76 145 118  19  20]\n",
            " [ 34  16 114 182  70 362  61 116  22  23]\n",
            " [  9  30  75 100  88  74 544  45  16  19]\n",
            " [ 45  23  60  64 103 105  41 493  12  54]\n",
            " [161  90  27   9  25  20  16  18 563  71]\n",
            " [ 78 179  16  28  14  23  28  42  64 528]]\n",
            "Average Total Loss over Batches: 0.2667372084567211\n",
            "[[511  44  71  44  30  27  11  19 166  77]\n",
            " [ 38 506  15  32  18  16  24  23  70 258]\n",
            " [ 81  15 291 132 135 137  58  94  30  27]\n",
            " [ 30  20  99 331  76 207  68  92  22  55]\n",
            " [ 42  18 110 122 305 110  99 142  25  27]\n",
            " [ 30  13  78 217  50 417  34 110  22  29]\n",
            " [ 10  26  64 150  93 107 445  58  13  34]\n",
            " [ 31  16  52  93  82 124  27 489  15  71]\n",
            " [146  71  21  19  27  23  15  15 562 101]\n",
            " [ 52 141  11  33   6  21  18  32  59 627]]\n",
            "Average Total Loss over Batches: 0.2605513913764614\n",
            "[[520  51  62  38  26  31  16  11 201  44]\n",
            " [ 48 567  14  27  26  15  25  18 106 154]\n",
            " [ 82  15 305  99 159 121  89  76  43  11]\n",
            " [ 37  25 113 276  89 208 103  73  42  34]\n",
            " [ 45  14 119  78 355  94 143 109  29  14]\n",
            " [ 31  11  93 179  71 421  55  94  27  18]\n",
            " [ 10  22  73 103 108  74 531  37  21  21]\n",
            " [ 37  16  63  60 121 140  43 456  18  46]\n",
            " [126  69  22  20  25  24  14  12 635  53]\n",
            " [ 77 166  14  29  13  41  25  36  81 518]]\n",
            "Average Total Loss over Batches: 0.2524219063865584\n",
            "[[574  70  78  44  28  21  15  15 121  34]\n",
            " [ 65 590  22  32  31  18  31  25  62 124]\n",
            " [ 92  15 314 111 148 108 107  67  27  11]\n",
            " [ 40  23 135 321  84 170 122  58  21  26]\n",
            " [ 57  14 129  92 348  74 161  96  17  12]\n",
            " [ 29  13 122 198  78 369  71  88  20  12]\n",
            " [ 16  15  88  94 107  64 560  32  13  11]\n",
            " [ 44  20  71  73 130 116  45 454  15  32]\n",
            " [228 116  28  29  27  15  20  16 454  67]\n",
            " [ 84 193  19  40  20  37  40  46  69 452]]\n",
            "Average Total Loss over Batches: 0.2597497326285273\n",
            "[[489  44  88  43  45  42  11  25 152  61]\n",
            " [ 47 537  18  35  25  22  19  25  83 189]\n",
            " [ 72  13 286 139 151 134  56  94  33  22]\n",
            " [ 24  17 112 315  90 232  78  72  22  38]\n",
            " [ 36  13 119  92 357 111 103 132  19  18]\n",
            " [ 18   9  85 217  67 426  41 103  19  15]\n",
            " [  8  17  81 125 127  97 458  52  14  21]\n",
            " [ 21  12  60  76 126 132  24 491  14  44]\n",
            " [138  71  30  29  34  27  13  20 560  78]\n",
            " [ 67 142  18  34  16  38  25  38  70 552]]\n",
            "Average Total Loss over Batches: 0.25498684666725496\n",
            "[[565  52  60  30  27  38  11  23 156  38]\n",
            " [ 58 594  17  23  24  16  12  22  85 149]\n",
            " [ 94  20 273 108 185 100  62 105  37  16]\n",
            " [ 37  32 108 275 114 178  87  91  46  32]\n",
            " [ 56  17 115  86 380  66 114 125  25  16]\n",
            " [ 34  16  87 178  90 373  50 126  26  20]\n",
            " [ 14  33  65 110 145  59 493  44  18  19]\n",
            " [ 37  20  53  60 132 109  34 505  14  36]\n",
            " [157  67  18  24  22  24  12  18 604  54]\n",
            " [ 81 184  12  28  16  26  22  36  85 510]]\n",
            "Average Total Loss over Batches: 0.25501708680915036\n",
            "[[556  56  65  15  35  27  13  17 168  48]\n",
            " [ 52 584  13  20  22  16  16  16  82 179]\n",
            " [ 99  21 282  67 168 124  78  95  40  26]\n",
            " [ 41  31 113 230 102 228  87  93  29  46]\n",
            " [ 57  21  99  58 372  97 116 138  24  18]\n",
            " [ 31  21  83 149  80 418  48 119  29  22]\n",
            " [ 24  33  60  83 109 110 477  55  18  31]\n",
            " [ 43  26  49  47 105 129  40 495  15  51]\n",
            " [168  79  22   9  31  20  11  18 573  69]\n",
            " [ 68 171  10  14   5  30  20  44  76 562]]\n",
            "Average Total Loss over Batches: 0.25761259354956384\n",
            "[[510  62  76  45  43  32  13  21 151  47]\n",
            " [ 57 543  21  45  29  22  19  27  82 155]\n",
            " [ 84  17 302 140 132 126  65  84  33  17]\n",
            " [ 36  26 103 335  80 224  70  65  33  28]\n",
            " [ 48  18 123 136 309 106 107 127  16  10]\n",
            " [ 27  14  88 231  54 410  42 102  19  13]\n",
            " [ 14  23  84 155  95 104 451  44  17  13]\n",
            " [ 41  16  61 102 103 128  32 465  15  37]\n",
            " [169  80  22  36  35  24  13  23 538  60]\n",
            " [ 76 197  15  46  22  33  28  46  79 458]]\n",
            "Average Total Loss over Batches: 0.2529689989667796\n",
            "[[563  44  73  31  35  23  20  13 156  42]\n",
            " [ 54 592  25  29  25  16  27  21  92 119]\n",
            " [ 97  16 334  80 145  86 127  74  31  10]\n",
            " [ 35  24 150 280  76 182 146  58  26  23]\n",
            " [ 49  12 144  81 337  67 189  91  22   8]\n",
            " [ 33  13 108 192  66 374  89  92  21  12]\n",
            " [ 10  14  78  84  72  48 649  26  11   8]\n",
            " [ 40  19  77  62 121 108  57 470  18  28]\n",
            " [148  78  35  19  23  23  20  11 596  47]\n",
            " [ 77 185  23  32   9  31  42  41  92 468]]\n",
            "Average Total Loss over Batches: 0.25116043582543374\n",
            "[[543  52  75  30  47  24  12  17 140  60]\n",
            " [ 40 585  17  21  22  12  22  15  74 192]\n",
            " [ 90  22 278  98 189 105  82  91  22  23]\n",
            " [ 31  26 113 279 103 212  94  76  21  45]\n",
            " [ 48  16  98  74 399  83 126 119  18  19]\n",
            " [ 26  14  86 159  84 440  47 102  17  25]\n",
            " [ 14  27  50 113 122 108 494  46  11  15]\n",
            " [ 36  23  49  53 138 126  37 481  13  44]\n",
            " [169  94  31  18  32  20  13  18 524  81]\n",
            " [ 68 168  13  27  17  31  31  38  63 544]]\n"
          ]
        }
      ],
      "source": [
        "batch_size = 4\n",
        "\n",
        "for epochs in range(100):\n",
        "  total_loss = 0\n",
        "  for batch in range( train_X.shape[0] // batch_size ):\n",
        "    x_batch, y_batch = get_batch(train_X, train_Y, batch_size)\n",
        "\n",
        "    cnn_optimizer.zero_grad()\n",
        "    logits = cifar_model( x_batch )\n",
        "    loss = loss_function( logits, y_batch )\n",
        "\n",
        "    loss.backward()\n",
        "    cnn_optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print( \"Average Total Loss over Batches:\", total_loss / ( train_X.shape[0] // batch_size ) )\n",
        "  print( confusion_matrix( cifar_model, test_X, test_Y ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta4Mwqto9y6s"
      },
      "source": [
        "There are some interesting narratives that develop over the course of training - for instance the back and forth misidentifications between cars and trucks.\n",
        "\n",
        "If your run is anything like mine, you see steady decrease in the loss, and trending of the confusion matrix to focus on the diagonal (i.e., correctly identifying things). There are persistent misclassifications, but this is not the most complext network you could construct here.\n",
        "\n",
        "An interesting thing happens - at least for me - if you try to add another convolutional layer to the network (mine had 20 features). Why?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}