{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc0e15f-5077-4d32-9504-2018f5540ec1",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0d5683-32ba-44ad-9881-10bdf3408ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import func3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b52eced-7bfb-4f15-81f5-e55d6c378945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqGen(n):\n",
    "    ind = random.sample(range(10), 5)\n",
    "    ind.sort()\n",
    "    randmu = 2*torch.rand((5)) - 1\n",
    "    T = random.randint(0, n)\n",
    "    haspoint = random.random()\n",
    "    seq = torch.randn((n,10))\n",
    "    if haspoint < 0.8:\n",
    "        for t in range(n):\n",
    "                \n",
    "                j = 0\n",
    "                for i in range(10):\n",
    "                    if i in ind:\n",
    "                        seq[t][i] = seq[t][i] + randmu[j]\n",
    "                        j = j+1\n",
    "    else: \n",
    "    else\n",
    "    return seq, ind, randmu, haspoint, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d60e1b3a-79a6-47bf-8a3a-f4851b249985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "[0, 2, 4, 6, 8]\n",
      "tensor([ 0.9979,  0.3047, -0.1415, -0.7205,  0.8178])\n",
      "0 22\n",
      "0 tensor([ 1.0399,  1.1474, -0.9378, -0.3513,  2.3777,  0.2853,  0.2406,  1.6800,\n",
      "         1.3886, -0.0181])\n",
      "1 tensor([ 0.7223,  0.6952,  1.0068,  0.1953, -0.1839,  1.2503,  1.4580, -0.5539,\n",
      "        -0.1916,  0.7588])\n",
      "2 tensor([ 0.6995,  0.1181, -0.2898,  0.8608, -2.6839, -0.1904,  0.7820, -1.0848,\n",
      "        -2.1385,  2.4629])\n",
      "3 tensor([-0.4236,  1.2407,  0.7797,  0.4725,  0.1904, -1.2370, -1.0132, -1.8932,\n",
      "        -0.2487, -0.7997])\n",
      "4 tensor([ 0.3581, -1.5211,  0.0377, -0.2268,  0.2915, -0.2531,  0.5637,  2.4980,\n",
      "         0.6208,  1.2561])\n",
      "5 tensor([-0.6611,  0.0340,  1.2407, -0.6805, -1.2731,  0.7915,  1.3916, -0.2622,\n",
      "        -0.7772, -0.0042])\n",
      "6 tensor([ 0.0613,  0.0961,  0.4298,  1.2918,  1.7247,  0.7117, -1.7152, -0.4974,\n",
      "         1.5140,  0.1750])\n",
      "7 tensor([-0.5678, -0.5550, -1.6377, -1.2251, -1.1410, -0.7413,  1.4224,  1.7350,\n",
      "         1.0121, -0.6490])\n",
      "8 tensor([ 0.4246,  1.7240,  0.1867, -1.0061,  0.8928, -0.4119,  0.0654,  0.0110,\n",
      "        -0.7618,  0.0546])\n",
      "9 tensor([-3.4850,  1.0239,  0.0525,  0.0356, -0.2099,  1.4621, -0.2307, -0.2536,\n",
      "        -0.1989,  1.0919])\n",
      "10 tensor([ 1.2310,  0.6450,  1.4474,  0.0412, -1.3793, -1.4279,  0.1442,  0.1370,\n",
      "        -0.0252,  2.5725])\n",
      "11 tensor([ 0.0978, -1.5332, -0.0769,  0.7746,  0.3927, -1.6831, -2.3808,  1.9574,\n",
      "         1.0087, -1.3730])\n",
      "12 tensor([ 0.4443,  0.9810,  0.7511, -0.5005,  0.3697,  0.8666, -1.3497,  0.2656,\n",
      "         0.0767, -1.0443])\n",
      "13 tensor([ 0.9448, -1.7039,  1.2213,  1.1835, -0.1029, -0.0457,  1.1650,  1.2532,\n",
      "        -0.5624,  1.6390])\n",
      "14 tensor([-0.3080, -0.2323,  0.8279, -0.0164, -0.3784,  1.5913,  0.8285, -2.1262,\n",
      "         0.0540, -0.3468])\n",
      "15 tensor([ 0.2628,  0.5815, -0.1845,  0.5141,  1.0673, -0.5839, -1.0838, -1.1702,\n",
      "         1.8835, -0.2663])\n",
      "16 tensor([ 0.0233,  0.9354,  0.8940, -0.8090,  0.1741, -0.7993,  1.2126, -1.1663,\n",
      "        -1.7599, -0.3273])\n",
      "17 tensor([ 0.3459,  0.7437,  0.4315,  0.6870,  1.2710, -0.6416,  2.0917,  1.7189,\n",
      "         0.7545,  0.8889])\n",
      "18 tensor([-1.3217,  0.2459,  1.1092, -0.3882, -0.9329,  0.3001, -0.3848, -0.1862,\n",
      "         2.0001, -2.2796])\n",
      "19 tensor([-0.5911,  0.7525,  0.8003,  0.0371, -0.7727,  0.4309, -1.0253, -0.4300,\n",
      "         1.4019,  1.3028])\n",
      "20 tensor([-1.1372,  2.7725, -1.4784, -0.5771, -0.4867, -0.5797, -0.6623,  0.1221,\n",
      "         0.9539, -0.7508])\n",
      "21 tensor([-2.4430, -0.8912,  0.3709,  0.3756,  1.0839,  1.6378, -1.2754,  0.8680,\n",
      "         0.4227, -0.8167])\n",
      "22 tensor([-0.1928,  0.7750,  0.6254,  0.0737,  0.2487, -0.2639, -0.9112, -1.6339,\n",
      "        -0.5799,  0.4800])\n",
      "23 tensor([ 0.0128,  0.4076, -2.7953,  1.3257, -1.1029,  0.2328,  0.3116,  1.2437,\n",
      "         1.1438, -0.7163])\n",
      "24 tensor([-0.4680, -0.0812, -1.1438,  0.9842, -0.8587, -0.2989, -1.4548,  1.5547,\n",
      "        -0.0493,  0.7700])\n",
      "25 tensor([ 1.3222, -1.1661,  1.0254, -0.6423,  1.4050,  0.2609, -2.4602, -0.3606,\n",
      "        -0.2875,  1.6216])\n",
      "26 tensor([ 0.3301, -0.3057,  1.8880,  1.9766, -0.3025, -0.7011,  2.3039,  0.3403,\n",
      "        -2.8325, -0.2751])\n",
      "27 tensor([-3.3394, -0.7681, -0.5387, -0.9972, -0.9472,  0.7849,  0.5680,  0.3937,\n",
      "        -0.6864,  0.1503])\n",
      "28 tensor([ 0.3836, -0.8600, -1.3201, -0.0843,  0.4579, -1.1647,  0.0864,  1.7724,\n",
      "        -0.0225, -0.6601])\n",
      "29 tensor([-0.5962,  2.3326,  0.3459,  2.1521, -0.4021, -0.1154, -1.1924, -0.7336,\n",
      "        -0.4151,  0.6423])\n",
      "30 tensor([-1.3462, -0.0467, -0.2946, -0.8408, -0.0378, -0.4516,  0.9719, -0.0632,\n",
      "        -2.3599,  0.1088])\n",
      "31 tensor([-1.3808, -0.6265, -2.2198, -0.7466, -0.8064,  1.6195,  0.4795, -0.8933,\n",
      "        -1.0305,  0.3371])\n",
      "32 tensor([-0.2071, -0.2750,  1.3463,  0.5057,  0.5057,  1.7088, -0.4713, -0.9338,\n",
      "         0.4425,  0.7075])\n",
      "33 tensor([-0.3022, -0.4151, -1.2538, -0.2811,  0.9035,  1.9469,  0.9264,  2.0769,\n",
      "        -0.0207,  1.2934])\n",
      "34 tensor([-0.5056, -0.1177, -0.8862, -0.2565,  0.8992, -0.5398,  0.1721,  1.1388,\n",
      "        -0.8863,  0.9046])\n",
      "35 tensor([ 0.6296, -0.0508,  0.9568,  1.3871, -0.2980, -1.4235, -0.7216, -0.5389,\n",
      "         0.4384, -1.3634])\n",
      "36 tensor([ 0.0854,  2.1100,  0.1462,  2.0486, -1.2220,  1.1035, -1.2186, -0.6578,\n",
      "         0.3447, -0.8080])\n",
      "37 tensor([-0.8112,  0.6651, -0.3141,  0.5644, -0.2792, -0.2065, -0.2461, -0.0923,\n",
      "         2.0538,  1.5521])\n",
      "38 tensor([ 0.5359, -1.6205,  1.9275, -0.2016,  0.0315, -2.2005, -0.3521,  0.0245,\n",
      "        -0.7536, -0.3261])\n",
      "39 tensor([ 0.6756,  2.1922,  0.5280, -1.7459, -0.9751, -0.4956,  1.5588,  0.4567,\n",
      "         1.8497,  1.6864])\n",
      "40 tensor([ 2.2270,  1.2509, -0.5239,  0.1714, -0.7787,  1.8655,  0.1576, -0.2600,\n",
      "         0.4461, -0.8544])\n",
      "41 tensor([-0.5110, -0.0225,  0.6728,  0.6065,  0.9687,  1.5616, -1.4822,  0.5738,\n",
      "         0.3719, -0.3526])\n",
      "42 tensor([ 0.6539, -1.5806,  0.0272,  1.4182, -1.5754, -0.1381,  0.5825,  0.2775,\n",
      "        -1.6816, -1.8394])\n",
      "43 tensor([ 0.2617,  0.4714, -0.3214, -2.7526,  1.6830,  0.6466, -0.2898, -0.5977,\n",
      "         0.8372,  0.1730])\n",
      "44 tensor([-0.3856, -0.2051,  0.3448,  0.6705,  0.1499,  1.5503, -1.2850, -2.4768,\n",
      "        -1.0213,  0.0474])\n",
      "45 tensor([-0.6694,  0.5230, -0.1546, -0.7847, -1.1734,  1.1982,  0.4057,  0.8674,\n",
      "        -1.0942, -0.3734])\n",
      "46 tensor([-1.2037,  0.4583,  0.8118,  0.4198,  2.1297, -0.5274,  0.9077,  0.1264,\n",
      "         0.0981,  0.9496])\n",
      "47 tensor([ 2.9236, -1.2995, -0.2796,  1.5694, -0.8999,  0.5589, -0.8785, -0.4474,\n",
      "        -0.4976,  1.3619])\n",
      "48 tensor([ 0.2991, -1.5713, -1.0134, -0.2839, -1.3419,  1.0406,  0.4322, -0.2381,\n",
      "         0.0531,  0.4823])\n",
      "49 tensor([-0.5526, -0.6126, -0.1328,  0.6681, -1.0024, -0.4282, -0.3148,  0.1347,\n",
      "         1.0339, -1.1245])\n"
     ]
    }
   ],
   "source": [
    "seq, ind, randmu, haspoint, T = seqGen(50)\n",
    "print(len(seq))\n",
    "print(ind)\n",
    "print(randmu)\n",
    "print(haspoint, T)\n",
    "for i in range(len(seq)):\n",
    "    print(i, seq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "970e090d-6507-4e44-99cc-4ccad20acd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class changeDetectRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(changeDetectRNN, self).__init__()\n",
    "        self.current_itemf = torch.nn.Linear( in_features = 10, out_features = 100, bias = True )\n",
    "        self.current_itemb = torch.nn.Linear( in_features = 10, out_features = 100, bias = True )\n",
    "\n",
    "        self.prev_itemf1 = torch.nn.Linear( in_features = 100, out_features = 100, bias = False )\n",
    "        self.prev_itemb1 = torch.nn.Linear( in_features = 100, out_features = 100, bias = False )\n",
    "\n",
    "        self.prev_itemf2 = torch.nn.Linear( in_features = 100, out_features = 100, bias = False )\n",
    "        self.prev_itemb2 = torch.nn.Linear( in_features = 100, out_features = 100, bias = False )\n",
    "\n",
    "        self.prev_itemf3 = torch.nn.Linear( in_features = 100, out_features = 100, bias = False )\n",
    "        self.prev_itemb3 = torch.nn.Linear( in_features = 100, out_features = 100, bias = False )\n",
    "\n",
    "        \n",
    "        self.normf = nn.LayerNorm(100)\n",
    "        self.normb = nn.LayerNorm(100)\n",
    "        \n",
    "        self.point_detector = nn.Sequential(nn.Linear(200,1000), nn.LayerNorm(1000),nn.Sigmoid(),nn.Linear(1000,1000),nn.Sigmoid(), nn.LayerNorm(1000), nn.Linear(1000,1),nn.Sigmoid() )\n",
    "    def forward(self, seq):\n",
    "        n = seq.shape[1]\n",
    "        batch_size = seq.shape[0]\n",
    "        seqb = torch.flip(seq, [1])\n",
    "        yf = []\n",
    "        yb = []\n",
    "        yf.append(nn.Sigmoid()(self.normf(self.current_itemf(seq[:,0,:]))))\n",
    "        yb.append(nn.Sigmoid()(self.normb(self.current_itemb(seqb[:,0,:]))))\n",
    "        yf.append(nn.Sigmoid()(self.normf(self.current_itemf(seq[:,1,:])+ self.prev_itemf1(yf[-1]))))\n",
    "        yb.append(nn.Sigmoid()(self.normb(self.current_itemb(seqb[:,1,:])+ self.prev_itemb1(yb[-1]))))\n",
    "        yf.append(nn.Sigmoid()(self.normf(self.current_itemf(seq[:,2,:]) + self.prev_itemf1(yf[-1])+ self.prev_itemf2(yf[-2]))))\n",
    "        yb.append(nn.Sigmoid()(self.normb(self.current_itemb(seqb[:,2,:]) + self.prev_itemb1(yb[-1])+ self.prev_itemb2(yf[-2]))))\n",
    "        for i in range(3,n):\n",
    "            # print(yf[-1].shape)\n",
    "            yf.append(nn.Sigmoid()(self.normf(self.current_itemf(seq[:,i,:]) + self.prev_itemf1(yf[-1])+ self.prev_itemf2(yf[-2])+self.prev_itemf3(yf[-3]))))\n",
    "            yb.append(nn.Sigmoid()(self.normb(self.current_itemb(seqb[:,i,:]) + self.prev_itemb1(yb[-1])+ self.prev_itemb2(yf[-2])+self.prev_itemb3(yf[-3]))))\n",
    "        yb.reverse()\n",
    "\n",
    "        changepoints = torch.zeros((batch_size, n))\n",
    "        \n",
    "        for i in range(n):\n",
    "            \n",
    "            changepoints[:,i] = self.point_detector(torch.cat((yf[i], yb[i]), 1))[:,0]\n",
    "\n",
    "        return changepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b53e87b-0eb1-4a17-ba7a-763d0c86a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class changeDetectLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(changeDetectLSTM, self).__init__()\n",
    "        self.lstm1 = nn.RNN(10, 10,1,batch_first=True, bidirectional=True)\n",
    "        #  nn.LSTM(10, 20,5,batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(10, 5,3,batch_first=True)\n",
    "\n",
    "        self.classifyLSTM = nn.LSTM(100, 1, 1, batch_first=True)\n",
    "        self.classifier = nn.Sequential(nn.Linear(20,200), nn.ELU(), nn.LayerNorm(200),\n",
    "                                        nn.Linear(200,100), nn.ELU(), nn.LayerNorm(100), nn.Linear(100,1))\n",
    "        self.point_detector = nn.Sequential(nn.Linear(200,1000), nn.LayerNorm(1000),nn.Sigmoid(),nn.Linear(1000,1000),nn.Sigmoid(), nn.LayerNorm(1000), nn.Linear(1000,1),nn.Sigmoid() )\n",
    "\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        # seq = torch.mul(seq, 10)\n",
    "        seq, _ = self.lstm1(seq)\n",
    "        seq = self.classifier(seq)\n",
    "        \n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd1c174f-88aa-4cff-88a0-705212367651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4984, -0.3807, -0.4210,  0.1526,  0.1696,  0.4242,  0.7521,  0.1741,\n",
      "         0.8790,  0.4640])\n",
      "tensor([ 1.4984, -0.3807, -0.4210,  0.1526,  0.1696,  0.4242,  0.7521,  0.1741,\n",
      "         0.8790,  0.4640,  1.4984, -0.3807, -0.4210,  0.1526,  0.1696,  0.4242,\n",
      "         0.7521,  0.1741,  0.8790,  0.4640])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10)\n",
    "print(x)\n",
    "print(torch.cat((x,x), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643802e1-513a-476b-b4f5-72c847387da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, seq_length, device):\n",
    "    x_batch = torch.zeros((batch_size, seq_length, 10), device=device)\n",
    "    y_batch = torch.zeros((batch_size, seq_length), device=device)\n",
    "    for i in range(batch_size):\n",
    "        seq, _, _, haspoint, T = seqGen(seq_length)\n",
    "        for j in range(len(seq)):\n",
    "            if haspoint == 1 and j >= T:\n",
    "                y_batch[i][j] = 1\n",
    "            \n",
    "        x_batch[i] = seq\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "168718aa-ea50-4c9a-b028-ca7e13e3e689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.4049, -0.8837,  0.2189,  0.2833,  0.0850, -1.1273, -0.5754,\n",
      "           0.3827,  0.4344, -0.8288],\n",
      "         [-1.0201, -0.6424, -0.3385, -0.1234, -0.4842, -1.3048, -0.5308,\n",
      "           1.0170,  0.7136,  1.6644],\n",
      "         [ 1.3211,  0.5331, -1.3790,  0.1452, -0.2916,  0.6121, -0.3462,\n",
      "          -1.4847,  1.1317, -0.2574],\n",
      "         [-2.4638, -1.2155, -1.1739, -1.5042,  1.0520,  0.7329,  0.4771,\n",
      "          -0.0115,  0.6189, -0.6080],\n",
      "         [-0.5209,  0.4390, -0.3737,  1.7966, -0.9788,  0.2802, -1.2945,\n",
      "          -1.8794, -1.3292, -0.3854],\n",
      "         [-0.3010,  1.3106, -0.8410, -1.0394, -0.6550,  0.9290,  0.7821,\n",
      "           0.5405,  0.3706,  0.8595],\n",
      "         [-0.7153,  1.0079, -0.0314, -1.0289, -2.5729, -0.9301,  1.1003,\n",
      "           1.5781, -0.8748, -1.3504],\n",
      "         [ 0.2485, -0.7091,  0.9974,  2.6215, -0.8839, -1.7273,  0.4768,\n",
      "           0.5396, -0.7275,  0.0954],\n",
      "         [ 0.5676,  0.8866, -0.3271,  1.1786,  0.2498, -1.3037, -1.3073,\n",
      "          -0.7880,  1.5421,  1.2406],\n",
      "         [-0.8465, -0.6530,  0.2417,  0.9759,  0.7215, -0.6293,  0.1233,\n",
      "          -0.6986,  0.6075,  0.3477]]])\n",
      "tensor([[[-0.8465, -0.6530,  0.2417,  0.9759,  0.7215, -0.6293,  0.1233,\n",
      "          -0.6986,  0.6075,  0.3477],\n",
      "         [ 0.5676,  0.8866, -0.3271,  1.1786,  0.2498, -1.3037, -1.3073,\n",
      "          -0.7880,  1.5421,  1.2406],\n",
      "         [ 0.2485, -0.7091,  0.9974,  2.6215, -0.8839, -1.7273,  0.4768,\n",
      "           0.5396, -0.7275,  0.0954],\n",
      "         [-0.7153,  1.0079, -0.0314, -1.0289, -2.5729, -0.9301,  1.1003,\n",
      "           1.5781, -0.8748, -1.3504],\n",
      "         [-0.3010,  1.3106, -0.8410, -1.0394, -0.6550,  0.9290,  0.7821,\n",
      "           0.5405,  0.3706,  0.8595],\n",
      "         [-0.5209,  0.4390, -0.3737,  1.7966, -0.9788,  0.2802, -1.2945,\n",
      "          -1.8794, -1.3292, -0.3854],\n",
      "         [-2.4638, -1.2155, -1.1739, -1.5042,  1.0520,  0.7329,  0.4771,\n",
      "          -0.0115,  0.6189, -0.6080],\n",
      "         [ 1.3211,  0.5331, -1.3790,  0.1452, -0.2916,  0.6121, -0.3462,\n",
      "          -1.4847,  1.1317, -0.2574],\n",
      "         [-1.0201, -0.6424, -0.3385, -0.1234, -0.4842, -1.3048, -0.5308,\n",
      "           1.0170,  0.7136,  1.6644],\n",
      "         [-1.4049, -0.8837,  0.2189,  0.2833,  0.0850, -1.1273, -0.5754,\n",
      "           0.3827,  0.4344, -0.8288]]])\n"
     ]
    }
   ],
   "source": [
    "tensor, _ = get_batch(1,10)\n",
    "print(tensor)\n",
    "print(torch.flip(tensor, [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63f95f24-ac8d-443b-8b6d-7e8735ad76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfunc(predictions, actual_labels):\n",
    "    return torch.mean( -1 * torch.log( predictions + 0.0001 ).to(device) * actual_labels - torch.log( 1 - predictions + 0.0001 ) * (1 - actual_labels ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f216d060-2593-4691-b7b2-03e8e90fe1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Test loss 116.81209564208984 Training loss 3507.976368010044\n",
      "2 Test loss 117.92870330810547 Training loss 3729.502596616745\n",
      "3 Test loss 119.26509857177734 Training loss 3715.859140753746\n",
      "4 Test loss 118.62266540527344 Training loss 3206.1830335259438\n",
      "5 Test loss 120.1336441040039 Training loss 3405.397356033325\n",
      "6 Test loss 117.21597290039062 Training loss 3734.0032049417496\n",
      "7 Test loss 117.03057098388672 Training loss 3357.841537952423\n",
      "8 Test loss 117.29861450195312 Training loss 3262.6579213142395\n",
      "9 Test loss 116.7618637084961 Training loss 3275.1898350715637\n",
      "10 Test loss 116.6441650390625 Training loss 3251.3819422721863\n",
      "11 Test loss 117.50759887695312 Training loss 3796.944023370743\n",
      "12 Test loss 122.03592681884766 Training loss 3127.3643123209476\n",
      "13 Test loss 127.79537963867188 Training loss 3614.823840856552\n",
      "14 Test loss 117.5074462890625 Training loss 3441.292931675911\n",
      "15 Test loss 117.22074127197266 Training loss 3529.906070590019\n",
      "16 Test loss 116.75135040283203 Training loss 3849.470707297325\n",
      "17 Test loss 117.48165893554688 Training loss 3403.446630835533\n",
      "18 Test loss 118.76703643798828 Training loss 3241.0062497854233\n",
      "19 Test loss 122.88192749023438 Training loss 3678.3421561717987\n",
      "20 Test loss 118.30358123779297 Training loss 3725.673289477825\n",
      "21 Test loss 119.98538208007812 Training loss 3195.7533264160156\n",
      "22 Test loss 117.27864837646484 Training loss 3048.5723321437836\n",
      "23 Test loss 119.01673889160156 Training loss 3259.6701073646545\n",
      "24 Test loss 120.18550872802734 Training loss 3732.3003611564636\n",
      "25 Test loss 119.2337417602539 Training loss 3684.7158982753754\n",
      "26 Test loss 118.00142669677734 Training loss 3862.57503926754\n",
      "27 Test loss 119.2691650390625 Training loss 2816.6266397237778\n",
      "28 Test loss 118.32640075683594 Training loss 3710.810010969639\n",
      "29 Test loss 120.41349029541016 Training loss 3758.982334136963\n",
      "30 Test loss 122.33470916748047 Training loss 3999.546030640602\n",
      "31 Test loss 125.35309600830078 Training loss 3236.3313550949097\n",
      "32 Test loss 126.9161148071289 Training loss 3191.2813576459885\n",
      "33 Test loss 118.24542999267578 Training loss 3639.2550382614136\n",
      "34 Test loss 119.45628356933594 Training loss 3479.517497777939\n",
      "35 Test loss 116.8546142578125 Training loss 3221.2044349312782\n",
      "36 Test loss 118.8556137084961 Training loss 3641.867193222046\n",
      "37 Test loss 116.95883178710938 Training loss 3362.4920634031296\n",
      "38 Test loss 122.51567840576172 Training loss 3500.9819016456604\n",
      "39 Test loss 117.60277557373047 Training loss 2841.0884311795235\n",
      "40 Test loss 121.59700012207031 Training loss 2925.5597134828568\n",
      "41 Test loss 120.116943359375 Training loss 3472.5723206996918\n",
      "42 Test loss 117.07244110107422 Training loss 3625.1230013370514\n",
      "43 Test loss 117.50508117675781 Training loss 3446.761799097061\n",
      "44 Test loss 118.058837890625 Training loss 3082.2886475920677\n",
      "45 Test loss 118.63725280761719 Training loss 3324.76518535614\n",
      "46 Test loss 119.04373931884766 Training loss 3311.7427438646555\n",
      "47 Test loss 119.083984375 Training loss 3367.227200090885\n",
      "48 Test loss 120.49030303955078 Training loss 3495.55188536644\n",
      "49 Test loss 117.8458480834961 Training loss 4289.534894943237\n",
      "50 Test loss 117.68408203125 Training loss 3413.2181491851807\n",
      "51 Test loss 117.63423919677734 Training loss 3627.928643643856\n",
      "52 Test loss 117.42217254638672 Training loss 3564.3098307847977\n",
      "53 Test loss 120.34134674072266 Training loss 3621.393691778183\n",
      "54 Test loss 126.36510467529297 Training loss 3252.4475078582764\n",
      "55 Test loss 119.40556335449219 Training loss 3794.812052845955\n",
      "56 Test loss 123.19584655761719 Training loss 3090.345962166786\n",
      "57 Test loss 118.3438949584961 Training loss 3994.000596523285\n",
      "58 Test loss 118.42826843261719 Training loss 3238.19043636322\n",
      "59 Test loss 118.49978637695312 Training loss 3707.757936477661\n",
      "60 Test loss 120.89126586914062 Training loss 3185.7945848703384\n",
      "61 Test loss 125.48867797851562 Training loss 4265.90927696228\n",
      "62 Test loss 117.90777587890625 Training loss 3504.3685686588287\n",
      "63 Test loss 119.46508026123047 Training loss 3645.789271593094\n",
      "64 Test loss 127.63936614990234 Training loss 3818.7184448242188\n",
      "65 Test loss 128.8042755126953 Training loss 3792.4094088077545\n",
      "66 Test loss 121.29083251953125 Training loss 4171.790947437286\n",
      "67 Test loss 119.95903778076172 Training loss 3917.9764897823334\n",
      "68 Test loss 129.7335968017578 Training loss 3721.0609047412872\n",
      "69 Test loss 120.23877716064453 Training loss 3133.85014629364\n",
      "70 Test loss 118.26561737060547 Training loss 3186.529105603695\n",
      "71 Test loss 124.38680267333984 Training loss 3004.8396711349487\n",
      "72 Test loss 117.96636199951172 Training loss 3064.121125459671\n",
      "73 Test loss 126.34799194335938 Training loss 3600.7680673599243\n",
      "74 Test loss 118.55960083007812 Training loss 4391.218100070953\n",
      "75 Test loss 126.08317565917969 Training loss 3375.902882218361\n",
      "76 Test loss 120.7302017211914 Training loss 3602.7018485069275\n",
      "77 Test loss 126.22186279296875 Training loss 3589.0663952827454\n",
      "78 Test loss 120.88131713867188 Training loss 3791.2095992565155\n",
      "79 Test loss 117.33053588867188 Training loss 3554.8612356185913\n",
      "80 Test loss 121.4683837890625 Training loss 3552.1400393247604\n",
      "81 Test loss 123.18351745605469 Training loss 3015.8719177246094\n",
      "82 Test loss 125.69638061523438 Training loss 3320.6780372858047\n",
      "83 Test loss 128.1824951171875 Training loss 3585.261878490448\n",
      "84 Test loss 153.64453125 Training loss 3555.010980606079\n",
      "85 Test loss 122.6452407836914 Training loss 3371.83067548275\n",
      "86 Test loss 126.70447540283203 Training loss 3134.5818531513214\n",
      "87 Test loss 118.22025299072266 Training loss 3426.2480268478394\n",
      "88 Test loss 127.44878387451172 Training loss 3300.0604186058044\n",
      "89 Test loss 121.58671569824219 Training loss 3496.4349467754364\n",
      "90 Test loss 120.6998062133789 Training loss 3751.8377046585083\n",
      "91 Test loss 127.14056396484375 Training loss 3975.1473026275635\n",
      "92 Test loss 118.07384490966797 Training loss 3481.039940714836\n",
      "93 Test loss 127.7210693359375 Training loss 3296.80401968956\n",
      "94 Test loss 117.62247467041016 Training loss 3742.8006587028503\n",
      "95 Test loss 118.27237701416016 Training loss 3460.160203933716\n",
      "96 Test loss 124.04579162597656 Training loss 3693.74684381485\n",
      "97 Test loss 119.2353515625 Training loss 3760.3685398101807\n",
      "98 Test loss 118.33570861816406 Training loss 3775.9677443504333\n",
      "99 Test loss 128.56497192382812 Training loss 3373.536681175232\n",
      "100 Test loss 118.81608581542969 Training loss 3297.026781320572\n",
      "101 Test loss 118.83888244628906 Training loss 3570.279929637909\n",
      "102 Test loss 152.11758422851562 Training loss 3768.9801881313324\n",
      "103 Test loss 119.044677734375 Training loss 3871.9171826839447\n",
      "104 Test loss 129.18870544433594 Training loss 3778.11390376091\n",
      "105 Test loss 123.37295532226562 Training loss 3433.418687582016\n",
      "106 Test loss 143.4945068359375 Training loss 4074.269986510277\n",
      "107 Test loss 155.31971740722656 Training loss 3525.574038028717\n",
      "108 Test loss 131.0309600830078 Training loss 3442.13627243042\n",
      "109 Test loss 133.96890258789062 Training loss 3620.7449855804443\n",
      "110 Test loss 131.27674865722656 Training loss 3489.5795373916626\n",
      "111 Test loss 123.84992980957031 Training loss 3384.0331795215607\n",
      "112 Test loss 238.56703186035156 Training loss 3750.7367537021637\n",
      "113 Test loss 121.91146087646484 Training loss 3948.6814136505127\n",
      "114 Test loss 122.14183044433594 Training loss 4273.342868804932\n",
      "115 Test loss 152.4887237548828 Training loss 3478.532331943512\n",
      "116 Test loss 120.96603393554688 Training loss 3877.6630890369415\n",
      "117 Test loss 122.47541046142578 Training loss 3465.971429347992\n",
      "118 Test loss 158.03826904296875 Training loss 4128.957658290863\n",
      "119 Test loss 147.65040588378906 Training loss 3660.8983182907104\n",
      "120 Test loss 121.72401428222656 Training loss 3862.287295937538\n",
      "121 Test loss 119.56880187988281 Training loss 3160.9241169691086\n",
      "122 Test loss 129.49220275878906 Training loss 3898.446559906006\n",
      "123 Test loss 149.8466339111328 Training loss 4434.239794850349\n",
      "124 Test loss 151.7723846435547 Training loss 3674.1098115444183\n",
      "125 Test loss 162.47450256347656 Training loss 4727.122235536575\n",
      "126 Test loss 136.82763671875 Training loss 4978.821879863739\n",
      "127 Test loss 242.98875427246094 Training loss 4558.346862792969\n",
      "128 Test loss 149.1902618408203 Training loss 4277.282876491547\n",
      "129 Test loss 133.26084899902344 Training loss 3587.3587505817413\n",
      "130 Test loss 186.75796508789062 Training loss 3833.4117871522903\n",
      "131 Test loss 157.6883544921875 Training loss 3965.660141468048\n",
      "132 Test loss 122.84408569335938 Training loss 5159.145514965057\n",
      "133 Test loss 125.1119384765625 Training loss 4357.692985057831\n",
      "134 Test loss 136.55870056152344 Training loss 4071.0247354507446\n",
      "135 Test loss 139.20326232910156 Training loss 3817.5982728004456\n",
      "136 Test loss 153.99095153808594 Training loss 4415.505888462067\n",
      "137 Test loss 138.8289794921875 Training loss 4847.900020122528\n",
      "138 Test loss 125.7752685546875 Training loss 4653.972038269043\n",
      "139 Test loss 128.7982635498047 Training loss 4053.904229402542\n",
      "140 Test loss 124.56752014160156 Training loss 4090.6920882463455\n",
      "141 Test loss 162.36056518554688 Training loss 4535.932027339935\n",
      "142 Test loss 139.5555419921875 Training loss 4474.145987987518\n",
      "143 Test loss 201.34695434570312 Training loss 4836.927328824997\n",
      "144 Test loss 285.7216491699219 Training loss 4250.090406894684\n",
      "145 Test loss 165.1539764404297 Training loss 3492.0950314998627\n",
      "146 Test loss 164.3660125732422 Training loss 5381.714725971222\n",
      "147 Test loss 173.6907501220703 Training loss 4590.785395741463\n",
      "148 Test loss 268.2598571777344 Training loss 3835.4513313770294\n",
      "149 Test loss 164.33587646484375 Training loss 4437.246204853058\n",
      "150 Test loss 247.57444763183594 Training loss 4190.55771446228\n",
      "151 Test loss 194.50112915039062 Training loss 4455.096280574799\n",
      "152 Test loss 204.3050079345703 Training loss 5043.148481845856\n",
      "153 Test loss 257.5262451171875 Training loss 5020.272354125977\n",
      "154 Test loss 150.09725952148438 Training loss 4789.97767829895\n",
      "155 Test loss 167.51296997070312 Training loss 5815.677951097488\n",
      "156 Test loss 408.42132568359375 Training loss 4651.910596847534\n",
      "157 Test loss 176.5177001953125 Training loss 4780.333215713501\n",
      "158 Test loss 217.40663146972656 Training loss 5469.336661815643\n",
      "159 Test loss 257.46917724609375 Training loss 5486.742562294006\n",
      "160 Test loss 212.55482482910156 Training loss 4919.6426256895065\n",
      "161 Test loss 237.6937713623047 Training loss 4942.4611493349075\n",
      "162 Test loss 263.35540771484375 Training loss 5504.108686447144\n",
      "163 Test loss 211.94712829589844 Training loss 6123.159366130829\n",
      "164 Test loss 217.28622436523438 Training loss 5940.109708309174\n",
      "165 Test loss 144.40701293945312 Training loss 6693.436633110046\n",
      "166 Test loss 266.4503173828125 Training loss 5998.458076477051\n",
      "167 Test loss 413.61956787109375 Training loss 5877.975695610046\n",
      "168 Test loss 402.39251708984375 Training loss 8182.2870054244995\n",
      "169 Test loss 136.79437255859375 Training loss 6781.537467956543\n",
      "170 Test loss 269.19940185546875 Training loss 5742.355576038361\n",
      "171 Test loss 128.4632110595703 Training loss 5661.339086771011\n",
      "172 Test loss 190.51963806152344 Training loss 6484.160834312439\n",
      "173 Test loss 168.8236846923828 Training loss 6696.54030752182\n",
      "174 Test loss 421.3736877441406 Training loss 5399.0214121341705\n",
      "175 Test loss 288.640625 Training loss 6170.663635253906\n",
      "176 Test loss 140.63731384277344 Training loss 7017.828407764435\n",
      "177 Test loss 346.0245056152344 Training loss 6397.709432840347\n",
      "178 Test loss 192.45066833496094 Training loss 7897.742420196533\n",
      "179 Test loss 177.60113525390625 Training loss 6779.491196632385\n",
      "180 Test loss 162.50660705566406 Training loss 5904.738143920898\n",
      "181 Test loss 127.90249633789062 Training loss 6946.344550609589\n",
      "182 Test loss 280.3265686035156 Training loss 7156.610010147095\n",
      "183 Test loss 602.6467895507812 Training loss 6214.170200824738\n",
      "184 Test loss 208.48373413085938 Training loss 7460.806985616684\n",
      "185 Test loss 487.7912292480469 Training loss 6056.604285240173\n",
      "186 Test loss 226.56320190429688 Training loss 7028.992788076401\n",
      "187 Test loss 394.2015075683594 Training loss 7669.0426030159\n",
      "188 Test loss 172.68991088867188 Training loss 6489.519849777222\n",
      "189 Test loss 182.98497009277344 Training loss 7534.943400382996\n",
      "190 Test loss 436.4163513183594 Training loss 7367.347083330154\n",
      "191 Test loss 200.26626586914062 Training loss 7253.239513635635\n",
      "192 Test loss 872.30126953125 Training loss 8503.082954883575\n",
      "193 Test loss 300.9175109863281 Training loss 7203.132992744446\n",
      "194 Test loss 277.30987548828125 Training loss 7416.150819778442\n",
      "195 Test loss 156.85879516601562 Training loss 7858.448066711426\n",
      "196 Test loss 403.6185302734375 Training loss 7237.956527709961\n",
      "197 Test loss 417.847412109375 Training loss 9264.544816732407\n",
      "198 Test loss 269.80657958984375 Training loss 7262.938519001007\n",
      "199 Test loss 252.96456909179688 Training loss 9068.78750038147\n",
      "200 Test loss 432.01202392578125 Training loss 7260.01509642601\n",
      "201 Test loss 436.48077392578125 Training loss 7763.728176116943\n",
      "202 Test loss 221.39366149902344 Training loss 6927.221983909607\n",
      "203 Test loss 191.15237426757812 Training loss 5966.014837741852\n",
      "204 Test loss 511.3387451171875 Training loss 7700.109681129456\n",
      "205 Test loss 433.52978515625 Training loss 8097.846834659576\n",
      "206 Test loss 240.25157165527344 Training loss 7273.411471366882\n",
      "207 Test loss 396.33221435546875 Training loss 7405.916192531586\n",
      "208 Test loss 577.4310302734375 Training loss 7286.803566455841\n",
      "209 Test loss 560.6793823242188 Training loss 7131.362122058868\n",
      "210 Test loss 443.66534423828125 Training loss 9724.129703044891\n",
      "211 Test loss 145.8200225830078 Training loss 8422.07769536972\n",
      "212 Test loss 626.8223876953125 Training loss 7179.319441795349\n",
      "213 Test loss 146.97706604003906 Training loss 6549.238642692566\n",
      "214 Test loss 176.21414184570312 Training loss 8046.16068983078\n",
      "215 Test loss 551.664306640625 Training loss 8233.380284786224\n",
      "216 Test loss 161.38731384277344 Training loss 9529.96347284317\n",
      "217 Test loss 318.2394104003906 Training loss 6397.6148953437805\n",
      "218 Test loss 392.945068359375 Training loss 9588.478839874268\n",
      "219 Test loss 649.8506469726562 Training loss 9576.47560787201\n",
      "220 Test loss 248.57351684570312 Training loss 8498.19159412384\n",
      "221 Test loss 465.8126525878906 Training loss 8454.853900909424\n",
      "222 Test loss 436.65460205078125 Training loss 9068.89016008377\n",
      "223 Test loss 652.6494140625 Training loss 9005.097336292267\n",
      "224 Test loss 603.2652587890625 Training loss 9294.425352096558\n",
      "225 Test loss 240.31573486328125 Training loss 10948.881847381592\n",
      "226 Test loss 701.0576782226562 Training loss 10478.35064649582\n",
      "227 Test loss 143.21060180664062 Training loss 11569.51133108139\n",
      "228 Test loss 189.4612579345703 Training loss 11569.684649944305\n",
      "229 Test loss 309.079833984375 Training loss 9662.542074680328\n",
      "230 Test loss 629.264404296875 Training loss 11148.837195396423\n",
      "231 Test loss 213.1087646484375 Training loss 10835.615015983582\n",
      "232 Test loss 452.1632385253906 Training loss 10113.109760761261\n",
      "233 Test loss 750.962890625 Training loss 11216.17326927185\n",
      "234 Test loss 139.76121520996094 Training loss 12001.175839424133\n",
      "235 Test loss 797.1544189453125 Training loss 11016.36672782898\n",
      "236 Test loss 328.7379455566406 Training loss 10993.117666721344\n",
      "237 Test loss 753.480224609375 Training loss 10682.157815694809\n",
      "238 Test loss 239.63450622558594 Training loss 11888.48155784607\n",
      "239 Test loss 281.260986328125 Training loss 13818.489976882935\n",
      "240 Test loss 1207.6575927734375 Training loss 10778.767093658447\n",
      "241 Test loss 442.1523132324219 Training loss 13005.584421157837\n",
      "242 Test loss 183.4080810546875 Training loss 13279.59873843193\n",
      "243 Test loss 322.2145690917969 Training loss 11733.46833562851\n",
      "244 Test loss 400.4342956542969 Training loss 12305.782030582428\n",
      "245 Test loss 794.436767578125 Training loss 13524.52923822403\n",
      "246 Test loss 755.521484375 Training loss 14019.309701442719\n",
      "247 Test loss 381.5694580078125 Training loss 13477.059574127197\n",
      "248 Test loss 935.87255859375 Training loss 11639.910091400146\n",
      "249 Test loss 557.822998046875 Training loss 13280.58616566658\n",
      "250 Test loss 527.864501953125 Training loss 12308.407298922539\n",
      "251 Test loss 1055.216552734375 Training loss 11158.261143684387\n",
      "252 Test loss 356.7204284667969 Training loss 12782.052095651627\n",
      "253 Test loss 249.66339111328125 Training loss 15580.2900223732\n",
      "254 Test loss 656.8943481445312 Training loss 15235.771217346191\n",
      "255 Test loss 205.43466186523438 Training loss 15698.718378543854\n",
      "256 Test loss 511.7956848144531 Training loss 11889.77346086502\n",
      "257 Test loss 605.9329223632812 Training loss 17291.092427015305\n",
      "258 Test loss 337.30908203125 Training loss 13441.139992952347\n",
      "259 Test loss 648.4986572265625 Training loss 11494.030227661133\n",
      "260 Test loss 1296.2110595703125 Training loss 12073.258191108704\n",
      "261 Test loss 286.17083740234375 Training loss 14026.03182888031\n",
      "262 Test loss 245.5574188232422 Training loss 14992.253568172455\n",
      "263 Test loss 836.4403076171875 Training loss 13697.303894996643\n",
      "264 Test loss 505.0650634765625 Training loss 16937.527532577515\n",
      "265 Test loss 330.4459533691406 Training loss 16857.017142295837\n",
      "266 Test loss 396.22088623046875 Training loss 13090.780715703964\n",
      "267 Test loss 476.17034912109375 Training loss 14737.727577209473\n",
      "268 Test loss 475.57794189453125 Training loss 13359.765897274017\n",
      "269 Test loss 319.4074401855469 Training loss 15140.075940132141\n",
      "270 Test loss 739.4677734375 Training loss 20337.1899766922\n",
      "271 Test loss 487.4301452636719 Training loss 17642.198189735413\n",
      "272 Test loss 771.3350219726562 Training loss 15986.838849544525\n",
      "273 Test loss 432.95037841796875 Training loss 14555.525704860687\n",
      "274 Test loss 788.5427856445312 Training loss 15057.047138214111\n",
      "275 Test loss 964.521728515625 Training loss 16087.734313964844\n",
      "276 Test loss 826.1873779296875 Training loss 12636.74684715271\n",
      "277 Test loss 750.0995483398438 Training loss 14779.515542030334\n",
      "278 Test loss 425.2374267578125 Training loss 15830.269076347351\n",
      "279 Test loss 657.6126708984375 Training loss 15698.726922988892\n",
      "280 Test loss 703.8873901367188 Training loss 17586.75768184662\n",
      "281 Test loss 718.8631591796875 Training loss 14802.495963096619\n",
      "282 Test loss 1361.4805908203125 Training loss 20295.674924373627\n",
      "283 Test loss 297.7887878417969 Training loss 14709.238644599915\n",
      "284 Test loss 592.9722290039062 Training loss 14193.17100739479\n",
      "285 Test loss 398.3008117675781 Training loss 15355.014482975006\n",
      "286 Test loss 1126.2342529296875 Training loss 16601.35587310791\n",
      "287 Test loss 891.539794921875 Training loss 18566.792947292328\n",
      "288 Test loss 605.6136474609375 Training loss 14624.23459470272\n",
      "289 Test loss 1060.4002685546875 Training loss 18607.845163822174\n",
      "290 Test loss 625.1880493164062 Training loss 21393.10451555252\n",
      "291 Test loss 1221.8331298828125 Training loss 18534.004236221313\n",
      "292 Test loss 1104.05322265625 Training loss 16983.562066078186\n",
      "293 Test loss 1045.2530517578125 Training loss 18700.819173812866\n",
      "294 Test loss 1101.6737060546875 Training loss 19383.403295755386\n",
      "295 Test loss 2108.78271484375 Training loss 20880.43577480316\n",
      "296 Test loss 753.9404296875 Training loss 21523.136554718018\n",
      "297 Test loss 441.6299133300781 Training loss 16128.551768183708\n",
      "298 Test loss 827.7932739257812 Training loss 21780.378061771393\n",
      "299 Test loss 539.5743408203125 Training loss 17802.58544445038\n",
      "300 Test loss 865.1610107421875 Training loss 23027.300176620483\n",
      "301 Test loss 1656.6917724609375 Training loss 19951.24728012085\n",
      "302 Test loss 1481.9327392578125 Training loss 20865.040680885315\n",
      "303 Test loss 884.3385620117188 Training loss 21768.740445137024\n",
      "304 Test loss 1213.364013671875 Training loss 24611.10448074341\n",
      "305 Test loss 1390.642822265625 Training loss 20850.079815864563\n",
      "306 Test loss 595.7510375976562 Training loss 25472.117053985596\n",
      "307 Test loss 723.129150390625 Training loss 23201.61895275116\n",
      "308 Test loss 833.391845703125 Training loss 26189.177965164185\n",
      "309 Test loss 1788.802490234375 Training loss 20635.617272257805\n",
      "310 Test loss 1147.0771484375 Training loss 24028.25766324997\n",
      "311 Test loss 1773.503662109375 Training loss 25051.46549987793\n",
      "312 Test loss 1392.158447265625 Training loss 24563.86829018593\n",
      "313 Test loss 1590.26904296875 Training loss 25921.290195465088\n",
      "314 Test loss 1518.30712890625 Training loss 30161.036213994026\n",
      "315 Test loss 1504.4326171875 Training loss 24208.16404247284\n",
      "316 Test loss 1112.80615234375 Training loss 35447.70444107056\n",
      "317 Test loss 1014.591064453125 Training loss 31665.009448051453\n",
      "318 Test loss 1795.388427734375 Training loss 21349.164937138557\n",
      "319 Test loss 1701.8743896484375 Training loss 28489.34270608425\n",
      "320 Test loss 1530.2498779296875 Training loss 30593.55200099945\n",
      "321 Test loss 809.6263427734375 Training loss 27769.69818687439\n",
      "322 Test loss 2862.8271484375 Training loss 22576.9619294405\n",
      "323 Test loss 687.4758911132812 Training loss 26145.172333478928\n",
      "324 Test loss 201.23162841796875 Training loss 32775.462792396545\n",
      "325 Test loss 2149.903564453125 Training loss 29851.049735188484\n",
      "326 Test loss 1994.40185546875 Training loss 23636.628490924835\n",
      "327 Test loss 2792.904541015625 Training loss 32931.68824374676\n",
      "328 Test loss 1429.6600341796875 Training loss 29482.172330856323\n",
      "329 Test loss 2214.008544921875 Training loss 32919.40014505386\n",
      "330 Test loss 2055.400146484375 Training loss 25766.46178907156\n",
      "331 Test loss 1108.2930908203125 Training loss 34673.20982885361\n",
      "332 Test loss 992.123291015625 Training loss 31683.827335357666\n",
      "333 Test loss 1128.451416015625 Training loss 27801.318864822388\n",
      "334 Test loss 1425.3720703125 Training loss 26261.619631767273\n",
      "335 Test loss 1480.5313720703125 Training loss 28159.89973771572\n",
      "336 Test loss 2164.61328125 Training loss 35231.558121204376\n",
      "337 Test loss 1312.1214599609375 Training loss 30599.906061172485\n",
      "338 Test loss 1734.1246337890625 Training loss 33142.847106933594\n",
      "339 Test loss 1103.0 Training loss 30421.310645103455\n",
      "340 Test loss 1563.6802978515625 Training loss 32740.009789466858\n",
      "341 Test loss 2262.07177734375 Training loss 26331.913028240204\n",
      "342 Test loss 1586.406982421875 Training loss 30027.15019416809\n",
      "343 Test loss 1195.9019775390625 Training loss 28495.870737552643\n",
      "344 Test loss 1226.2489013671875 Training loss 29346.40983581543\n",
      "345 Test loss 1063.7830810546875 Training loss 28014.113697052002\n",
      "346 Test loss 1759.0482177734375 Training loss 32748.744218826294\n",
      "347 Test loss 837.2778930664062 Training loss 31858.110163211823\n",
      "348 Test loss 1163.8192138671875 Training loss 32223.735958099365\n",
      "349 Test loss 827.4343872070312 Training loss 28213.344387054443\n",
      "350 Test loss 1927.1611328125 Training loss 30554.51880443096\n",
      "351 Test loss 1686.7762451171875 Training loss 30831.43648481369\n",
      "352 Test loss 1544.8670654296875 Training loss 24387.08271741867\n",
      "353 Test loss 1874.393310546875 Training loss 37338.96006011963\n",
      "354 Test loss 1592.5069580078125 Training loss 36671.73838329315\n",
      "355 Test loss 1817.3353271484375 Training loss 36487.124450683594\n",
      "356 Test loss 2005.1776123046875 Training loss 33216.57292175293\n",
      "357 Test loss 800.6796875 Training loss 30558.211122512817\n",
      "358 Test loss 1489.005615234375 Training loss 34311.087821006775\n",
      "359 Test loss 973.7921752929688 Training loss 25954.415885925293\n",
      "360 Test loss 2848.470947265625 Training loss 35580.973641872406\n",
      "361 Test loss 1360.3935546875 Training loss 30815.515340805054\n",
      "362 Test loss 2006.7164306640625 Training loss 27727.94807457924\n",
      "363 Test loss 2291.787109375 Training loss 40975.587324142456\n",
      "364 Test loss 2185.938232421875 Training loss 27061.079365730286\n",
      "365 Test loss 2277.399658203125 Training loss 33039.49491596222\n",
      "366 Test loss 2274.614990234375 Training loss 30588.139635562897\n",
      "367 Test loss 1758.845947265625 Training loss 30559.184312343597\n",
      "368 Test loss 928.7301635742188 Training loss 34406.64659118652\n",
      "369 Test loss 2041.3900146484375 Training loss 27795.29999923706\n",
      "370 Test loss 1864.1099853515625 Training loss 31917.62255883217\n",
      "371 Test loss 1157.7802734375 Training loss 36574.06660461426\n",
      "372 Test loss 3862.500244140625 Training loss 35876.74626111984\n",
      "373 Test loss 1473.2235107421875 Training loss 28348.89603328705\n",
      "374 Test loss 2144.172119140625 Training loss 34664.236365795135\n",
      "375 Test loss 1039.0633544921875 Training loss 36013.184244155884\n",
      "376 Test loss 1623.140625 Training loss 29757.43158340454\n",
      "377 Test loss 1090.96826171875 Training loss 27661.288947582245\n",
      "378 Test loss 1679.266357421875 Training loss 30483.928280591965\n",
      "379 Test loss 1164.78466796875 Training loss 35070.275550842285\n",
      "380 Test loss 1408.0313720703125 Training loss 33697.016540527344\n",
      "381 Test loss 1961.4578857421875 Training loss 31601.619574546814\n",
      "382 Test loss 1548.9078369140625 Training loss 33709.384793281555\n",
      "383 Test loss 2162.39013671875 Training loss 31887.836616516113\n",
      "384 Test loss 651.57763671875 Training loss 31757.252264022827\n",
      "385 Test loss 2709.800048828125 Training loss 27795.780755519867\n",
      "386 Test loss 2443.782470703125 Training loss 32812.066853523254\n",
      "387 Test loss 2693.006103515625 Training loss 33630.566301345825\n",
      "388 Test loss 2015.4453125 Training loss 35793.524602890015\n",
      "389 Test loss 2381.693603515625 Training loss 33239.996099472046\n",
      "390 Test loss 1340.498291015625 Training loss 30970.718613624573\n",
      "391 Test loss 4649.0400390625 Training loss 35079.47122859955\n",
      "392 Test loss 2917.979248046875 Training loss 39466.40713238716\n",
      "393 Test loss 1569.8336181640625 Training loss 37878.819288253784\n",
      "394 Test loss 2515.897216796875 Training loss 38568.70632362366\n",
      "395 Test loss 3739.146728515625 Training loss 33523.15937995911\n",
      "396 Test loss 1839.5181884765625 Training loss 47596.473258018494\n",
      "397 Test loss 1161.0810546875 Training loss 32705.060992240906\n",
      "398 Test loss 1136.8804931640625 Training loss 35816.63677430153\n",
      "399 Test loss 2384.0126953125 Training loss 38437.37039852142\n",
      "400 Test loss 2778.564697265625 Training loss 37678.51310014725\n"
     ]
    }
   ],
   "source": [
    "model = changeDetectLSTM()\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.1)\n",
    "x_test, y_test = get_batch(1000,100, device)\n",
    "for epoch in range(400):\n",
    "    totalLoss = 0\n",
    "    for i in range(2**6):\n",
    "        L = random.randint(2,100)\n",
    "        # L = 10\n",
    "        x_batch, y_batch = get_batch(64, L, device)\n",
    "        optimizer.zero_grad \n",
    "        # print(y_batch.shape)\n",
    "        output = model.forward(x_batch)\n",
    "        # print(output.shape)\n",
    "        # print(y_batch[0])\n",
    "        loss = torch.nn.CrossEntropyLoss()(output[:,:,0], y_batch)\n",
    "        totalLoss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print((epoch+1),\"Test loss\",torch.nn.CrossEntropyLoss(reduction='mean')(model.forward(x_test)[:,:,0], y_test).item(), \"Training loss\", totalLoss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "f535e0a0-e311-4f6a-9fb8-9ee6b50ab8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([[0.4496, 0.4573, 0.4482, 0.4603, 0.4501, 0.4457, 0.4735, 0.4493, 0.4499,\n",
      "         0.4462, 0.4667, 0.4605, 0.4568, 0.4503, 0.4508, 0.4518, 0.4530, 0.4488,\n",
      "         0.4455, 0.4598, 0.4500, 0.4536, 0.4507, 0.4546, 0.4557, 0.4551, 0.4547,\n",
      "         0.4457, 0.4531, 0.4715, 0.4490, 0.4602, 0.4655, 0.4741, 0.4515, 0.4493,\n",
      "         0.4499, 0.4487, 0.4497, 0.4657, 0.4456, 0.4536, 0.4477, 0.4480, 0.4515,\n",
      "         0.4512, 0.4563, 0.4589, 0.4612, 0.4540, 0.4516, 0.4630, 0.4500, 0.4604,\n",
      "         0.4485, 0.4584, 0.4536, 0.4673, 0.4506, 0.4662, 0.4581, 0.4498, 0.4524,\n",
      "         0.4471, 0.4465, 0.4519, 0.4554, 0.4602, 0.4609, 0.4478, 0.4521, 0.4497,\n",
      "         0.4630, 0.4451, 0.4699, 0.4676, 0.4541, 0.4525, 0.4498, 0.4703, 0.4499,\n",
      "         0.4476, 0.4680, 0.4419, 0.4545, 0.4545, 0.4558, 0.4552, 0.4516, 0.4544,\n",
      "         0.4580, 0.4531, 0.4533, 0.4496, 0.4563, 0.4528, 0.4444, 0.4665, 0.4579,\n",
      "         0.4610]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x,y = get_batch(1, 100, device)\n",
    "print(y)\n",
    "print(nn.Sigmoid()(model(x)[:,:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "a3f3c6a0-36ec-4ba5-bb9d-c22a27fd6839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.0241, -0.0050, -0.0436,  ...,  0.0630, -0.1782,  0.0905],\n",
      "         [-0.0416, -0.0136, -0.0568,  ...,  0.0361, -0.1718,  0.0904],\n",
      "         [-0.0542, -0.0191, -0.0371,  ...,  0.0546, -0.1764,  0.0810],\n",
      "         ...,\n",
      "         [-0.0719, -0.0675, -0.0368,  ...,  0.0970, -0.2168,  0.0861],\n",
      "         [-0.0404, -0.0170, -0.0637,  ...,  0.0535, -0.1707,  0.0789],\n",
      "         [-0.0438, -0.0119, -0.0418,  ...,  0.0529, -0.1878,  0.0725]]],\n",
      "       device='cuda:0', grad_fn=<CudnnRnnBackward0>), tensor([[[ 0.3606,  0.3763, -0.0703,  ...,  0.4405, -0.0639, -0.0504],\n",
      "         [-0.1602,  0.0735,  0.1989,  ..., -0.1785, -0.3362,  0.3406],\n",
      "         [ 0.3833,  0.1238,  0.0771,  ..., -0.2267,  0.0039,  0.1872],\n",
      "         ...,\n",
      "         [ 0.0477, -0.1614,  0.4257,  ..., -0.1218, -0.0999, -0.2118],\n",
      "         [ 0.2183,  0.1890,  0.2391,  ..., -0.0024, -0.1767,  0.2191],\n",
      "         [ 0.3539,  0.1075, -0.1330,  ...,  0.2023,  0.1865,  0.1567]],\n",
      "\n",
      "        [[ 0.1841, -0.1916,  0.1030,  ..., -0.1199, -0.0565, -0.3316],\n",
      "         [ 0.2708, -0.0513,  0.1375,  ..., -0.0514,  0.0226,  0.0029],\n",
      "         [ 0.2293, -0.1262,  0.0797,  ...,  0.0062,  0.0398, -0.1160],\n",
      "         ...,\n",
      "         [ 0.3206, -0.2211,  0.1171,  ..., -0.0858, -0.2871, -0.0889],\n",
      "         [ 0.2439, -0.1291,  0.1587,  ..., -0.0200,  0.0680, -0.1142],\n",
      "         [ 0.2231, -0.1402,  0.0637,  ..., -0.0234, -0.0695, -0.1595]],\n",
      "\n",
      "        [[ 0.0701,  0.1192,  0.0980,  ...,  0.0359, -0.0475, -0.0033],\n",
      "         [-0.0464, -0.0015,  0.0441,  ...,  0.0048, -0.0827,  0.0074],\n",
      "         [-0.0815,  0.0010, -0.0035,  ..., -0.0254,  0.0032,  0.0539],\n",
      "         ...,\n",
      "         [-0.1808,  0.0359,  0.0187,  ..., -0.0930,  0.0773,  0.1740],\n",
      "         [-0.0752,  0.0244,  0.0567,  ...,  0.0040, -0.0628,  0.0536],\n",
      "         [-0.0180,  0.0627,  0.1131,  ...,  0.0078, -0.0759, -0.0309]],\n",
      "\n",
      "        [[-0.0167,  0.0488, -0.0876,  ...,  0.0214,  0.0788,  0.1387],\n",
      "         [-0.0754, -0.0093, -0.1435,  ..., -0.0124,  0.0448,  0.1228],\n",
      "         [-0.0804, -0.0005, -0.1231,  ..., -0.0592,  0.0351,  0.1422],\n",
      "         ...,\n",
      "         [-0.0053,  0.0070, -0.1649,  ..., -0.0232, -0.0051,  0.0953],\n",
      "         [ 0.0078,  0.0070, -0.1240,  ..., -0.0235,  0.0620,  0.1457],\n",
      "         [-0.0146,  0.0153, -0.0982,  ..., -0.0272,  0.0500,  0.1434]],\n",
      "\n",
      "        [[-0.0241, -0.0050, -0.0436,  ...,  0.0630, -0.1782,  0.0905],\n",
      "         [-0.0416, -0.0136, -0.0568,  ...,  0.0361, -0.1718,  0.0904],\n",
      "         [-0.0542, -0.0191, -0.0371,  ...,  0.0546, -0.1764,  0.0810],\n",
      "         ...,\n",
      "         [-0.0719, -0.0675, -0.0368,  ...,  0.0970, -0.2168,  0.0861],\n",
      "         [-0.0404, -0.0170, -0.0637,  ...,  0.0535, -0.1707,  0.0789],\n",
      "         [-0.0438, -0.0119, -0.0418,  ...,  0.0529, -0.1878,  0.0725]]],\n",
      "       device='cuda:0', grad_fn=<CudnnRnnBackward0>))\n"
     ]
    }
   ],
   "source": [
    "myRNN= nn.RNN(10,100,5).to(device)\n",
    "print(myRNN(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
