{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91bd7ef4",
   "metadata": {},
   "source": [
    "# Problem 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d3d81",
   "metadata": {},
   "source": [
    "Note for no layers, we have 784 inputs going to 10 outputs, and we assume a bias term per each.  This will yield \n",
    "$10 \\cdot 784 $ parameters, as we have each pixel weighted, and 10 outputs, with a further 10 bias terms, one for each \n",
    "output node.  Therefore there is a total of $7840 + 10 = 7850$ parameters when we don't include a hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf20ebc",
   "metadata": {},
   "source": [
    "If a model has $k$ hidden layers with $m$ nodes a piece we have the following calculation: For the first layer we have $784m + m$ parameters for intaking from the image to the first layer.  For the next $k-1$ layers, for each layer we have $m^2$ \n",
    "interconnects, and $m$ bias terms yielding $(k-1)(m^2 + m)$ parameters.  Finally we have $10m$ connections into the output layer, with an additionally $10$ bias.  Therefore, $$Params(m,k) = 784m + m + (k-1)(m^2 + m) + 10(m+1)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476809af",
   "metadata": {},
   "source": [
    "# Problem 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fda411",
   "metadata": {},
   "source": [
    "For a given number of parameters P, we have the equation $784m + m + (k-1)(m^2 + m) + 10(m+1) = P.$  We can rearrange the equation to find that $k = \\frac{P-10}{m(m+1)} - \\frac{795}{m+1} + 1$.  In order to maximize/minimize the function \n",
    "we can take the m derivative, and set the numerator of the fraction equal to zero.  In this scenerio \n",
    "we have to solve the equation $0=-795m^2 + 2(P-10)m + (P-10)$.  This has the resulting $m$ values of \n",
    "$$ m = \\frac{1}{795} \\left(P-10 \\pm \\sqrt{(P-10)^2+795(P-10)} \\right) $$.  Note that for sufficently large $P$ that \n",
    "the square root will always be positive, thus we will always have two roots, a maximum and a minimum.  However, \n",
    "for the smaller of the two roots, the limit approaches -0.5, and since we know that by our model we have to have at least 1 parameter per layer implies that solving for $m=1$ will yield $k_p$.  For the smallest $k$, we know for the smaller root $m_b$ that $\\lim_{P \\to \\infty} k(m_b) = -1$.  Thus \n",
    "the smallest possible $k$ would have to be $k=1$.  Note that if $k_p$ is not an integer then one should round $k_p$ down since $m$ is already the \n",
    "correct integer of 1 then the actual number of used parameters falls \"under budget\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6968830-b954-40a9-8bd6-c5deeecafa28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Problem 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35bb12e6-a7f1-42ff-ba7e-b4b488405a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_num(k,P):\n",
    "    if k == 1:\n",
    "        return (P-10) // 795\n",
    "    else:\n",
    "        return (int)(math.sqrt((794+k)**2 + 4*(P-10)*(k-1)))// (2*(k-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc31a06f-7666-4878-b682-973212477a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff1ecff4-a10c-451e-9e1f-ee0971a4bedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9.9%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0368690e-6fb3-411e-8894-545bfd8a6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.Tensor( testset.data ) / 256.0 - 0.5\n",
    "test_x = test_x.to(device)\n",
    "test_y = torch.Tensor( testset.targets ).long()\n",
    "test_y = test_y.to(device)\n",
    "train_x = torch.Tensor( trainset.data ) / 256.0 - 0.5\n",
    "train_x = train_x.to(device)\n",
    "train_y = torch.Tensor( trainset.targets ).long()\n",
    "train_y = train_y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b6377a8-1311-4d31-b15c-7086f4b5fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size):\n",
    "    n = x.shape[0]\n",
    "\n",
    "    batch_indices = random.sample( [ i for i in range(n) ], k = batch_size )\n",
    "\n",
    "    x_batch = x[ batch_indices ]\n",
    "    y_batch = y[ batch_indices ]\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cf9c2d23-6de6-490a-95bd-f0be6ae05d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layerTesting(nn.Module):\n",
    "    def __init__(self,k,m):\n",
    "        super(layerTesting, self).__init__()\n",
    "\n",
    "        self.layer_input = torch.nn.Linear( in_features = 28*28*1, out_features = m, bias=True )\n",
    "        self.layer_output = torch.nn.Linear( in_features = m, out_features = 10, bias=True )\n",
    "        self.linears = nn.ModuleList([nn.Linear(m, m) for i in range(k-1)])\n",
    "        self.normalize = nn.LayerNorm(m)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        output = nn.Flatten()( input_tensor )\n",
    "        output = self.layer_input(output)\n",
    "        output = nn.ELU()(output)\n",
    "        output = self.normalize(output)\n",
    "        for l in self.linears:\n",
    "            output = l(output)\n",
    "            output = nn.ELU()(output)\n",
    "            output = self.normalize(output)\n",
    "        output = self.layer_output(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b4f7904e-74d5-4c40-a80a-3555ba27285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSoftmaxRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearSoftmaxRegression, self).__init__()\n",
    "\n",
    "        self.layer_1 = torch.nn.Linear( in_features = 28*28*1, out_features = 10, bias=True )\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        flattened = nn.Flatten()( input_tensor )\n",
    "\n",
    "        logits = self.layer_1( flattened )\n",
    "\n",
    "        return logits\n",
    "\n",
    "        # NOTE: Correcting a mistake I made previously, I am outputing the results of a linear layer,\n",
    "        # For softmax to be applied elsewhere. Shiwei correctly pointed out that if we use the built in\n",
    "        # cross entropy loss function, it expects to receive these linear values, and will apply\n",
    "        # logOfSoftmax internally when calculating the loss, so that we don't have to.\n",
    "\n",
    "        # But if we want the probabilities, we do need to apply softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fe7c3f7-4edc-4a5b-8971-939e11f90c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix( model, x, y ):\n",
    "    identification_counts = np.zeros( shape = (10,10), dtype = np.int32 )\n",
    "    \n",
    "    logits = model.forward( x )\n",
    "    predicted_classes = torch.argmax( logits, dim = 1 )\n",
    "\n",
    "    n = x.shape[0]\n",
    "\n",
    "    for i in range(n):\n",
    "        actual_class = int( y[i].item() )\n",
    "        predicted_class = predicted_classes[i].item()\n",
    "        identification_counts[actual_class, predicted_class] += 1\n",
    "\n",
    "    return identification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "38314cfa-ace0-4c98-baa0-0351c8bf4ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Confusion Matrix\n",
      "[[233  22 132 323   6   4 142  61  13  44]\n",
      " [ 77  35  16  48   2   0 376 186   0 395]\n",
      " [127  31  29  95  19   8 449 131   7 136]\n",
      " [260  14  54  64   3   5 106  29   2 473]\n",
      " [357  19  36 113  22   3 156  62 102 112]\n",
      " [144  19  77 119   6   5 124  15   8 375]\n",
      " [197  22 122 181  23   8 305  34  39  27]\n",
      " [ 78   8  31 304  20   1  98  15  53 420]\n",
      " [167  33  47 104   7   2 139  25   4 446]\n",
      " [271   6  12 284  23   0 105   6  65 237]]\n"
     ]
    }
   ],
   "source": [
    "model3 = layerTesting(3,m_num(3,100000))\n",
    "model3.to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Initial Confusion Matrix\")\n",
    "print( confusion_matrix( model3, test_x, test_y ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd8395-8e07-4689-ba5b-759aeea340fb",
   "metadata": {},
   "source": [
    "Based off of Dr.Cowan's suggestions, I'm using $P=100000$ and $k=1\\ldots 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3cf0b669-f66f-40f5-8b60-c2eda0b5309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 layer(s), total Loss over Batches: 111.9888157993555\n",
      "1 layer(s), total Loss over Batches: 72.65987592935562\n",
      "1 layer(s), total Loss over Batches: 61.81042563915253\n",
      "1 layer(s), total Loss over Batches: 54.045449927449226\n",
      "1 layer(s), total Loss over Batches: 46.09091565012932\n",
      "1 layer(s), total Loss over Batches: 40.23024322092533\n",
      "1 layer(s), total Loss over Batches: 35.244326919317245\n",
      "1 layer(s), total Loss over Batches: 30.797077879309654\n",
      "1 layer(s), total Loss over Batches: 27.994732320308685\n",
      "1 layer(s), total Loss over Batches: 24.662280913442373\n",
      "1 layer(s), total Loss over Batches: 21.852006442844868\n",
      "1 layer(s), total Loss over Batches: 20.159483103081584\n",
      "1 layer(s), total Loss over Batches: 18.617620626464486\n",
      "1 layer(s), total Loss over Batches: 16.782256538048387\n",
      "1 layer(s), total Loss over Batches: 16.019007852301\n",
      "1 layer(s), total Loss over Batches: 14.633070329204202\n",
      "1 layer(s), total Loss over Batches: 13.992907313629985\n",
      "1 layer(s), total Loss over Batches: 12.648381918668747\n",
      "1 layer(s), total Loss over Batches: 12.406010435894132\n",
      "1 layer(s), total Loss over Batches: 10.437918454408646\n",
      "Current Confusion Matrix\n",
      "[[ 967    0    0    1    1    4    4    1    1    1]\n",
      " [   0 1117    3    2    1    2    4    0    6    0]\n",
      " [   1    0 1008    2    3    1    3    5    9    0]\n",
      " [   0    0    3  984    0   12    0    2    7    2]\n",
      " [   0    0    3    0  958    0    6    0    2   13]\n",
      " [   2    1    0    4    3  874    4    0    3    1]\n",
      " [   4    1    0    0    8    9  930    0    6    0]\n",
      " [   1    4    9    4    2    2    1  980    7   18]\n",
      " [   3    0    2    2    4    4    5    1  950    3]\n",
      " [   3    4    0    7   10    5    1    1    5  973]]\n",
      "\n",
      "2 layer(s), total Loss over Batches: 93.07781668007374\n",
      "2 layer(s), total Loss over Batches: 42.154163390398026\n",
      "2 layer(s), total Loss over Batches: 28.88255785778165\n",
      "2 layer(s), total Loss over Batches: 21.63181807845831\n",
      "2 layer(s), total Loss over Batches: 17.823188353329897\n",
      "2 layer(s), total Loss over Batches: 13.657530901953578\n",
      "2 layer(s), total Loss over Batches: 12.584995748475194\n",
      "2 layer(s), total Loss over Batches: 11.180000808089972\n",
      "2 layer(s), total Loss over Batches: 9.720992479473352\n",
      "2 layer(s), total Loss over Batches: 7.944428754970431\n",
      "2 layer(s), total Loss over Batches: 7.253992983605713\n",
      "2 layer(s), total Loss over Batches: 6.884550828486681\n",
      "2 layer(s), total Loss over Batches: 6.273768645245582\n",
      "2 layer(s), total Loss over Batches: 4.823822471546009\n",
      "2 layer(s), total Loss over Batches: 4.156545295380056\n",
      "2 layer(s), total Loss over Batches: 3.734671559650451\n",
      "2 layer(s), total Loss over Batches: 4.528676468646154\n",
      "2 layer(s), total Loss over Batches: 4.202677905326709\n",
      "2 layer(s), total Loss over Batches: 3.072849430842325\n",
      "2 layer(s), total Loss over Batches: 2.923630115459673\n",
      "Current Confusion Matrix\n",
      "[[ 973    2    1    0    0    0    2    0    1    1]\n",
      " [   0 1129    3    1    0    0    1    0    1    0]\n",
      " [   1    0 1022    1    2    0    1    2    3    0]\n",
      " [   1    0    7  980    0    4    0    6    4    8]\n",
      " [   0    0    1    0  971    0    4    0    0    6]\n",
      " [   2    0    0    5    2  875    2    0    4    2]\n",
      " [   3    4    1    0    5    4  939    0    2    0]\n",
      " [   1    3   11    1   11    0    0  988    3   10]\n",
      " [   1    1    3    2    2    2    0    3  957    3]\n",
      " [   3    2    0    0   14    2    0    2    3  983]]\n",
      "\n",
      "3 layer(s), total Loss over Batches: 89.67063961923122\n",
      "3 layer(s), total Loss over Batches: 39.275847397744656\n",
      "3 layer(s), total Loss over Batches: 28.133977618068457\n",
      "3 layer(s), total Loss over Batches: 21.216862440109253\n",
      "3 layer(s), total Loss over Batches: 18.132062919437885\n",
      "3 layer(s), total Loss over Batches: 15.677472149953246\n",
      "3 layer(s), total Loss over Batches: 13.22076618578285\n",
      "3 layer(s), total Loss over Batches: 11.338528637774289\n",
      "3 layer(s), total Loss over Batches: 10.5165338460356\n",
      "3 layer(s), total Loss over Batches: 9.018682667054236\n",
      "3 layer(s), total Loss over Batches: 8.541129489894956\n",
      "3 layer(s), total Loss over Batches: 8.41726800147444\n",
      "3 layer(s), total Loss over Batches: 7.161408929154277\n",
      "3 layer(s), total Loss over Batches: 6.855616327840835\n",
      "3 layer(s), total Loss over Batches: 5.565767248161137\n",
      "3 layer(s), total Loss over Batches: 5.587741962284781\n",
      "3 layer(s), total Loss over Batches: 6.604574755765498\n",
      "3 layer(s), total Loss over Batches: 4.001584599376656\n",
      "3 layer(s), total Loss over Batches: 3.727410107734613\n",
      "3 layer(s), total Loss over Batches: 4.25509754254017\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 8.56 GiB is free. Of the allocated memory 2.27 GiB is allocated by PyTorch, and 132.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m( k,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer(s), total Loss over Batches:\u001b[39m\u001b[38;5;124m\"\u001b[39m,total_loss )\n\u001b[0;32m     25\u001b[0m finalLossTest\u001b[38;5;241m.\u001b[39mappend(loss_function(model(test_x), test_y))\n\u001b[1;32m---> 26\u001b[0m finalLossTrain\u001b[38;5;241m.\u001b[39mappend(loss_function(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m)\u001b[49m, train_y))\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent Confusion Matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m( confusion_matrix( model, test_x, test_y ) )\n",
      "File \u001b[1;32mc:\\users\\hashem\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\hashem\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[92], line 17\u001b[0m, in \u001b[0;36mlayerTesting.forward\u001b[1;34m(self, input_tensor)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinears:\n\u001b[0;32m     16\u001b[0m     output \u001b[38;5;241m=\u001b[39m l(output)\n\u001b[1;32m---> 17\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mELU\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(output)\n\u001b[0;32m     19\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_output(output)\n",
      "File \u001b[1;32mc:\\users\\hashem\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\hashem\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\hashem\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\activation.py:514\u001b[0m, in \u001b[0;36mELU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\hashem\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\functional.py:1564\u001b[0m, in \u001b[0;36melu\u001b[1;34m(input, alpha, inplace)\u001b[0m\n\u001b[0;32m   1562\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39melu_(\u001b[38;5;28minput\u001b[39m, alpha)\n\u001b[0;32m   1563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1564\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 70.00 MiB. GPU 0 has a total capacity of 12.00 GiB of which 8.56 GiB is free. Of the allocated memory 2.27 GiB is allocated by PyTorch, and 132.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "P = 100000\n",
    "batch_size = 256\n",
    "finalLossTest = []\n",
    "finalLossTrain = []\n",
    "for k in range(1,10+1):\n",
    "    model = layerTesting(k,m_num(k,P))\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.0005 )\n",
    "    for epochs in range(20):\n",
    "        total_loss = 0\n",
    "        for batch in range( train_x.shape[0] // batch_size ):\n",
    "            x_batch, y_batch = get_batch(train_x, train_y, batch_size)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            logits = model( x_batch )\n",
    "            loss = loss_function( logits, y_batch )\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        print( k,\"layer(s), total Loss over Batches:\",total_loss )\n",
    "    finalLossTest.append(loss_function(model(test_x), test_y))\n",
    "    finalLossTrain.append(loss_function(model(train_x), train_y))\n",
    "    print(\"Current Confusion Matrix\")\n",
    "    print( confusion_matrix( model, test_x, test_y ) )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "70c1c91f-7abd-41a8-a4c4-80226205b64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08009275794029236\n"
     ]
    }
   ],
   "source": [
    "print(finalLoss[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2c120e3c-6b5e-4c53-91ff-c149ed8abaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'testing loss')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(1,11)],[j.item() for j in finalLossTest],'o')\n",
    "plt.xlabel(\"k values\")\n",
    "plt.ylabel(\"testing loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4adee1-3a66-4c24-a11b-5622251ec441",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(1,11)],[j.item() for j in finalLossTrain],'o')\n",
    "plt.xlabel(\"k values\")\n",
    "plt.ylabel(\"training loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ea849-571a-43dd-b48d-05c0bab30963",
   "metadata": {},
   "source": [
    "Now for comparison with the no hidden layers linear model:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
