{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91bd7ef4",
   "metadata": {},
   "source": [
    "# Problem 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d3d81",
   "metadata": {},
   "source": [
    "Note for no layers, we have 784 inputs going to 10 outputs, and we assume a bias term per each.  This will yield \n",
    "$10 \\cdot 784 $ parameters, as we have each pixel weighted, and 10 outputs, with a further 10 bias terms, one for each \n",
    "output node.  Therefore there is a total of $7840 + 10 = 7850$ parameters when we don't include a hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf20ebc",
   "metadata": {},
   "source": [
    "If a model has $k$ hidden layers with $m$ nodes a piece we have the following calculation: For the first layer we have $784m + m$ parameters for intaking from the image to the first layer.  For the next $k-1$ layers, for each layer we have $m^2$ \n",
    "interconnects, and $m$ bias terms yielding $(k-1)(m^2 + m)$ parameters.  Finally we have $10m$ connections into the output layer, with an additionally $10$ bias.  Therefore, $$Params(m,k) = 784m + m + (k-1)(m^2 + m) + 10(m+1)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476809af",
   "metadata": {},
   "source": [
    "# Problem 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fda411",
   "metadata": {},
   "source": [
    "For a given number of parameters P, we have the equation $784m + m + (k-1)(m^2 + m) + 10(m+1) = P.$  We can rearrange the equation to find that $k = \\frac{P-10}{m(m+1)} - \\frac{795}{m+1} + 1$.  In order to maximize/minimize the function \n",
    "we can take the m derivative, and set the numerator of the fraction equal to zero.  In this scenerio \n",
    "we have to solve the equation $0=-795m^2 + 2(P-10)m + (P-10)$.  This has the resulting $m$ values of \n",
    "$$ m = \\frac{1}{795} \\left(P-10 \\pm \\sqrt{(P-10)^2+795(P-10)} \\right) $$.  Note that for sufficently large $P$ that \n",
    "the square root will always be positive, thus we will always have two roots, a maximum and a minimum.  However, \n",
    "for the smaller of the two roots, the limit approaches -0.5, and since we know that by our model we have to have at least 1 parameter per layer implies that solving for $m=1$ will yield $k_p$.  For the smallest $k$, we know for the smaller root $m_b$ that $\\lim_{P \\to \\infty} k(m_b) = -1$.  Thus \n",
    "the smallest possible $k$ would have to be $k=1$.  Note that if $k_p$ is not an integer then one should round $k_p$ down since $m$ is already the \n",
    "correct integer of 1 then the actual number of used parameters falls \"under budget\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6968830-b954-40a9-8bd6-c5deeecafa28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Problem 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35bb12e6-a7f1-42ff-ba7e-b4b488405a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_num(k,P):\n",
    "    if k == 1:\n",
    "        return (P-10) // 795\n",
    "    else:\n",
    "        return (int)(math.sqrt((794+k)**2 + 4*(P-10)*(k-1)))// (2*(k-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc31a06f-7666-4878-b682-973212477a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff1ecff4-a10c-451e-9e1f-ee0971a4bedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9.9%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0368690e-6fb3-411e-8894-545bfd8a6c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.Tensor( testset.data ) / 256.0 - 0.5\n",
    "test_x = test_x.to(device)\n",
    "test_y = torch.Tensor( testset.targets ).long()\n",
    "test_y = test_y.to(device)\n",
    "train_x = torch.Tensor( trainset.data ) / 256.0 - 0.5\n",
    "train_x = train_x.to(device)\n",
    "train_y = torch.Tensor( trainset.targets ).long()\n",
    "train_y = train_y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b6377a8-1311-4d31-b15c-7086f4b5fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x, y, batch_size):\n",
    "    n = x.shape[0]\n",
    "\n",
    "    batch_indices = random.sample( [ i for i in range(n) ], k = batch_size )\n",
    "\n",
    "    x_batch = x[ batch_indices ]\n",
    "    y_batch = y[ batch_indices ]\n",
    "\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf9c2d23-6de6-490a-95bd-f0be6ae05d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layerTesting(nn.Module):\n",
    "    def __init__(self,k,m):\n",
    "        super(layerTesting, self).__init__()\n",
    "\n",
    "        self.layer_input = torch.nn.Linear( in_features = 28*28*1, out_features = m, bias=True )\n",
    "        self.layer_output = torch.nn.Linear( in_features = m, out_features = 10, bias=True )\n",
    "        self.linears = nn.ModuleList([nn.Linear(m, m) for i in range(k-1)])\n",
    "        self.normalize = nn.LayerNorm(m)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        output = nn.Flatten()( input_tensor )\n",
    "        output = self.layer_input(output)\n",
    "        output = nn.ReLU()(output)\n",
    "        output = self.normalize(output)\n",
    "        for l in self.linears:\n",
    "            output = l(output)\n",
    "            output = nn.ReLU()(output)\n",
    "            output = self.normalize(output)\n",
    "        output = self.layer_output(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fe7c3f7-4edc-4a5b-8971-939e11f90c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix( model, x, y ):\n",
    "    identification_counts = np.zeros( shape = (10,10), dtype = np.int32 )\n",
    "    \n",
    "    logits = model.forward( x )\n",
    "    predicted_classes = torch.argmax( logits, dim = 1 )\n",
    "\n",
    "    n = x.shape[0]\n",
    "\n",
    "    for i in range(n):\n",
    "        actual_class = int( y[i].item() )\n",
    "        predicted_class = predicted_classes[i].item()\n",
    "        identification_counts[actual_class, predicted_class] += 1\n",
    "\n",
    "    return identification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "38314cfa-ace0-4c98-baa0-0351c8bf4ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Confusion Matrix\n",
      "[[233  22 132 323   6   4 142  61  13  44]\n",
      " [ 77  35  16  48   2   0 376 186   0 395]\n",
      " [127  31  29  95  19   8 449 131   7 136]\n",
      " [260  14  54  64   3   5 106  29   2 473]\n",
      " [357  19  36 113  22   3 156  62 102 112]\n",
      " [144  19  77 119   6   5 124  15   8 375]\n",
      " [197  22 122 181  23   8 305  34  39  27]\n",
      " [ 78   8  31 304  20   1  98  15  53 420]\n",
      " [167  33  47 104   7   2 139  25   4 446]\n",
      " [271   6  12 284  23   0 105   6  65 237]]\n"
     ]
    }
   ],
   "source": [
    "model3 = layerTesting(3,m_num(3,100000))\n",
    "model3.to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Initial Confusion Matrix\")\n",
    "print( confusion_matrix( model3, test_x, test_y ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd8395-8e07-4689-ba5b-759aeea340fb",
   "metadata": {},
   "source": [
    "Based off of Dr.Cowan's suggestions, I'm using $P=100000$ and $k=1\\ldots 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3cf0b669-f66f-40f5-8b60-c2eda0b5309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 layer(s), total Loss over Batches: 112.13224667310715\n",
      "1 layer(s), total Loss over Batches: 47.265556663274765\n",
      "1 layer(s), total Loss over Batches: 34.30731004476547\n",
      "1 layer(s), total Loss over Batches: 28.28384868055582\n",
      "1 layer(s), total Loss over Batches: 22.089966915547848\n",
      "1 layer(s), total Loss over Batches: 19.833004884421825\n",
      "1 layer(s), total Loss over Batches: 18.212733067572117\n",
      "1 layer(s), total Loss over Batches: 15.826028142124414\n",
      "1 layer(s), total Loss over Batches: 14.187432203441858\n",
      "1 layer(s), total Loss over Batches: 13.56040854845196\n",
      "1 layer(s), total Loss over Batches: 11.824485362507403\n",
      "1 layer(s), total Loss over Batches: 10.471789793111384\n",
      "1 layer(s), total Loss over Batches: 9.833379683084786\n",
      "1 layer(s), total Loss over Batches: 9.011000362224877\n",
      "1 layer(s), total Loss over Batches: 8.571620187722147\n",
      "1 layer(s), total Loss over Batches: 8.24829336348921\n",
      "1 layer(s), total Loss over Batches: 7.8487286157906055\n",
      "1 layer(s), total Loss over Batches: 7.630080476403236\n",
      "1 layer(s), total Loss over Batches: 6.160114013589919\n",
      "1 layer(s), total Loss over Batches: 6.682900725398213\n",
      "1 layer(s), total Loss over Batches: 6.589402767363936\n",
      "1 layer(s), total Loss over Batches: 6.502876988612115\n",
      "1 layer(s), total Loss over Batches: 5.370750941801816\n",
      "1 layer(s), total Loss over Batches: 4.988460854627192\n",
      "1 layer(s), total Loss over Batches: 5.002591820433736\n",
      "1 layer(s), total Loss over Batches: 4.719413923099637\n",
      "1 layer(s), total Loss over Batches: 4.779408785048872\n",
      "1 layer(s), total Loss over Batches: 4.4985807600896806\n",
      "1 layer(s), total Loss over Batches: 3.7596401830669492\n",
      "1 layer(s), total Loss over Batches: 3.1305262639652938\n",
      "Current Confusion Matrix\n",
      "[[ 972    1    0    1    1    0    1    1    1    2]\n",
      " [   0 1123    4    0    0    1    2    0    4    1]\n",
      " [   4    1 1005    4    2    1    2    8    5    0]\n",
      " [   1    0    4  976    1   16    0    3    4    5]\n",
      " [   1    0    5    0  960    0    2    0    0   14]\n",
      " [   1    0    0    7    0  873    3    2    3    3]\n",
      " [   3    3    1    1    5    4  939    0    1    1]\n",
      " [   3    1    9    5    1    1    0 1002    1    5]\n",
      " [   4    1    2    4    3    4    2    4  946    4]\n",
      " [   2    2    0    4    5    2    0    3    0  991]]\n",
      "\n",
      "2 layer(s), total Loss over Batches: 64.51257485896349\n",
      "2 layer(s), total Loss over Batches: 24.70069432258606\n",
      "2 layer(s), total Loss over Batches: 17.6134826913476\n",
      "2 layer(s), total Loss over Batches: 13.661501608788967\n",
      "2 layer(s), total Loss over Batches: 11.632496307604015\n",
      "2 layer(s), total Loss over Batches: 8.986666088923812\n",
      "2 layer(s), total Loss over Batches: 7.9753059865906835\n",
      "2 layer(s), total Loss over Batches: 7.4170820959843695\n",
      "2 layer(s), total Loss over Batches: 6.739366909489036\n",
      "2 layer(s), total Loss over Batches: 4.81834039860405\n",
      "2 layer(s), total Loss over Batches: 5.5178959760814905\n",
      "2 layer(s), total Loss over Batches: 4.638897250872105\n",
      "2 layer(s), total Loss over Batches: 3.8767435027984902\n",
      "2 layer(s), total Loss over Batches: 5.223767205607146\n",
      "2 layer(s), total Loss over Batches: 3.6395092388847843\n",
      "2 layer(s), total Loss over Batches: 4.524924315046519\n",
      "2 layer(s), total Loss over Batches: 4.073324290919118\n",
      "2 layer(s), total Loss over Batches: 2.592771212453954\n",
      "2 layer(s), total Loss over Batches: 2.8879863614565693\n",
      "2 layer(s), total Loss over Batches: 2.8852613318013027\n",
      "2 layer(s), total Loss over Batches: 2.1094975526211783\n",
      "2 layer(s), total Loss over Batches: 3.282032080227509\n",
      "2 layer(s), total Loss over Batches: 2.1952518102771137\n",
      "2 layer(s), total Loss over Batches: 1.6973466461058706\n",
      "2 layer(s), total Loss over Batches: 2.945759067311883\n",
      "2 layer(s), total Loss over Batches: 2.394949788635131\n",
      "2 layer(s), total Loss over Batches: 1.8179275688307825\n",
      "2 layer(s), total Loss over Batches: 2.262378642539261\n",
      "2 layer(s), total Loss over Batches: 1.9691921033663675\n",
      "2 layer(s), total Loss over Batches: 1.8895257611002307\n",
      "Current Confusion Matrix\n",
      "[[ 972    0    1    1    0    1    4    1    0    0]\n",
      " [   1 1123    0    1    0    2    4    0    4    0]\n",
      " [   7    1 1011    3    3    0    1    2    4    0]\n",
      " [   1    0    3  995    0    4    0    3    4    0]\n",
      " [   0    2    2    0  956    0    6    1    0   15]\n",
      " [   3    0    0    6    0  875    4    1    2    1]\n",
      " [   3    2    0    0    1    3  948    0    1    0]\n",
      " [   0    8   12    3    0    1    0  998    2    4]\n",
      " [   2    0    2    4    1    0    2    2  957    4]\n",
      " [   3    2    0    7   11    5    0    4    1  976]]\n",
      "\n",
      "3 layer(s), total Loss over Batches: 71.01927758008242\n",
      "3 layer(s), total Loss over Batches: 26.556726895272732\n",
      "3 layer(s), total Loss over Batches: 19.18619380146265\n",
      "3 layer(s), total Loss over Batches: 15.834926415234804\n",
      "3 layer(s), total Loss over Batches: 12.0380086787045\n",
      "3 layer(s), total Loss over Batches: 10.234184711240232\n",
      "3 layer(s), total Loss over Batches: 9.062196790240705\n",
      "3 layer(s), total Loss over Batches: 9.075767022557557\n",
      "3 layer(s), total Loss over Batches: 7.153938364004716\n",
      "3 layer(s), total Loss over Batches: 6.926222140900791\n",
      "3 layer(s), total Loss over Batches: 6.421337000094354\n",
      "3 layer(s), total Loss over Batches: 5.465446122223511\n",
      "3 layer(s), total Loss over Batches: 5.393497905693948\n",
      "3 layer(s), total Loss over Batches: 4.625668679596856\n",
      "3 layer(s), total Loss over Batches: 4.370164329186082\n",
      "3 layer(s), total Loss over Batches: 4.847378584556282\n",
      "3 layer(s), total Loss over Batches: 3.5761480616638437\n",
      "3 layer(s), total Loss over Batches: 3.0890175401000306\n",
      "3 layer(s), total Loss over Batches: 5.161807164782658\n",
      "3 layer(s), total Loss over Batches: 4.1873386048246175\n",
      "3 layer(s), total Loss over Batches: 2.9773648681584746\n",
      "3 layer(s), total Loss over Batches: 2.3612464345642366\n",
      "3 layer(s), total Loss over Batches: 3.232711238146294\n",
      "3 layer(s), total Loss over Batches: 3.375734170607757\n",
      "3 layer(s), total Loss over Batches: 2.716117308184039\n",
      "3 layer(s), total Loss over Batches: 3.2356654764007544\n",
      "3 layer(s), total Loss over Batches: 2.1622491291491315\n",
      "3 layer(s), total Loss over Batches: 2.3848654667381197\n",
      "3 layer(s), total Loss over Batches: 2.3423427579109557\n",
      "3 layer(s), total Loss over Batches: 1.7349169658846222\n",
      "Current Confusion Matrix\n",
      "[[ 962    0    3    0    1    5    6    1    1    1]\n",
      " [   0 1126    2    3    0    1    1    1    1    0]\n",
      " [   0    1 1020    2    3    0    2    3    1    0]\n",
      " [   0    0    4  994    0    4    0    2    6    0]\n",
      " [   0    0    5    0  956    1    2    3    0   15]\n",
      " [   2    0    0    7    0  876    3    0    1    3]\n",
      " [   0    5    2    1    5    5  940    0    0    0]\n",
      " [   0    2    8    3    1    0    0 1009    0    5]\n",
      " [   1    0    4    4    0    4    1    1  957    2]\n",
      " [   2    1    1   13    4    4    0    4    1  979]]\n",
      "\n",
      "4 layer(s), total Loss over Batches: 74.40759860724211\n",
      "4 layer(s), total Loss over Batches: 27.936811294406652\n",
      "4 layer(s), total Loss over Batches: 22.26345937512815\n",
      "4 layer(s), total Loss over Batches: 16.940373053774238\n",
      "4 layer(s), total Loss over Batches: 13.461754477582872\n",
      "4 layer(s), total Loss over Batches: 11.919268667697906\n",
      "4 layer(s), total Loss over Batches: 11.423724830150604\n",
      "4 layer(s), total Loss over Batches: 10.315290561877191\n",
      "4 layer(s), total Loss over Batches: 8.425658366642892\n",
      "4 layer(s), total Loss over Batches: 7.733480670489371\n",
      "4 layer(s), total Loss over Batches: 6.741302530048415\n",
      "4 layer(s), total Loss over Batches: 6.692898345179856\n",
      "4 layer(s), total Loss over Batches: 5.146302749635652\n",
      "4 layer(s), total Loss over Batches: 5.490162147441879\n",
      "4 layer(s), total Loss over Batches: 6.140394553076476\n",
      "4 layer(s), total Loss over Batches: 5.926356910611503\n",
      "4 layer(s), total Loss over Batches: 4.239018783555366\n",
      "4 layer(s), total Loss over Batches: 5.640371499117464\n",
      "4 layer(s), total Loss over Batches: 3.5196351087652147\n",
      "4 layer(s), total Loss over Batches: 3.6811306703602895\n",
      "4 layer(s), total Loss over Batches: 6.67377925326582\n",
      "4 layer(s), total Loss over Batches: 3.75691942544654\n",
      "4 layer(s), total Loss over Batches: 3.07963933236897\n",
      "4 layer(s), total Loss over Batches: 3.4402384331915528\n",
      "4 layer(s), total Loss over Batches: 3.4610107725602575\n",
      "4 layer(s), total Loss over Batches: 2.869080043921713\n",
      "4 layer(s), total Loss over Batches: 2.8142829065036494\n",
      "4 layer(s), total Loss over Batches: 3.137134656892158\n",
      "4 layer(s), total Loss over Batches: 4.152903860609513\n",
      "4 layer(s), total Loss over Batches: 2.7219628524035215\n",
      "Current Confusion Matrix\n",
      "[[ 970    0    2    2    1    0    2    0    2    1]\n",
      " [   0 1132    0    1    0    0    0    1    1    0]\n",
      " [   2    2 1011    5    2    0    1    6    3    0]\n",
      " [   0    0    4  996    0    2    0    3    3    2]\n",
      " [   2    2    1    0  959    0    2    3    1   12]\n",
      " [   1    0    0   10    1  873    2    0    3    2]\n",
      " [   3    3    0    1    7    7  934    0    2    1]\n",
      " [   0    8    4    0    1    0    0 1008    1    6]\n",
      " [   1    1    1    4    0    0    0    6  959    2]\n",
      " [   2    3    0    3    5    1    1    4    2  988]]\n",
      "\n",
      "5 layer(s), total Loss over Batches: 76.98109892010689\n",
      "5 layer(s), total Loss over Batches: 31.476135656237602\n",
      "5 layer(s), total Loss over Batches: 22.788921592757106\n",
      "5 layer(s), total Loss over Batches: 18.02861100062728\n",
      "5 layer(s), total Loss over Batches: 15.136992886662483\n",
      "5 layer(s), total Loss over Batches: 14.154342820867896\n",
      "5 layer(s), total Loss over Batches: 12.639974812045693\n",
      "5 layer(s), total Loss over Batches: 10.405015531927347\n",
      "5 layer(s), total Loss over Batches: 10.241249719634652\n",
      "5 layer(s), total Loss over Batches: 9.407985869329423\n",
      "5 layer(s), total Loss over Batches: 8.257411415223032\n",
      "5 layer(s), total Loss over Batches: 7.6232145603280514\n",
      "5 layer(s), total Loss over Batches: 6.393655224237591\n",
      "5 layer(s), total Loss over Batches: 6.112509626895189\n",
      "5 layer(s), total Loss over Batches: 6.2464781971648335\n",
      "5 layer(s), total Loss over Batches: 6.077352162799798\n",
      "5 layer(s), total Loss over Batches: 5.909907938214019\n",
      "5 layer(s), total Loss over Batches: 5.525349516887218\n",
      "5 layer(s), total Loss over Batches: 5.2521099334117025\n",
      "5 layer(s), total Loss over Batches: 4.754923903150484\n",
      "5 layer(s), total Loss over Batches: 5.9543328317813575\n",
      "5 layer(s), total Loss over Batches: 3.77042406226974\n",
      "5 layer(s), total Loss over Batches: 2.9796151953050867\n",
      "5 layer(s), total Loss over Batches: 4.731275791302323\n",
      "5 layer(s), total Loss over Batches: 4.569016342400573\n",
      "5 layer(s), total Loss over Batches: 3.814290101523511\n",
      "5 layer(s), total Loss over Batches: 3.8116964944056235\n",
      "5 layer(s), total Loss over Batches: 2.8716098624281585\n",
      "5 layer(s), total Loss over Batches: 2.7957811097730882\n",
      "5 layer(s), total Loss over Batches: 4.870178420096636\n",
      "Current Confusion Matrix\n",
      "[[ 968    1    0    0    1    3    4    1    2    0]\n",
      " [   0 1127    0    1    0    1    1    1    4    0]\n",
      " [   7    0  999    5    2    0    3    6    9    1]\n",
      " [   1    1    6  976    1   13    0    4    8    0]\n",
      " [   1    0    0    0  952    1    5    1    6   16]\n",
      " [   2    0    0    8    1  864    6    1    7    3]\n",
      " [   1    2    0    0    2    3  944    0    5    1]\n",
      " [   0    3    7    6    1    0    0  994    9    8]\n",
      " [   3    0    2    1    0    4    0    1  962    1]\n",
      " [   0    2    0    3    4    6    1    5   16  972]]\n",
      "\n",
      "6 layer(s), total Loss over Batches: 85.78531213104725\n",
      "6 layer(s), total Loss over Batches: 35.22286934405565\n",
      "6 layer(s), total Loss over Batches: 26.814150411635637\n",
      "6 layer(s), total Loss over Batches: 19.912294682115316\n",
      "6 layer(s), total Loss over Batches: 17.790503911674023\n",
      "6 layer(s), total Loss over Batches: 16.940455434843898\n",
      "6 layer(s), total Loss over Batches: 13.487663617357612\n",
      "6 layer(s), total Loss over Batches: 12.013162285089493\n",
      "6 layer(s), total Loss over Batches: 11.814715594053268\n",
      "6 layer(s), total Loss over Batches: 11.145256334915757\n",
      "6 layer(s), total Loss over Batches: 9.74485997390002\n",
      "6 layer(s), total Loss over Batches: 9.323858101852238\n",
      "6 layer(s), total Loss over Batches: 8.545406483346596\n",
      "6 layer(s), total Loss over Batches: 7.4001664449460804\n",
      "6 layer(s), total Loss over Batches: 7.6823879044968635\n",
      "6 layer(s), total Loss over Batches: 6.093559625558555\n",
      "6 layer(s), total Loss over Batches: 7.5654641832225025\n",
      "6 layer(s), total Loss over Batches: 5.929521685931832\n",
      "6 layer(s), total Loss over Batches: 6.009389034006745\n",
      "6 layer(s), total Loss over Batches: 5.95985169603955\n",
      "6 layer(s), total Loss over Batches: 5.093703561811708\n",
      "6 layer(s), total Loss over Batches: 7.039523302111775\n",
      "6 layer(s), total Loss over Batches: 4.784119460964575\n",
      "6 layer(s), total Loss over Batches: 3.9364714231342077\n",
      "6 layer(s), total Loss over Batches: 5.5337823843583465\n",
      "6 layer(s), total Loss over Batches: 4.426013941061683\n",
      "6 layer(s), total Loss over Batches: 5.357809884706512\n",
      "6 layer(s), total Loss over Batches: 4.457746784435585\n",
      "6 layer(s), total Loss over Batches: 3.82047768344637\n",
      "6 layer(s), total Loss over Batches: 2.765275388257578\n",
      "Current Confusion Matrix\n",
      "[[ 964    0    3    1    1    3    3    1    2    2]\n",
      " [   0 1123    1    1    0    0    1    7    2    0]\n",
      " [   1    2 1010    3    0    0    4    8    4    0]\n",
      " [   0    0    6  989    0    5    0    6    3    1]\n",
      " [   1    1    2    0  936    1   11    6    2   22]\n",
      " [   3    1    0   13    1  865    3    2    4    0]\n",
      " [   3    3    3    1    7   17  922    0    2    0]\n",
      " [   2    1    5    0    0    0    0 1009    6    5]\n",
      " [   3    1    4    5    2    6    0    4  948    1]\n",
      " [   1    5    1   10    6    6    2   10    4  964]]\n",
      "\n",
      "7 layer(s), total Loss over Batches: 93.48981420695782\n",
      "7 layer(s), total Loss over Batches: 37.69954725727439\n",
      "7 layer(s), total Loss over Batches: 28.231026686728\n",
      "7 layer(s), total Loss over Batches: 22.776258412748575\n",
      "7 layer(s), total Loss over Batches: 19.857543386518955\n",
      "7 layer(s), total Loss over Batches: 16.214249696582556\n",
      "7 layer(s), total Loss over Batches: 14.913205014541745\n",
      "7 layer(s), total Loss over Batches: 14.0002116356045\n",
      "7 layer(s), total Loss over Batches: 13.237026751972735\n",
      "7 layer(s), total Loss over Batches: 10.855059714987874\n",
      "7 layer(s), total Loss over Batches: 10.284578206017613\n",
      "7 layer(s), total Loss over Batches: 8.565443676430732\n",
      "7 layer(s), total Loss over Batches: 9.061971283052117\n",
      "7 layer(s), total Loss over Batches: 8.923152958508581\n",
      "7 layer(s), total Loss over Batches: 7.841030190233141\n",
      "7 layer(s), total Loss over Batches: 8.079477969091386\n",
      "7 layer(s), total Loss over Batches: 7.535074986983091\n",
      "7 layer(s), total Loss over Batches: 7.389200867852196\n",
      "7 layer(s), total Loss over Batches: 6.045001796912402\n",
      "7 layer(s), total Loss over Batches: 7.403259789687581\n",
      "7 layer(s), total Loss over Batches: 4.804031865671277\n",
      "7 layer(s), total Loss over Batches: 5.590153201017529\n",
      "7 layer(s), total Loss over Batches: 6.605293798726052\n",
      "7 layer(s), total Loss over Batches: 5.291100575996097\n",
      "7 layer(s), total Loss over Batches: 5.456686775898561\n",
      "7 layer(s), total Loss over Batches: 5.08113138214685\n",
      "7 layer(s), total Loss over Batches: 5.077609569998458\n",
      "7 layer(s), total Loss over Batches: 4.565738674718887\n",
      "7 layer(s), total Loss over Batches: 4.265639663324691\n",
      "7 layer(s), total Loss over Batches: 4.9915423293132335\n",
      "Current Confusion Matrix\n",
      "[[ 968    0    3    1    0    3    0    0    1    4]\n",
      " [   0 1128    0    0    2    1    2    1    1    0]\n",
      " [   2    3 1010    4    2    0    0    9    2    0]\n",
      " [   0    0    5  976    0    3    0   11    4   11]\n",
      " [   0    0    3    0  954    0    1    2    0   22]\n",
      " [   2    0    0   16    1  852    1    2    8   10]\n",
      " [   4    1    4    2    8    3  932    0    4    0]\n",
      " [   1    5    7    0    4    0    0  996    1   14]\n",
      " [   0    2    8    8    0    2    0    2  943    9]\n",
      " [   0    1    0    1    5    1    1    1    0  999]]\n",
      "\n",
      "8 layer(s), total Loss over Batches: 101.88351683318615\n",
      "8 layer(s), total Loss over Batches: 41.785486064851284\n",
      "8 layer(s), total Loss over Batches: 30.003713835030794\n",
      "8 layer(s), total Loss over Batches: 24.446686670184135\n",
      "8 layer(s), total Loss over Batches: 21.98605756647885\n",
      "8 layer(s), total Loss over Batches: 19.509249042719603\n",
      "8 layer(s), total Loss over Batches: 17.395328994840384\n",
      "8 layer(s), total Loss over Batches: 15.447146851569414\n",
      "8 layer(s), total Loss over Batches: 14.95471095573157\n",
      "8 layer(s), total Loss over Batches: 13.295750444754958\n",
      "8 layer(s), total Loss over Batches: 12.08754675462842\n",
      "8 layer(s), total Loss over Batches: 12.165791778825223\n",
      "8 layer(s), total Loss over Batches: 11.992780765518546\n",
      "8 layer(s), total Loss over Batches: 10.6009905571118\n",
      "8 layer(s), total Loss over Batches: 9.353036186192185\n",
      "8 layer(s), total Loss over Batches: 9.336253061890602\n",
      "8 layer(s), total Loss over Batches: 9.293430386343971\n",
      "8 layer(s), total Loss over Batches: 9.02377196541056\n",
      "8 layer(s), total Loss over Batches: 8.12787399534136\n",
      "8 layer(s), total Loss over Batches: 7.894272758858278\n",
      "8 layer(s), total Loss over Batches: 7.589227407705039\n",
      "8 layer(s), total Loss over Batches: 6.863044544588774\n",
      "8 layer(s), total Loss over Batches: 6.959351507714018\n",
      "8 layer(s), total Loss over Batches: 6.562904962571338\n",
      "8 layer(s), total Loss over Batches: 6.728066507028416\n",
      "8 layer(s), total Loss over Batches: 6.58349329023622\n",
      "8 layer(s), total Loss over Batches: 5.870432231226005\n",
      "8 layer(s), total Loss over Batches: 5.477855872828513\n",
      "8 layer(s), total Loss over Batches: 4.68002911133226\n",
      "8 layer(s), total Loss over Batches: 5.398987055930775\n",
      "Current Confusion Matrix\n",
      "[[ 972    0    1    0    0    1    1    2    3    0]\n",
      " [   0 1123    0    0    1    2    1    1    7    0]\n",
      " [   4    0 1001   11    1    2    3    5    5    0]\n",
      " [   0    0    3  987    0   11    0    2    4    3]\n",
      " [   1    0    2    1  948    0    4    3    1   22]\n",
      " [   2    0    0    2    0  872    3    1    2   10]\n",
      " [   3    2    1    1    1    7  940    0    3    0]\n",
      " [   0    2   10    3    1    0    0 1007    1    4]\n",
      " [   2    0    6    9    1    9    0    1  940    6]\n",
      " [   3    3    0    4    2    2    0   10    2  983]]\n",
      "\n",
      "9 layer(s), total Loss over Batches: 105.33455761522055\n",
      "9 layer(s), total Loss over Batches: 43.864848751574755\n",
      "9 layer(s), total Loss over Batches: 32.682070925831795\n",
      "9 layer(s), total Loss over Batches: 26.461439438164234\n",
      "9 layer(s), total Loss over Batches: 23.637143336236477\n",
      "9 layer(s), total Loss over Batches: 21.8376325648278\n",
      "9 layer(s), total Loss over Batches: 18.859812418930233\n",
      "9 layer(s), total Loss over Batches: 16.945133788511157\n",
      "9 layer(s), total Loss over Batches: 17.191354458220303\n",
      "9 layer(s), total Loss over Batches: 15.124145399779081\n",
      "9 layer(s), total Loss over Batches: 14.875995595008135\n",
      "9 layer(s), total Loss over Batches: 12.987140192650259\n",
      "9 layer(s), total Loss over Batches: 11.378572803456336\n",
      "9 layer(s), total Loss over Batches: 10.827043720986694\n",
      "9 layer(s), total Loss over Batches: 10.547279311809689\n",
      "9 layer(s), total Loss over Batches: 11.606523156631738\n",
      "9 layer(s), total Loss over Batches: 10.641844784840941\n",
      "9 layer(s), total Loss over Batches: 9.741977483034134\n",
      "9 layer(s), total Loss over Batches: 8.594555460382253\n",
      "9 layer(s), total Loss over Batches: 8.357123288325965\n",
      "9 layer(s), total Loss over Batches: 7.4441146198660135\n",
      "9 layer(s), total Loss over Batches: 8.40838033426553\n",
      "9 layer(s), total Loss over Batches: 6.633166622486897\n",
      "9 layer(s), total Loss over Batches: 8.166472758632153\n",
      "9 layer(s), total Loss over Batches: 7.007704259827733\n",
      "9 layer(s), total Loss over Batches: 5.71781698288396\n",
      "9 layer(s), total Loss over Batches: 7.817173012532294\n",
      "9 layer(s), total Loss over Batches: 6.712539662141353\n",
      "9 layer(s), total Loss over Batches: 6.177594197215512\n",
      "9 layer(s), total Loss over Batches: 5.810232440009713\n",
      "Current Confusion Matrix\n",
      "[[ 971    0    0    0    0    1    4    2    0    2]\n",
      " [   0 1123    2    2    0    1    1    2    4    0]\n",
      " [   7    0 1005    9    1    0    0    5    5    0]\n",
      " [   1    0    5  982    0    4    0    4    6    8]\n",
      " [   2    1    2    0  929    0    6    2    6   34]\n",
      " [   5    0    0    6    1  862    6    1    5    6]\n",
      " [   5    2    0    0    3    2  945    0    1    0]\n",
      " [   1    4   11    4    2    0    0  989    5   12]\n",
      " [   7    0    2    3    0    2    2    2  955    1]\n",
      " [   5    3    0    7    1    3    1    0    4  985]]\n",
      "\n",
      "10 layer(s), total Loss over Batches: 116.85889267921448\n",
      "10 layer(s), total Loss over Batches: 45.86411950737238\n",
      "10 layer(s), total Loss over Batches: 35.359826415777206\n",
      "10 layer(s), total Loss over Batches: 27.71391359716654\n",
      "10 layer(s), total Loss over Batches: 23.893485503271222\n",
      "10 layer(s), total Loss over Batches: 21.40596979856491\n",
      "10 layer(s), total Loss over Batches: 21.237488079816103\n",
      "10 layer(s), total Loss over Batches: 16.86958801653236\n",
      "10 layer(s), total Loss over Batches: 17.388496714644134\n",
      "10 layer(s), total Loss over Batches: 15.58216882776469\n",
      "10 layer(s), total Loss over Batches: 14.102613359689713\n",
      "10 layer(s), total Loss over Batches: 13.309329193085432\n",
      "10 layer(s), total Loss over Batches: 12.773060092702508\n",
      "10 layer(s), total Loss over Batches: 12.351254384964705\n",
      "10 layer(s), total Loss over Batches: 11.152356940321624\n",
      "10 layer(s), total Loss over Batches: 11.3038993710652\n",
      "10 layer(s), total Loss over Batches: 9.606172024970874\n",
      "10 layer(s), total Loss over Batches: 10.37779428716749\n",
      "10 layer(s), total Loss over Batches: 11.01310938084498\n",
      "10 layer(s), total Loss over Batches: 8.710800122004002\n",
      "10 layer(s), total Loss over Batches: 7.6898549816105515\n",
      "10 layer(s), total Loss over Batches: 8.640829682117328\n",
      "10 layer(s), total Loss over Batches: 8.028975626919419\n",
      "10 layer(s), total Loss over Batches: 7.6462343339808285\n",
      "10 layer(s), total Loss over Batches: 7.749486362095922\n",
      "10 layer(s), total Loss over Batches: 7.60533879743889\n",
      "10 layer(s), total Loss over Batches: 7.744461385766044\n",
      "10 layer(s), total Loss over Batches: 7.109896469628438\n",
      "10 layer(s), total Loss over Batches: 6.052999977953732\n",
      "10 layer(s), total Loss over Batches: 5.373938774107955\n",
      "Current Confusion Matrix\n",
      "[[ 972    0    0    1    0    1    2    1    3    0]\n",
      " [   0 1115    0    3    0    2    0   10    5    0]\n",
      " [   6    1  999    3    5    0    1   10    6    1]\n",
      " [   0    0    5  983    0    7    0    3    9    3]\n",
      " [   1    1    0    0  942    0    4    3    5   26]\n",
      " [   2    0    0   11    0  864    5    0    7    3]\n",
      " [   2    2    0    0    3    4  941    0    6    0]\n",
      " [   1    1    8   22    0    1    0  978    4   13]\n",
      " [   0    1    3    2    0    6    0    4  952    6]\n",
      " [   3    2    0    1    9    5    1    1    9  978]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "P = 100000\n",
    "batch_size = 256\n",
    "finalLoss = []\n",
    "for k in range(1,10+1):\n",
    "    model = layerTesting(k,m_num(k,P))\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.0005 )\n",
    "    for epochs in range(30):\n",
    "        total_loss = 0\n",
    "        for batch in range( train_x.shape[0] // batch_size ):\n",
    "            x_batch, y_batch = get_batch(train_x, train_y, batch_size)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            logits = model( x_batch )\n",
    "            loss = loss_function( logits, y_batch )\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        print( k,\"layer(s), total Loss over Batches:\",total_loss )\n",
    "    finalLoss.append(loss_function(model(test_x), test_y))\n",
    "    print(\"Current Confusion Matrix\")\n",
    "    print( confusion_matrix( model, test_x, test_y ) )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "70c1c91f-7abd-41a8-a4c4-80226205b64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08009275794029236\n"
     ]
    }
   ],
   "source": [
    "print(finalLoss[1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2c120e3c-6b5e-4c53-91ff-c149ed8abaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'testing loss')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(1,11)],[j.item() for j in finalLoss],'o')\n",
    "plt.xlabel(\"k values\")\n",
    "plt.ylabel(\"testing loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e89a4-8452-4aeb-a8bb-0470a2134eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
