{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbmrY6aTcwCn",
        "outputId": "5a119c31-8762-459e-89bc-a345f2009746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "train_X = torch.Tensor( trainset.data/255.0 - 0.5 )\n",
        "train_X = train_X.permute( 0, 3, 1, 2 )\n",
        "\n",
        "train_X = train_X.to( device )\n",
        "\n",
        "test_X = torch.Tensor( testset.data/255.0 - 0.5 )\n",
        "test_X = test_X.permute( 0, 3, 1, 2 )\n",
        "\n",
        "test_X = test_X.to( device )\n",
        "\n",
        "train_Y = torch.Tensor( np.asarray( trainset.targets ) ).long()\n",
        "train_Y = train_Y.to( device )\n",
        "test_Y = torch.Tensor( np.asarray( testset.targets ) ).long()\n",
        "test_Y = test_Y.to( device )\n",
        "\n",
        "def get_batch(x, y, batch_size):\n",
        "  n = x.shape[0]\n",
        "\n",
        "  batch_indices = random.sample( [ i for i in range(n) ], k = batch_size )\n",
        "\n",
        "  x_batch = x[ batch_indices ]\n",
        "  y_batch = y[ batch_indices ]\n",
        "\n",
        "  return x_batch, y_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH19RuQcc2f2",
        "outputId": "e8e8247d-f570-4203-f2fc-789bc54aa48f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 47557363.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CIFARModel, self).__init__()\n",
        "\n",
        "    self.conv_layer_1 = nn.Conv2d(in_channels = 3, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.conv_layer_2 = nn.Conv2d(in_channels = 20, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.conv_layer_3 = nn.Conv2d(in_channels = 20, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.conv_layer_4 = nn.Conv2d(in_channels = 20, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "\n",
        "    self.linear_layer = torch.nn.Linear( in_features = 20*24*24, out_features = 10, bias=True )\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    output = self.conv_layer_1( input_tensor )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.conv_layer_2( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.conv_layer_3( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.conv_layer_4( output )\n",
        "\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = nn.Flatten()( output )\n",
        "    output = self.linear_layer( output )\n",
        "    return output"
      ],
      "metadata": {
        "id": "aNUm_URbc_TE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix( model, x, y ):\n",
        "  identification_counts = np.zeros( shape = (10,10), dtype = np.int32 )\n",
        "\n",
        "  logits = model( x )\n",
        "  predicted_classes = torch.argmax( logits, dim = 1 )\n",
        "\n",
        "  n = x.shape[0]\n",
        "\n",
        "  for i in range(n):\n",
        "    actual_class = int( y[i].item() )\n",
        "    predicted_class = predicted_classes[i].item()\n",
        "    identification_counts[actual_class, predicted_class] += 1\n",
        "\n",
        "  return identification_counts"
      ],
      "metadata": {
        "id": "SaU-dVwAdXCs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_model = CIFARModel()\n",
        "\n",
        "cifar_model.to( device )\n",
        "\n",
        "print( cifar_model )\n",
        "confusion_matrix( cifar_model, test_X, test_Y )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmHzxt5RdZow",
        "outputId": "d7560ff5-738e-4925-abbb-c6141288aa41"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFARModel(\n",
            "  (conv_layer_1): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv_layer_2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv_layer_3): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv_layer_4): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (linear_layer): Linear(in_features=11520, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0, 1000]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_optimizer = optim.Adam(cifar_model.parameters(), lr = 0.001 )\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "print(\"Initial Test Loss:\", loss_function( cifar_model( test_X ), test_Y ).item() )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22KnWMI5dcUz",
        "outputId": "f4609317-4734-4285-e512-01a4338bb7fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Test Loss: 2.331589937210083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "for epochs in range(10):\n",
        "  total_loss = 0\n",
        "  for batch in range( train_X.shape[0] // batch_size ):\n",
        "    x_batch, y_batch = get_batch(train_X, train_Y, batch_size)\n",
        "\n",
        "    cnn_optimizer.zero_grad()\n",
        "    logits = cifar_model( x_batch )\n",
        "    loss = loss_function( logits, y_batch )\n",
        "\n",
        "    loss.backward()\n",
        "    cnn_optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print( \"Average Total Loss over Batches:\", total_loss / ( train_X.shape[0] // batch_size ) )\n",
        "  print( confusion_matrix( cifar_model, test_X, test_Y ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAMozejfdgID",
        "outputId": "a2d8199a-ada2-4ce5-852c-67b73c769727"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Total Loss over Batches: 2.22976244594574\n",
            "[[421 114  25  78   7   1  53  16 167 118]\n",
            " [ 43 403  19  69  31  23 140  27 109 136]\n",
            " [ 93  72 106 109  50  19 422  30  41  58]\n",
            " [ 47  84  74 188  31  43 397  37  32  67]\n",
            " [ 41  34  42  77  46  13 615  44  38  50]\n",
            " [ 52 134  58 173  56  72 338  26  42  49]\n",
            " [ 17  38  24  75  16   5 749  25   9  42]\n",
            " [ 45 130  55  79  49  11 288 122  37 184]\n",
            " [180 128  10  93   2  15  37  17 356 162]\n",
            " [ 36 186  22  45  18   3 122  33 129 406]]\n",
            "Average Total Loss over Batches: 1.9956081354522706\n",
            "[[427  60  46  18  22  26  57  55 127 162]\n",
            " [ 28 340  21  10  74  77 148  76  93 133]\n",
            " [ 69  43 123  20  82  79 472  61  20  31]\n",
            " [ 29  28  65  51  86 136 469  86  15  35]\n",
            " [ 30  20  37  20  67  45 655  64  21  41]\n",
            " [ 27  49  50  23 100 194 441  75  17  24]\n",
            " [  6  15  18  22  40  25 795  58   6  15]\n",
            " [ 21  64  32  25  98  44 335 272   9 100]\n",
            " [219  78  24  13  29  65  35  53 271 213]\n",
            " [ 35 138  19  23  64  21 145 143  69 343]]\n",
            "Average Total Loss over Batches: 1.8680519980621337\n",
            "[[308  57  56  33  17  24  28  47 255 175]\n",
            " [ 10 433  21  46  33  29  84  17  62 265]\n",
            " [ 36  60 187  79 178  61 209  86  47  57]\n",
            " [ 22  47  84 173 120 144 211  75  28  96]\n",
            " [ 34  38  49  58 347  54 260  81  31  48]\n",
            " [ 11  62  74 107 145 256 171  82  36  56]\n",
            " [  6  24  40  45 115  46 625  33   6  60]\n",
            " [ 17  45  51  68 166  62  80 368  18 125]\n",
            " [ 54  82  14  29   5  42  18  30 465 261]\n",
            " [ 21 140  19  35  18  19  72  34  55 587]]\n",
            "Average Total Loss over Batches: 1.6987939631652833\n",
            "[[530 109  29  49   7  26   3  16 136  95]\n",
            " [ 20 688   3  39   0  26   1  12  52 159]\n",
            " [ 85 120 184 161  91 190  28  69  18  54]\n",
            " [ 34 144  28 305  21 260  28  52  17 111]\n",
            " [ 64 101 105 140 232 139  49 110  16  44]\n",
            " [ 24  98  37 190  16 484   9  60  21  61]\n",
            " [  8 155  44 273  75  95 221  45   5  79]\n",
            " [ 34  92  22 141  27 165   0 353  12 154]\n",
            " [196 134   5  42   2  34   4   9 435 139]\n",
            " [ 38 335   5  50   0  19   3  11  54 485]]\n",
            "Average Total Loss over Batches: 1.5516739665794372\n",
            "[[528  60  53  26  25  23  21  30 160  74]\n",
            " [ 34 603  11  14  18  18  25  19  63 195]\n",
            " [ 84  25 410  71 112  91  91  61  29  26]\n",
            " [ 32  38 103 261  62 226 146  60  24  48]\n",
            " [ 56  15 250  56 324  67 123  71  24  14]\n",
            " [ 20  26 127 157  40 416  79  86  26  23]\n",
            " [  7  19 109  89  82  56 589  20  16  13]\n",
            " [ 26  12  97  79  62 140  48 484  13  39]\n",
            " [148  71  20  16  11  28  17  16 576  97]\n",
            " [ 49 209  28  34  17  20  32  44  75 492]]\n",
            "Average Total Loss over Batches: 1.4547565765953063\n",
            "[[410  78  32  19  60  19  28  23 255  76]\n",
            " [ 11 644   5   8  13   9  42  24  71 173]\n",
            " [ 55  25 247  55 242  95 143  76  37  25]\n",
            " [ 19  33  51 173 109 229 215  91  24  56]\n",
            " [ 38  16  86  34 452  67 194  78  22  13]\n",
            " [  8  25  57  91 109 420 120 117  29  24]\n",
            " [  5  17  40  33 118  27 698  34  12  16]\n",
            " [ 14  14  35  39 125 112  80 529  10  42]\n",
            " [ 67  93  10  15  24  25  17  17 654  78]\n",
            " [ 35 233  15  19  22  13  49  45  84 485]]\n",
            "Average Total Loss over Batches: 1.3903409751701354\n",
            "[[545  32  53  27  29  10  14  23 223  44]\n",
            " [ 48 589  11   9  16   4  10  24 117 172]\n",
            " [ 91  12 364  73 195  50  54 105  35  21]\n",
            " [ 33  30 100 258 139 116 136 101  42  45]\n",
            " [ 64  11 154  54 497  23  61  97  26  13]\n",
            " [ 16  16  89 191 119 287  78 152  30  22]\n",
            " [ 13  20  96  44 208  14 517  39  22  27]\n",
            " [ 39   7  64  59 104  65  34 566  21  41]\n",
            " [125  48  17  11   9  14   3  15 707  51]\n",
            " [ 71 158  25  20  16   6  19  50 118 517]]\n",
            "Average Total Loss over Batches: 1.3268491472053527\n",
            "[[464  46  45  23  37   9  21  19 219 117]\n",
            " [ 17 643   7   9   7   6  16  11  81 203]\n",
            " [ 67  23 338  57 194  65  91 103  32  30]\n",
            " [ 18  30  75 274 101 161 140 109  28  64]\n",
            " [ 34  16 131  59 448  42 105 119  21  25]\n",
            " [  9  24  60 167  99 346  73 166  30  26]\n",
            " [  5  18  60  49 106  23 651  50  12  26]\n",
            " [ 14  10  40  55  93  65  30 612  15  66]\n",
            " [ 74  54  20   9  10  11   2  16 721  83]\n",
            " [ 32 167   9  16  11  11  22  39  62 631]]\n",
            "Average Total Loss over Batches: 1.2614704728794097\n",
            "[[524  51  68  21  40   6  33  26 168  63]\n",
            " [ 26 714  11  11  11   3  30  17  47 130]\n",
            " [ 59  17 402  58 133  56 138  84  32  21]\n",
            " [ 19  23  73 286  95 177 197  69  30  31]\n",
            " [ 30  12 152  63 374  46 197 107  14   5]\n",
            " [ 13   9  95 166  84 370 104 123  23  13]\n",
            " [  3   6  53  40  62  27 760  31   8  10]\n",
            " [ 15   9  53  52  84  69  61 621   8  28]\n",
            " [103  83  26   9  16  10  17  14 661  61]\n",
            " [ 41 185  17  22  16  13  46  53  54 553]]\n",
            "Average Total Loss over Batches: 1.190602359981537\n",
            "[[507  23  68  51  20   6  17  16 249  43]\n",
            " [ 28 639  10  37   4   7  18  11 113 133]\n",
            " [ 77   9 371 166 105  58  70  81  47  16]\n",
            " [ 20  20  56 527  53 116  88  65  37  18]\n",
            " [ 37   8 135 165 358  44 101 125  26   1]\n",
            " [ 14   8  73 302  55 366  47  97  27  11]\n",
            " [  9   8  57 149  57  27 633  31  19  10]\n",
            " [ 16   5  43 116  65  62  19 630   7  37]\n",
            " [ 59  39  24  24   6   9   3  12 787  37]\n",
            " [ 41 134  11  48  10  10  22  42 109 573]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARModelBatch(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CIFARModelBatch, self).__init__()\n",
        "\n",
        "    self.conv_layer_1 = nn.Conv2d(in_channels = 3, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.batch_norm_1 = torch.nn.BatchNorm2d(20)\n",
        "    self.conv_layer_2 = nn.Conv2d(in_channels = 20, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.batch_norm_2 = torch.nn.BatchNorm2d(20)\n",
        "    self.conv_layer_3 = nn.Conv2d(in_channels = 20, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.batch_norm_3 = torch.nn.BatchNorm2d(20)\n",
        "    self.conv_layer_4 = nn.Conv2d(in_channels = 20, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.batch_norm_4 = torch.nn.BatchNorm2d(20)\n",
        "\n",
        "    self.linear_layer = torch.nn.Linear( in_features = 20*24*24, out_features = 10, bias=True )\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    output = self.conv_layer_1( input_tensor )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.batch_norm_1( output )\n",
        "    output = self.conv_layer_2( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.batch_norm_2( output )\n",
        "    output = self.conv_layer_3( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.batch_norm_3( output )\n",
        "    output = self.conv_layer_4( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.batch_norm_4( output )\n",
        "\n",
        "\n",
        "    output = nn.Flatten()( output )\n",
        "    output = self.linear_layer( output )\n",
        "    return output"
      ],
      "metadata": {
        "id": "Y-oS-rJ9fjmN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_model = CIFARModelBatch()\n",
        "\n",
        "cifar_model.to( device )\n",
        "\n",
        "print( cifar_model )\n",
        "confusion_matrix( cifar_model, test_X, test_Y )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTyeJGsp8_0m",
        "outputId": "2e49bd15-e6bf-4ebe-fade-a443d7db70d9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFARModelBatch(\n",
            "  (conv_layer_1): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch_norm_1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_layer_2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch_norm_2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_layer_3): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch_norm_3): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_layer_4): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch_norm_4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (linear_layer): Linear(in_features=11520, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 58,  92,  45,  53,  41,  86, 226,  33,  98, 268],\n",
              "       [ 64,  80, 134,  91,  51,  57, 144, 148,  97, 134],\n",
              "       [ 61,  83,  79, 138, 115,  94, 101, 133,  99,  97],\n",
              "       [ 66,  76, 116, 114, 109,  97, 107, 150,  80,  85],\n",
              "       [ 55,  69,  51, 180, 154,  89,  70, 203,  83,  46],\n",
              "       [ 85,  69, 101, 122, 107,  93, 115, 125,  60, 123],\n",
              "       [ 33,  62,  93, 174, 132,  83,  29, 254,  94,  46],\n",
              "       [ 93,  97,  53, 141, 101, 126,  79, 154,  68,  88],\n",
              "       [ 66,  67,  57,  53,  21,  96, 184,  51,  78, 327],\n",
              "       [ 63, 109, 100, 119,  59,  71, 112, 109, 125, 133]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_optimizer = optim.Adam(cifar_model.parameters(), lr = 0.001 )\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "print(\"Initial Test Loss:\", loss_function( cifar_model( test_X ), test_Y ).item() )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mgf1OcMZ9GlH",
        "outputId": "1d1bce06-1431-47f0-a5a2-daab13045d03"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Test Loss: 2.4296348094940186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "for epochs in range(10):\n",
        "  total_loss = 0\n",
        "  cifar_model.train()\n",
        "  for batch in range( train_X.shape[0] // batch_size ):\n",
        "    x_batch, y_batch = get_batch(train_X, train_Y, batch_size)\n",
        "\n",
        "    cnn_optimizer.zero_grad()\n",
        "    logits = cifar_model( x_batch )\n",
        "    loss = loss_function( logits, y_batch )\n",
        "\n",
        "    loss.backward()\n",
        "    cnn_optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print( \"Average Total Loss over Batches:\", total_loss / ( train_X.shape[0] // batch_size ) )\n",
        "  cifar_model.eval()\n",
        "  print( confusion_matrix( cifar_model, test_X, test_Y ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSK91UOK80Or",
        "outputId": "a0a21207-c673-4bc5-801c-8b65c79bcd7b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Total Loss over Batches: 1.9165997318077088\n",
            "[[536  36 104  27  28  25  11  20 118  95]\n",
            " [ 51 622   7  10   8   9  13   6  53 221]\n",
            " [ 70  17 371 127 150  76  76  63  24  26]\n",
            " [ 10  17  56 369 111 203 107  53  28  46]\n",
            " [ 30  14  97  91 469  62 111  98  16  12]\n",
            " [  9   5  77 187 102 443  35 107  11  24]\n",
            " [ 10  13  45  89  68  40 651  42  14  28]\n",
            " [ 14   7  30  57  98  85  14 633   7  55]\n",
            " [159  59  21  16   7  15  12  10 595 106]\n",
            " [ 28  99  16  18   8   5  28  16  46 736]]\n",
            "Average Total Loss over Batches: 1.0995260130691529\n",
            "[[748  33  60  11  16   8  17  16  48  43]\n",
            " [ 39 776   5   8   5   7  18   6  16 120]\n",
            " [102  12 475  55 117  75  88  54  14   8]\n",
            " [ 30  21  90 375  64 222 108  44  18  28]\n",
            " [ 41   7  81  50 511  55 134 107   9   5]\n",
            " [ 23  10 115 120  49 546  39  78  10  10]\n",
            " [ 10   6  33  61  35  23 809  12   6   5]\n",
            " [ 20   8  59  35  66  90  10 687   1  24]\n",
            " [149  85  25  12  10   7   7   7 622  76]\n",
            " [ 31 106  15  13   8   3   5  22  34 763]]\n",
            "Average Total Loss over Batches: 0.90152442923069\n",
            "[[732  15  43  44  10  17   7  23  68  41]\n",
            " [ 54 704   6  27   2   8  13   4  50 132]\n",
            " [103   2 379 127  93 143  52  69  13  19]\n",
            " [ 28  10  50 513  56 207  61  37  15  23]\n",
            " [ 36   6  69 113 517  74  49 121   8   7]\n",
            " [ 10   1  29 238  37 588  12  70   4  11]\n",
            " [ 10   4  33 118  56  43 697  22   9   8]\n",
            " [ 15   4  28  73  51 118   6 681   4  20]\n",
            " [169  37  12  28   8  10   6   8 666  56]\n",
            " [ 51  60   5  16   4   8   4  18  38 796]]\n",
            "Average Total Loss over Batches: 0.784111731004715\n",
            "[[616  24  56  23  19  11   6  23 168  54]\n",
            " [ 17 760   4  11   5   2   8   6  45 142]\n",
            " [ 94  14 409  98 112  97  67  62  27  20]\n",
            " [ 27  15  44 429 100 197  89  44  29  26]\n",
            " [ 29   7  70  66 598  41  62 101  18   8]\n",
            " [ 12   7  37 176  78 546  35  77  24   8]\n",
            " [ 11   8  51  76  61  32 724  11  14  12]\n",
            " [ 14   4  32  44  76  90  12 695   8  25]\n",
            " [ 65  43  11  13   3   2   3   8 813  39]\n",
            " [ 13  82   5   9   6   4   4  24  54 799]]\n",
            "Average Total Loss over Batches: 0.683144571518898\n",
            "[[679  25  85  21  16  11  16  19  74  54]\n",
            " [ 34 743   6  15   7   3  15   5  21 151]\n",
            " [ 86   9 450  88 104  80  87  61  16  19]\n",
            " [ 16  14  74 440  72 178 104  45  23  34]\n",
            " [ 28   7  76  77 569  42  78 101  12  10]\n",
            " [ 14   2  54 178  68 545  33  79  11  16]\n",
            " [  7  11  45  77  42  38 749   9   8  14]\n",
            " [ 18   8  49  44  67  91  15 671   6  31]\n",
            " [115  41  23  15  15   8  13   7 689  74]\n",
            " [ 27  74   7   9   5   9   3  12  31 823]]\n",
            "Average Total Loss over Batches: 0.5964629501605034\n",
            "[[649  29  71  38  23  11   3  21 114  41]\n",
            " [ 35 752  10  16   7   4  10   8  46 112]\n",
            " [ 96   9 523  77 120  60  35  44  26  10]\n",
            " [ 22  13 103 464 109 138  62  47  24  18]\n",
            " [ 20   2 126  80 604  22  36  90  17   3]\n",
            " [ 10   4 101 230  73 459  22  86   7   8]\n",
            " [ 13  10 109 101  85  21 629  14  12   6]\n",
            " [ 18  11  71  49  86  67   3 674   7  14]\n",
            " [ 89  38  20  16   6   8   6   7 758  52]\n",
            " [ 35  89  10  12   9   4   4  27  46 764]]\n",
            "Average Total Loss over Batches: 0.5323505341422557\n",
            "[[711  27  73  18  15   7   8  18  85  38]\n",
            " [ 42 756  14  11   1   3  12   8  39 114]\n",
            " [102  10 494  73 109  70  59  55  15  13]\n",
            " [ 23  15  99 414  85 173  82  54  20  35]\n",
            " [ 34   5 108  65 597  33  57  80  13   8]\n",
            " [ 14   5  82 202  62 503  28  86   6  12]\n",
            " [  9  11 102  77  59  23 690   6  13  10]\n",
            " [ 22   7  73  46  92  79   8 644   3  26]\n",
            " [112  51  24  15   8   2   6   5 732  45]\n",
            " [ 47  88  15  10   9   5   6  20  40 760]]\n",
            "Average Total Loss over Batches: 0.47891821133136747\n",
            "[[727  26  45  23  13  15   3  16  92  40]\n",
            " [ 51 743   6   9   4   5  15   2  51 114]\n",
            " [131   6 432 112  85  77  75  48  24  10]\n",
            " [ 32  16  79 452  77 168  92  39  29  16]\n",
            " [ 47   6 108  98 495  43  88  88  16  11]\n",
            " [ 11   7  60 232  59 511  35  64  13   8]\n",
            " [ 15   7  88  85  45  19 719   9  11   2]\n",
            " [ 33   8  66  71  64  91  13 623  12  19]\n",
            " [128  40  20  14   7   2   6   5 742  36]\n",
            " [ 55 105   9  16   7  10  11  16  69 702]]\n",
            "Average Total Loss over Batches: 0.4274218777680397\n",
            "[[681  26  80  23  36  16  10  18  73  37]\n",
            " [ 54 750  13   9  10  10  22   8  27  97]\n",
            " [ 88  10 459 105 107 105  64  46  11   5]\n",
            " [ 23  12  92 441  73 203  99  37  12   8]\n",
            " [ 22   5 105  99 527  80  83  69  10   0]\n",
            " [ 12   3  74 220  56 554  32  40   5   4]\n",
            " [  9   9 102  89  50  37 688   5   8   3]\n",
            " [ 17   8  68  75  84 139  12 580   6  11]\n",
            " [126  59  30  23  13  16   6   7 684  36]\n",
            " [ 66 116  18  20  16  15  13  25  46 665]]\n",
            "Average Total Loss over Batches: 0.38708286805748937\n",
            "[[588  38 125  30  15   7  12  23 124  38]\n",
            " [ 41 755  21  14  10   2  15   7  46  89]\n",
            " [ 59  10 548  96  82  59  81  38  21   6]\n",
            " [ 17  15 122 455  74 144  97  42  21  13]\n",
            " [ 22   2 177  95 480  39  94  77  13   1]\n",
            " [ 11   6 107 235  56 455  37  75  12   6]\n",
            " [  8   5 103  88  37  20 720  10   6   3]\n",
            " [ 19   8  89  90  73  60  15 628  10   8]\n",
            " [ 81  60  39  16   7   4  10  14 745  24]\n",
            " [ 57 121  25  24   8   5  11  36  56 657]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARModelDropout(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CIFARModelDropout, self).__init__()\n",
        "\n",
        "    self.conv_layer_1 = nn.Conv2d(in_channels = 3, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.conv_layer_2 = nn.Conv2d(in_channels = 20, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.conv_layer_3 = nn.Conv2d(in_channels = 20, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.conv_layer_4 = nn.Conv2d(in_channels = 20, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.dropout_4 = torch.nn.Dropout(0.5)\n",
        "\n",
        "    self.linear_layer = torch.nn.Linear( in_features = 20*24*24, out_features = 10, bias=True )\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    output = self.conv_layer_1( input_tensor )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.conv_layer_2( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.conv_layer_3( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.conv_layer_4( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.dropout_4( output )\n",
        "\n",
        "\n",
        "    output = nn.Flatten()( output )\n",
        "    output = self.linear_layer( output )\n",
        "    return output"
      ],
      "metadata": {
        "id": "Oen0zVdPGZqH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_model = CIFARModelDropout()\n",
        "\n",
        "cifar_model.to( device )\n",
        "\n",
        "print( cifar_model )\n",
        "confusion_matrix( cifar_model, test_X, test_Y )"
      ],
      "metadata": {
        "id": "f-Cy_OQNWE1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de2d830-eed3-499c-a8ed-831ae0df9435"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFARModelDropout(\n",
            "  (conv_layer_1): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv_layer_2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv_layer_3): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv_layer_4): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (dropout_4): Dropout(p=0.5, inplace=False)\n",
            "  (linear_layer): Linear(in_features=11520, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[324, 215,  44,   4,  32,  15,   4,  36,  77, 249],\n",
              "       [320, 222,  50,   3,  34,  11,   5,  24,  76, 255],\n",
              "       [335, 223,  38,   6,  35,  12,   3,  34,  65, 249],\n",
              "       [325, 216,  49,   4,  35,  12,   4,  34,  74, 247],\n",
              "       [308, 225,  49,   7,  32,  10,   4,  32,  72, 261],\n",
              "       [303, 232,  48,   2,  45,  10,   5,  25,  79, 251],\n",
              "       [317, 221,  47,   2,  39,  12,   4,  28,  74, 256],\n",
              "       [296, 226,  54,   1,  41,  12,   5,  26,  66, 273],\n",
              "       [336, 209,  46,   5,  34,  10,   3,  36,  59, 262],\n",
              "       [313, 229,  49,   4,  37,  12,   4,  39,  71, 242]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_optimizer = optim.Adam(cifar_model.parameters(), lr = 0.001 )\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "print(\"Initial Test Loss:\", loss_function( cifar_model( test_X ), test_Y ).item() )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFGE1TsBpQCr",
        "outputId": "fcc0ce3a-ec1c-4002-9206-eae01cb1e41e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Test Loss: 2.367394208908081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "for epochs in range(10):\n",
        "  total_loss = 0\n",
        "  cifar_model.train()\n",
        "  for batch in range( train_X.shape[0] // batch_size ):\n",
        "    x_batch, y_batch = get_batch(train_X, train_Y, batch_size)\n",
        "\n",
        "    cnn_optimizer.zero_grad()\n",
        "    logits = cifar_model( x_batch )\n",
        "    loss = loss_function( logits, y_batch )\n",
        "\n",
        "    loss.backward()\n",
        "    cnn_optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print( \"Average Total Loss over Batches:\", total_loss / ( train_X.shape[0] // batch_size ) )\n",
        "  cifar_model.eval()\n",
        "  print( confusion_matrix( cifar_model, test_X, test_Y ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rdi2mrSpVdI",
        "outputId": "c967bc01-7d15-4ab4-9e92-6dfb1c5a3864"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Total Loss over Batches: 2.298037365989685\n",
            "[[337  70  94  20  18  99  52  19 235  56]\n",
            " [ 90 254  32  20  50 114 109  51 165 115]\n",
            " [ 81  65 159  47  89  74 287  68  75  55]\n",
            " [ 74  55 132 111  62 153 209  56  65  83]\n",
            " [ 47  51  69  53  71  63 458  74  55  59]\n",
            " [ 96  66 100  85  51 267 148  56  77  54]\n",
            " [ 23  55  74  43  58  69 512  74  21  71]\n",
            " [110 105  80  59  88  64 143  82  84 185]\n",
            " [104  71  23  18  19 132  27  23 470 113]\n",
            " [ 75 132  27  16  29  30  76  45 253 317]]\n",
            "Average Total Loss over Batches: 2.0116825002670287\n",
            "[[441 118  39  26   6   5  30  31 183 121]\n",
            " [ 26 533  41  22  34   8  90  18  92 136]\n",
            " [ 92 109 303  65  39  14 196  85  58  39]\n",
            " [ 43 100 166 160  35  56 241  77  57  65]\n",
            " [ 53  58 234  47 122  14 256 131  43  42]\n",
            " [ 48 105 170 121  42 132 180  80  89  33]\n",
            " [ 12  41 137  58  43   9 601  38  20  41]\n",
            " [ 49 105 125  52  56  24  78 347  45 119]\n",
            " [134 141  15  33   4  11  28  21 485 128]\n",
            " [ 40 267  28  26  15   1  84  23 108 408]]\n",
            "Average Total Loss over Batches: 1.7813088577651977\n",
            "[[450  49  20  68  26  14  19  72 239  43]\n",
            " [ 31 462  10  64  22  32  50  42 149 138]\n",
            " [ 63  51 136 144 233  43 134 130  41  25]\n",
            " [ 24  55  48 320  89  89 157 111  57  50]\n",
            " [ 41  26  38  97 394  36 164 150  39  15]\n",
            " [ 20  51  57 200 145 217 112 122  59  17]\n",
            " [  6  21  35 127 157  22 535  54  12  31]\n",
            " [ 27  35  42  87 113  56  44 510  38  48]\n",
            " [116  70   7  67   4  20  14  43 576  83]\n",
            " [ 36 187   8  69  13  12  50  56 200 369]]\n",
            "Average Total Loss over Batches: 1.683010705795288\n",
            "[[411  62  29   8  17  30  34  56 294  59]\n",
            " [ 18 610   5   4   7  31  37  35 111 142]\n",
            " [ 66  56 211  30 120  65 256 116  57  23]\n",
            " [ 39  50  53  71  61 193 334 107  38  54]\n",
            " [ 41  32  84  14 286  57 298 140  36  12]\n",
            " [ 19  61  56  39  69 340 193 140  54  29]\n",
            " [  5  34  40  15  68  40 718  49  12  19]\n",
            " [ 40  41  33  21  63  98  82 542  26  54]\n",
            " [ 76  87   8   7   5  31  30  22 642  92]\n",
            " [ 33 255  11  12   8  15  55  47 156 408]]\n",
            "Average Total Loss over Batches: 1.5827786757659912\n",
            "[[468  57  16  92  14  24  27  66 149  87]\n",
            " [ 30 582   3  36   3  31  23  44  46 202]\n",
            " [ 75  22 155 173  66 105 233 113  36  22]\n",
            " [ 22  20  27 398  20 180 205  78  17  33]\n",
            " [ 36   9  57 127 195 101 311 132  20  12]\n",
            " [  7  18  37 235  21 414 108 122  20  18]\n",
            " [  3   9  27 176  28  66 639  34   5  13]\n",
            " [ 17  23  18 114  21 105  70 578  12  42]\n",
            " [105  78   1  57   9  42  18  40 526 124]\n",
            " [ 36 193   3  59   2  18  40  64  68 517]]\n",
            "Average Total Loss over Batches: 1.514703727836609\n",
            "[[427  65  41  40  21  27  20  37 220 102]\n",
            " [ 23 668   7  21   3  19  10  16  60 173]\n",
            " [ 57  32 270 113 143 105 126  83  41  30]\n",
            " [ 24  22  67 369  44 211 131  60  21  51]\n",
            " [ 25  12 112  89 356  91 146 116  30  23]\n",
            " [ 13  16  76 186  58 416  76  95  30  34]\n",
            " [  3  15  76 141  97  65 535  30   8  30]\n",
            " [ 17  20  31  79  59 118  25 565  23  63]\n",
            " [ 71  82  10  32   9  27  11  20 614 124]\n",
            " [ 29 222   9  28   5  15  18  28  83 563]]\n",
            "Average Total Loss over Batches: 1.4641215430831909\n",
            "[[491 100  34  22  23  14  29  42 161  84]\n",
            " [ 27 748   4   7   5   7  12  17  38 135]\n",
            " [ 79  32 249  62 182  56 167 101  37  35]\n",
            " [ 29  32  63 273 104 132 215  83  20  49]\n",
            " [ 48  15  83  46 432  46 171 123  20  16]\n",
            " [ 14  28  75 163 117 305 126 115  26  31]\n",
            " [  7  19  64  79 121  25 618  32   4  31]\n",
            " [ 21  25  29  50  89  60  41 609  15  61]\n",
            " [110 131   7  14   8  14  20  24 551 121]\n",
            " [ 37 299   6  14   4   9  26  27  51 527]]\n",
            "Average Total Loss over Batches: 1.4007376167106629\n",
            "[[568  55  28  21  14  20  24  26 181  63]\n",
            " [ 49 682   4   9   5  16   5  12  63 155]\n",
            " [ 95  20 204  98 185 121 118  82  41  36]\n",
            " [ 42  19  50 315  61 232 131  89  17  44]\n",
            " [ 57   9  60  83 412 104 122 117  21  15]\n",
            " [ 32  11  46 168  76 448  67  98  30  24]\n",
            " [ 12  12  43  97  99  69 586  45  12  25]\n",
            " [ 43  10  20  55  61 114  20 600  17  60]\n",
            " [134  92   5  20   7  24  11  13 620  74]\n",
            " [ 61 208   4  16   5  15  21  24  83 563]]\n",
            "Average Total Loss over Batches: 1.3664625010299682\n",
            "[[474  42  93  42  25  23  30  38 179  54]\n",
            " [ 29 632  14  17  11  21  19  23  54 180]\n",
            " [ 52  10 321  98 197  97 108  75  26  16]\n",
            " [ 19   7  70 348  79 223 150  73   9  22]\n",
            " [ 25   8 100  73 450  96 122 105  14   7]\n",
            " [ 12   6  72 189  85 462  71  78  17   8]\n",
            " [  2   5  65  89 106  45 629  35   8  16]\n",
            " [ 15   4  34  73  90 113  27 615   8  21]\n",
            " [ 97  74  24  27  18  26  21  19 613  81]\n",
            " [ 44 149  16  22  11  18  39  47  61 593]]\n",
            "Average Total Loss over Batches: 1.3224954841804504\n",
            "[[593  69  46  21  14   8  25  21 142  61]\n",
            " [ 41 695   6   6   7   7   9  13  45 171]\n",
            " [ 85  19 402  92 112  64 102  74  27  23]\n",
            " [ 40  15  71 395  79 141 135  78  15  31]\n",
            " [ 48  10 144  78 381  63 133 118  14  11]\n",
            " [ 23  12  80 250  61 354  61 117  22  20]\n",
            " [  6  13  71  99  69  27 636  42   9  28]\n",
            " [ 33  12  40  77  63  61  20 634  11  49]\n",
            " [131  86  21  20  16   6  13  16 605  86]\n",
            " [ 60 174  14  13   9   5  23  21  58 623]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFARModelBatchNormAndDropout(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CIFARModelBatchNormAndDropout, self).__init__()\n",
        "\n",
        "    self.conv_layer_1 = nn.Conv2d(in_channels = 3, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.batch_norm_1 = torch.nn.BatchNorm2d(20)\n",
        "    self.conv_layer_2 = nn.Conv2d(in_channels = 20, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.batch_norm_2 = torch.nn.BatchNorm2d(20)\n",
        "    self.conv_layer_3 = nn.Conv2d(in_channels = 20, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.batch_norm_3 = torch.nn.BatchNorm2d(20)\n",
        "    self.conv_layer_4 = nn.Conv2d(in_channels = 20, out_channels = 20, kernel_size = 3, stride = 1, bias=True)\n",
        "    self.batch_norm_4 = torch.nn.BatchNorm2d(20)\n",
        "    self.dropout_4 = torch.nn.Dropout(0.5)\n",
        "\n",
        "\n",
        "    self.linear_layer = torch.nn.Linear( in_features = 20*24*24, out_features = 10, bias=True )\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    output = self.conv_layer_1( input_tensor )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.batch_norm_1( output )\n",
        "    output = self.conv_layer_2( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.batch_norm_2( output )\n",
        "    output = self.conv_layer_3( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.batch_norm_3( output )\n",
        "    output = self.conv_layer_4( output )\n",
        "    output = nn.Sigmoid()( output )\n",
        "    output = self.batch_norm_4( output )\n",
        "    output = self.dropout_4( output )\n",
        "\n",
        "\n",
        "    output = nn.Flatten()( output )\n",
        "    output = self.linear_layer( output )\n",
        "    return output"
      ],
      "metadata": {
        "id": "h-k2J-nvpZhr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_model = CIFARModelBatchNormAndDropout()\n",
        "\n",
        "cifar_model.to( device )\n",
        "\n",
        "print( cifar_model )\n",
        "confusion_matrix( cifar_model, test_X, test_Y )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3OsqHvKq9Qw",
        "outputId": "ce37b0f3-8d40-42af-cd27-d36b8e89d518"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFARModelBatchNormAndDropout(\n",
            "  (conv_layer_1): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch_norm_1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_layer_2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch_norm_2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_layer_3): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch_norm_3): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_layer_4): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batch_norm_4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout_4): Dropout(p=0.5, inplace=False)\n",
            "  (linear_layer): Linear(in_features=11520, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[184,  95, 112, 199,  49,  39,  79,  65,  82,  96],\n",
              "       [121, 134,  93, 157, 111,  85,  88,  78,  66,  67],\n",
              "       [114,  93, 103,  90, 105, 113,  97,  95,  91,  99],\n",
              "       [ 83, 102, 121, 103, 117, 107,  98, 109,  85,  75],\n",
              "       [ 92,  82, 109, 100, 105, 150,  85, 110,  79,  88],\n",
              "       [122, 103, 115,  81, 106, 100,  94, 100,  99,  80],\n",
              "       [ 67,  86,  82,  72, 134, 159,  98, 132,  81,  89],\n",
              "       [ 91,  95, 111, 131, 132,  88,  99,  87,  86,  80],\n",
              "       [171, 101,  87, 225,  54,  51,  96,  63,  74,  78],\n",
              "       [ 82, 121, 110, 202,  91,  97, 106,  72,  49,  70]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_optimizer = optim.Adam(cifar_model.parameters(), lr = 0.001 )\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "print(\"Initial Test Loss:\", loss_function( cifar_model( test_X ), test_Y ).item() )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jyp9n7OWrA4v",
        "outputId": "6429d382-ec59-45a1-b239-d8430714f7a1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Test Loss: 2.5393030643463135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "for epochs in range(10):\n",
        "  total_loss = 0\n",
        "  cifar_model.train()\n",
        "  for batch in range( train_X.shape[0] // batch_size ):\n",
        "    x_batch, y_batch = get_batch(train_X, train_Y, batch_size)\n",
        "\n",
        "    cnn_optimizer.zero_grad()\n",
        "    logits = cifar_model( x_batch )\n",
        "    loss = loss_function( logits, y_batch )\n",
        "\n",
        "    loss.backward()\n",
        "    cnn_optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print( \"Average Total Loss over Batches:\", total_loss / ( train_X.shape[0] // batch_size ) )\n",
        "  cifar_model.eval()\n",
        "  print( confusion_matrix( cifar_model, test_X, test_Y ) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40_8Kr_-rD6N",
        "outputId": "4a8b932f-bd3c-41bc-ab75-f7fcea125219"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Total Loss over Batches: 2.029434037246704\n",
            "[[599  49  24  13  25  20  36  21 164  49]\n",
            " [ 48 688   6   9   4  21  14  20  51 139]\n",
            " [102  24 229 107 155 142 121  72  25  23]\n",
            " [ 21  21  28 309  81 275 157  63  16  29]\n",
            " [ 47  18  56  90 370 120 136 140  15   8]\n",
            " [ 18  11  35 184  70 492  71  96  14   9]\n",
            " [  9  25  30  92  43  50 703  23   7  18]\n",
            " [ 27  12  14  71  64 138  38 580  12  44]\n",
            " [178  82   7  16   6  16  22  12 588  73]\n",
            " [ 66 195   7  20   9  28  33  26  63 553]]\n",
            "Average Total Loss over Batches: 1.2454391029930114\n",
            "[[678  35  39  19   8  14  12  11 150  34]\n",
            " [ 52 772   5  10   3  10   9   2  60  77]\n",
            " [100  15 401  63  82 188  54  35  41  21]\n",
            " [ 21  20  50 356  52 337  47  34  49  34]\n",
            " [ 36   5 114  85 429 109  63 124  25  10]\n",
            " [ 12  10  68 142  25 623   9  59  28  24]\n",
            " [  8   6  65 103  53  92 618  16  21  18]\n",
            " [ 21  15  30  56  58 136   6 629   8  41]\n",
            " [107  80  10   8   3  11   4   4 742  31]\n",
            " [ 55 135  12  16   7  16   7  13  78 661]]\n",
            "Average Total Loss over Batches: 1.0411816386795043\n",
            "[[696  17  71  22  32   5  20  11  80  46]\n",
            " [ 30 739   8  21   4   5  22   4  38 129]\n",
            " [ 68   4 414  73 180 107  81  42  13  18]\n",
            " [ 15  12  54 446 115 190  91  45  14  18]\n",
            " [ 19   2  45  51 681  34  79  73  10   6]\n",
            " [  9   2  53 166  91 583  27  55   5   9]\n",
            " [  4   2  32  58  68  28 790  10   4   4]\n",
            " [ 11   8  24  41 132  84  15 660   5  20]\n",
            " [113  58  13  16  17   9  14   6 709  45]\n",
            " [ 31  58   7  14  16   7  16  23  39 789]]\n",
            "Average Total Loss over Batches: 0.9592112421417236\n",
            "[[659  31  38  12  23   4   7  20 151  55]\n",
            " [ 11 777   1   8   3   2   8   5  43 142]\n",
            " [ 94  12 422  54 146  91  62  79  22  18]\n",
            " [ 33  25  52 440  85 181  71  63  28  22]\n",
            " [ 22   7  41  44 655  28  56 120  20   7]\n",
            " [ 14   5  48 145  54 593  21  94  15  11]\n",
            " [ 13  11  44  58  72  22 752  19   4   5]\n",
            " [ 13  10  24  22  70  73   5 749   6  28]\n",
            " [ 50  49   8   9  14   4   6  10 803  47]\n",
            " [ 12  57   7   5  10   4   5  20  47 833]]\n",
            "Average Total Loss over Batches: 0.9020127507638931\n",
            "[[716  34  32  14  14   3   5  10 122  50]\n",
            " [ 19 802   4  13   1   3   2   4  33 119]\n",
            " [109  12 482  78  86  80  52  55  26  20]\n",
            " [ 34  21  58 509  62 177  54  40  24  21]\n",
            " [ 34   3  76  80 585  45  49 104  17   7]\n",
            " [ 18   5  51 194  43 577   7  72  19  14]\n",
            " [ 19   8  47  87  56  27 723  17  10   6]\n",
            " [ 17  10  29  49  55  78   4 730   6  22]\n",
            " [ 79  56   5  11   5   2   5   7 785  45]\n",
            " [ 36  66   7   9   9   3   3  14  38 815]]\n",
            "Average Total Loss over Batches: 0.8731889737462998\n",
            "[[721  28  80  20  34   9  12  10  47  39]\n",
            " [ 24 811  11   9   3   3  17   5  18  99]\n",
            " [ 59   5 576  52 110  64  82  34   5  13]\n",
            " [ 10  16 120 409  76 187 132  25   7  18]\n",
            " [ 12   4  92  44 665  23  92  58   8   2]\n",
            " [  4   2  84 157  73 590  36  43   4   7]\n",
            " [  5   2  70  39  41  13 821   6   1   2]\n",
            " [ 11   6  57  34  98  91  10 670   1  22]\n",
            " [120  61  31  19  13   8  12   8 688  40]\n",
            " [ 39  83  11  13  10   4   9  13  14 804]]\n",
            "Average Total Loss over Batches: 0.8472522318124771\n",
            "[[670  27  74  27  15   8   6  19 105  49]\n",
            " [ 21 767  10  16   4   5  13   3  39 122]\n",
            " [ 62   4 493  98  82  94  85  48  19  15]\n",
            " [ 12  11  74 549  46 181  64  30  15  18]\n",
            " [ 15   2  71  99 570  43  81 104  11   4]\n",
            " [  4   2  50 227  34 605  19  49   3   7]\n",
            " [  7   2  47 105  29  30 762  10   4   4]\n",
            " [ 11   2  42  66  50  99   4 708   2  16]\n",
            " [ 50  39  16  24   9   6   6   4 808  38]\n",
            " [ 26  39  10  20   8   6   3  12  46 830]]\n",
            "Average Total Loss over Batches: 0.8059015879821777\n",
            "[[726  31  38  17  19   5   9  15  74  66]\n",
            " [ 30 788   3   6   1   2  13   5  32 120]\n",
            " [ 88   6 459  60 163  57  65  59  22  21]\n",
            " [ 28  16  70 402 113 156 121  35  30  29]\n",
            " [ 30   5  46  34 697  19  61  88  16   4]\n",
            " [  8   2  54 148  83 549  36  84  19  17]\n",
            " [  9   6  36  31  84  16 780  17  10  11]\n",
            " [ 11   6  27  27  85  61   7 740   4  32]\n",
            " [100  48   7   9  10   1   6   5 767  47]\n",
            " [ 26  62   6   5   8   1   5  18  26 843]]\n",
            "Average Total Loss over Batches: 0.7913355104160309\n",
            "[[773  27  36  11  26  10  11  13  51  42]\n",
            " [ 29 780   3   7   6   4  16   5  27 123]\n",
            " [ 92   6 456  71 148  88  79  42   6  12]\n",
            " [ 23   5  62 529  82 183  66  21  11  18]\n",
            " [ 23   1  42  54 715  25  57  73   8   2]\n",
            " [  8   2  42 212  58 595  19  55   2   7]\n",
            " [  8   2  44  75  75  19 762   6   3   6]\n",
            " [ 10   4  37  41  95  88   6 695   1  23]\n",
            " [116  52  15  20  18   6   9   9 722  33]\n",
            " [ 34  57   7  10  11   5   6  15  18 837]]\n",
            "Average Total Loss over Batches: 0.7734747132778168\n",
            "[[711  31  37  12  20   0   6  16 105  62]\n",
            " [ 21 794   5  11   1   1   8   3  40 116]\n",
            " [ 88   6 530  65  98  51  68  52  26  16]\n",
            " [ 29  18  85 503  72 158  52  34  29  20]\n",
            " [ 29   8  52  60 667  25  51  81  24   3]\n",
            " [  9   5  69 184  62 557  19  70  18   7]\n",
            " [ 14   7  52  69  61  21 744  13  14   5]\n",
            " [ 13   5  38  47  77  65   6 728   2  19]\n",
            " [ 68  47   9  11   8   0   2   3 815  37]\n",
            " [ 20  70   5  14   8   4   5  13  30 831]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aDVLv1o2rGvu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}