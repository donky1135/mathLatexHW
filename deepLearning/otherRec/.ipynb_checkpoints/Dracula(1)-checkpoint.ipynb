{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF5AR1Aw04nS",
        "outputId": "4963a3f0-0231-439e-d93d-08777b830159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'\\xef\\xbb\\xbfThe Project Gutenberg eBook of Dracula\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\n",
            "of the Project Gutenberg License included with this ebook or online\n",
            "at www.gutenberg.org. If you are not located in the United States,\n",
            "you will have to check the laws of the country where you are located\n",
            "before using this eBook.\n",
            "\n",
            "Title: Dracula\n",
            "\n",
            "Author: Bram Stoker\n",
            "\n",
            "Release date: October 1, 1995 [eBook #345]\n",
            "                Most recently updated: November 12, 2023\n",
            "\n",
            "Language: English\n",
            "\n",
            "Credits: Chuck Greif and the Online Distributed Proofreading Team\n",
            "\n",
            "\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK DRACULA ***\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                DRACULA\n",
            "\n",
            "                                  _by_\n",
            "\n",
            "                              Bram Stoker\n",
            "\n",
            "                        [Illustration: colophon]\n",
            "\n",
            "                                NEW YORK\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch import nn\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import bs4\n",
        "import urllib.request\n",
        "\n",
        "webpage = str(urllib.request.urlopen(\"https://www.gutenberg.org/cache/epub/345/pg345.txt\").read())\n",
        "soup = bs4.BeautifulSoup(webpage)\n",
        "text = soup.get_text()\n",
        "\n",
        "# In the above lines, I download a webpage from the link (project gutenberg, free e-books)\n",
        "# Specifically the text to Bram Stoker's 'Dracula'\n",
        "# soup is the internal beautiful soup object/representation of the webpage\n",
        "# and I just grab the text from the webpage, representing the full text\n",
        "\n",
        "text = text.replace(\"\\\\r\\\\n\", '\\n')\n",
        "# Note that because of the processing that occurs, special characters like newlines are parsed as '\\' and 'n'\n",
        "# So this takes all pairs that /should/ represent newlines, and replaces them with actual newline characters\n",
        "\n",
        "print(text[0:1000]) # First 1000 characters of the text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = list(set(text)) # This creates an ordered list of every unique character that occurs /somewhere/ in the text\n",
        "tokens = [ \"[UNK]\" ] + tokens # Here I am appending a special 'unknown' token for when tokens are 'masked' or deliberately obscured from the text"
      ],
      "metadata": {
        "id": "8t7vGGPC8fYj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len( tokens ) # Total, 87 unique tokens ( 86 characters + one mask token )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE5jOHyC9XtK",
        "outputId": "d7888b78-4953-41ce-b474-0527961ed609"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_VZDWaC9YnQ",
        "outputId": "149b1aa4-998f-44f4-f1f7-596826a3ceb5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK]', 'r', 'm', '\\\\', '!', '3', '$', ']', 'l', 'B']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Purpose of this code is to generate a random subsequence from the text, of a given length\n",
        "## and grab the next character that follows\n",
        "\n",
        "# This will be used to generate training data pairs for the problem of \"predict the next token in this sequence\"\n",
        "\n",
        "def generate_sample_token_sequence(text, length):\n",
        "  start_point = random.randint(0, len( text ) - 1 - length - 1)\n",
        "  # We want to get a sequence of length 'length', and the next character\n",
        "  substring = text[start_point:start_point + length]\n",
        "  next_token = text[start_point + length]\n",
        "\n",
        "  return substring, next_token\n",
        "\n",
        "generate_sample_token_sequence(text, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL5q0Y51-apG",
        "outputId": "31be7bda-c240-4243-c431-fac43f385d00"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(' when we got back, save for some poor creature who\\nwas screaming away in one of the distant wards, a',\n",
              " 'n')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code generates a training batch\n",
        "# There will be batch_size instances of token sequences, of length 'length', and the corresponding next token for each\n",
        "# Note that these will be expressed in terms of 'indices', based on where the token exists in the dictionary\n",
        "# So that a string as a sequence of tokens is transformed into a sequence of integers\n",
        "\n",
        "# Index = 0 represents the [UNK] token, recall.\n",
        "\n",
        "def generate_batch(text, length, batch_size, token_dict):\n",
        "  x_index_lists = []\n",
        "  y_index_list = []\n",
        "\n",
        "  for i in range( batch_size ):\n",
        "    substring, next_token = generate_sample_token_sequence( text, length )\n",
        "    x_index_lists.append( [ token_dict.index( c ) for c in substring ]  )\n",
        "    y_index_list.append( token_dict.index( next_token ))\n",
        "\n",
        "  return torch.LongTensor(x_index_lists), torch.LongTensor(y_index_list)"
      ],
      "metadata": {
        "id": "jmnIXjqO_GT_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_batch(text, 10, 5, tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM487z9p_t3j",
        "outputId": "06a54c14-286c-439b-b6a9-dcb02eb80217"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[15, 56,  8, 43, 55, 15, 56, 60, 23, 80],\n",
              "         [55, 45, 67, 15, 55, 60, 23, 44, 80,  2],\n",
              "         [60, 55, 56, 77, 55, 20, 32, 26, 55, 77],\n",
              "         [18, 55, 67, 72, 43, 55, 15, 44, 55, 45],\n",
              "         [55, 43, 56, 43, 55, 72, 32, 60, 55, 70]]),\n",
              " tensor([44, 67, 44, 11, 32]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenRNNModel(nn.Module):\n",
        "  def __init__(self, token_dictionary, embedding_size):\n",
        "    super(TokenRNNModel, self).__init__()\n",
        "\n",
        "    self.tokens = token_dictionary\n",
        "    self.embedding_dim = embedding_size\n",
        "\n",
        "    self.embedding = nn.Embedding( len( self.tokens ), self.embedding_dim, max_norm=True)\n",
        "\n",
        "    # We create an embedding matrix, with a number of entries equal to the number of tokens\n",
        "    # And the embedding dimension (size of each token vector) as specified\n",
        "    # max_norm = True means that while the embedding vectors are learnable parameters, their norms\n",
        "    # will be kept from running away to infinity\n",
        "\n",
        "\n",
        "    # Here we create a stacked LSTM for processing a sequence of tokens\n",
        "    # The input_size is the number of features in each term of the sequence, in this case the embedding dimension\n",
        "    # The hidden size is the size of the internal memory (i.e. number of 'nodes' in the LSTM cell), which I am arbitrarily setting to 3 * the dimension of the embedding.\n",
        "    # We are stacking two LSTMs on top of each other - so the second LSTM will process the output sequence from the first\n",
        "    # batch_first just means that the data will be passed in a tensor based on (batch_size, sequence_length, feature_count)\n",
        "    # Dropout meaning that dropout will be applied between the iterations of the LSTM cells\n",
        "    # And bidirectional = True means that the sequence will be processed in two directions (read forward and read backwards)\n",
        "    # and the final output of the model will be the result of the forward pass concatenated with the result of the backwards pass\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size = self.embedding_dim,\n",
        "                              hidden_size = 3 * self.embedding_dim, num_layers=2, bias=True,\n",
        "                              batch_first = True, dropout=0.25, bidirectional=True)\n",
        "\n",
        "    # The final output of the LSTM will be be a vector of size 3*embedding_dim + 3*embedding_dim (one for the forward pass, one for the backward pass)\n",
        "    # We want to convert this vector into a vector of probabilities, one for each token, to represent what token comes 'next' in the sequence\n",
        "    # So we will have an output dimension equal to the number of tokens\n",
        "\n",
        "    self.logits = nn.Linear(in_features = 3*self.embedding_dim*2, out_features = len( self.tokens ) )\n",
        "\n",
        "  def forward(self, input_tensor, masking = False):\n",
        "    #############################################\n",
        "    ## If masking is set to true, we arbitrarily and at random 'mask'\n",
        "    ## about 15% of the tokens in the sequence, setting them to the [UNK] token\n",
        "    ## This forces the LSTM to learn relationships that are /not/ just between tokens\n",
        "    ## That are right next to each other - it has to learn longer distance correlations\n",
        "    ## In order to account for the 'lost' tokens. This is very similar to dropout.\n",
        "\n",
        "    embedded = input_tensor.clone()\n",
        "    batch_size, seq_length = embedded.shape\n",
        "\n",
        "    if masking:\n",
        "      for i in range( batch_size ):\n",
        "        for t in range( seq_length ):\n",
        "          if random.random() <= 0.15:\n",
        "            embedded[i, t] = 0\n",
        "    ############################################\n",
        "\n",
        "    embedded = self.embedding( embedded ) ## We take the 'masked' indices, and convert them to embedded vectors\n",
        "\n",
        "    ## NOTE: It may be useful here to print out the shape of embedded during a forward pass, to show exactly why it has the dimensions it does\n",
        "\n",
        "    lstm_output = self.lstm( embedded )\n",
        "\n",
        "    ## We pass it into the LSTM and collect the output. Note that the LSTM output is a complicated tuple of a) the computed sequences, and b) the final output vectors\n",
        "\n",
        "    lstm_output = lstm_output[0][:, -1, :]\n",
        "\n",
        "    # lstm_output[0] is the tensor of computed sequences from the lstm\n",
        "    # The first index of this tensor is the batch number, so we take all of them with the ':' slice\n",
        "    # The second index is the term of the sequence we want, and we want the last one, so we take -1\n",
        "    # The last index is the features, and we want all features of the last term of the sequence\n",
        "\n",
        "    # Again, printing out the shape of lstm_output[0] may be useful,\n",
        "    # as well as looking at the LSTM api page\n",
        "\n",
        "    output = self.logits( lstm_output ) # Lastly, for each sequence in the batch, we compute a vector of token logits (which we can use for softmaxing to get the probabilities for each token)\n",
        "    return output"
      ],
      "metadata": {
        "id": "-vRnVWTo9ZQJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch, y_batch = generate_batch(text, 10, 5, tokens)"
      ],
      "metadata": {
        "id": "akLkkTZPDB18"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = TokenRNNModel(tokens, 20)"
      ],
      "metadata": {
        "id": "q1gSO5wTDFlm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The purpose of this code is to take a sequence and predict the next token\n",
        "\n",
        "def predict_next_character(model, substring, tokens):\n",
        "  indices = torch.LongTensor( [ [ tokens.index(c) for c in substring ] ] ) # For each character in the sequence, identify its token index\n",
        "  next_token_logits = model( indices ).detach()[0] # Pass in the indices to the model, detach the results from the computation graph, and take the vector of logits that results\n",
        "  next_token_probabilities = torch.nn.Softmax( dim = 0 )( next_token_logits ) # Convert the logits to probabilities\n",
        "  next_token_probabilities = np.asarray( next_token_probabilities ) # Saving the probabilities as a numpy array.\n",
        "\n",
        "  next_character = random.choices( tokens, weights = next_token_probabilities, k = 1 ) # From the tokens, we choose a token, in accordance with the probabilities computed by the model\n",
        "  return next_character[0] # We return the selected token (there is only one element in this list since k = 1)\n",
        "\n",
        "# The following code starts with a prompt, and iteratively predicts the next token,\n",
        "# Appends it to the prompt, then uses that to predict the next token after,\n",
        "# until it has been extended to the desired length.\n",
        "\n",
        "def extend_prompt(model, prompt, tokens, character_count):\n",
        "  model.eval()\n",
        "  output = prompt\n",
        "  for i in range( character_count ):\n",
        "    next_character = predict_next_character(model, output, tokens)\n",
        "    output += next_character\n",
        "  return output"
      ],
      "metadata": {
        "id": "sMU-56cYDmEa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extend_prompt(lstm_model, \"Dracula, where is my money? \", tokens, 40)\n",
        "\n",
        "# Note that for the untrained model, this produces essentially a garbage sequence of random tokens - no surprises there."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Hp9-CSbRImTq",
        "outputId": "8c3dcee1-5da6-4351-eceb-a8d4ac2b1292"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Dracula, where is my money? {][\\nw'.Uc.W4pFy-8X]2Q9HU\\nkHNxbj#!tnbwa5'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = TokenRNNModel(tokens, 20)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam( lstm_model.parameters(), lr = 0.01 )"
      ],
      "metadata": {
        "id": "GxRaZM1SOJ8y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extend_prompt(lstm_model, \"Mina, \", tokens, 40)\n",
        "\n",
        "# Note, Mina Harker is one of the main characters of the story."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7HQcRZl7XwE5",
        "outputId": "56bfc12b-efca-4aad-ae0c-87ec08ee2743"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mina, RfQi2Cih,xIni[UNK]#%ydwg[v\\\\}[UNK],NTE16MBHBxx1Gw'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "for epoch in range(100):\n",
        "  print(\"Training Epoch\", epoch)\n",
        "  print()\n",
        "\n",
        "  lstm_model.train()\n",
        "\n",
        "  for batch in range(100):\n",
        "    seq_length = random.randint(1,50) # Pick a random sequence length to generate data with (I chose 50 to keep things small)\n",
        "    x_batch, y_batch =  generate_batch(text, seq_length, batch_size, tokens)\n",
        "    optimizer.zero_grad()\n",
        "    logits = lstm_model( x_batch, masking = True ) # Note that I am setting masking to true, to try to force the LSTM to learn longer dependencies\n",
        "    loss = loss_function( logits, y_batch )\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    #print( loss.item() ) ## If you uncomment this, you'll generally see the loss decreasing, but it clutters up the output\n",
        "\n",
        "  print(\"########################################################\")\n",
        "  print( extend_prompt(lstm_model, \"Mina, \", tokens, 40) ) # Observe how good the model is by testing its generative capacity\n",
        "  print(\"########################################################\")\n",
        "\n",
        "  ### NOTE\n",
        "  ### The important thing to observe here as this trains is that at first, the output is completely random\n",
        "  ### The first thing that you should observe though is that it starts to break up sequences of tokens with spaces\n",
        "  ### In a way that starts to mimic the sizes of words.\n",
        "  ### As it continues, it may start to get punctuation in \"the right place\"\n",
        "  ### And may even start to get sequences of tokens that spell actual words\n",
        "  ### It is slowly but surely learning what text 'looks like', and generating text accordingly.\n",
        "\n",
        "  ### One thing to consider here: What if we had used words as the tokens instead of characters?\n",
        "  ### Then the problem would be to learn relationships between words rather than relationships between characters\n",
        "  ### Which turns very quickly to relationships in 'meaning'. Whatever that means.\n",
        "  ### And the text generated could very quickly start to turn 'realistic'."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58VSG0kqN8MB",
        "outputId": "dfbcf155-5e96-427a-e0fd-9b66159d3193"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch 0\n",
            "\n",
            "########################################################\n",
            "Mina, jaceiknthe t arosthelo-e\\o melusous te t\n",
            "########################################################\n",
            "Training Epoch 1\n",
            "\n",
            "########################################################\n",
            "Mina, wy ag-yeate, bus ig terr nall, Vher\\xe2\\\n",
            "########################################################\n",
            "Training Epoch 2\n",
            "\n",
            "########################################################\n",
            "Mina, tot ackanover. illlcet et terere wat to \n",
            "########################################################\n",
            "Training Epoch 3\n",
            "\n",
            "########################################################\n",
            "Mina, He dew beve ofed to toll to shat hi he n\n",
            "########################################################\n",
            "Training Epoch 4\n",
            "\n",
            "########################################################\n",
            "Mina, helveone is ane seat yore the to uede to\n",
            "########################################################\n",
            "Training Epoch 5\n",
            "\n",
            "########################################################\n",
            "Mina, Thath cnourd. She dead thain lew the tar\n",
            "########################################################\n",
            "Training Epoch 6\n",
            "\n",
            "########################################################\n",
            "Mina, and then gill as mean, he a soaly fo bis\n",
            "########################################################\n",
            "Training Epoch 7\n",
            "\n",
            "########################################################\n",
            "Mina, srelow. He cay wably qintesct meny gove \n",
            "########################################################\n",
            "Training Epoch 8\n",
            "\n",
            "########################################################\n",
            "Mina, troo, an themow cain is yom a kome evos \n",
            "########################################################\n",
            "Training Epoch 9\n",
            "\n",
            "########################################################\n",
            "Mina, yould her to hisiched, would as\n",
            "qucaled.\n",
            "########################################################\n",
            "Training Epoch 10\n",
            "\n",
            "########################################################\n",
            "Mina, Moment wus. shar at ghell. To goye\n",
            "Lus I\n",
            "########################################################\n",
            "Training Epoch 11\n",
            "\n",
            "########################################################\n",
            "Mina, tell att e migh moss not when wo he mart\n",
            "########################################################\n",
            "Training Epoch 12\n",
            "\n",
            "########################################################\n",
            "Mina, the\n",
            "ledy at of, and taid the pans and sh\n",
            "########################################################\n",
            "Training Epoch 13\n",
            "\n",
            "########################################################\n",
            "Mina, And\n",
            "the shinly head:--\n",
            "\n",
            "-\\xe2\\x80\\x99xen\n",
            "########################################################\n",
            "Training Epoch 14\n",
            "\n",
            "########################################################\n",
            "Mina, in would facech pot to Ze out wheret sud\n",
            "########################################################\n",
            "Training Epoch 15\n",
            "\n",
            "########################################################\n",
            "Mina, feirne an whis, the eping wyer yroncele;\n",
            "########################################################\n",
            "Training Epoch 16\n",
            "\n",
            "########################################################\n",
            "Mina, for to\n",
            "decwiovy \\xe2\\x80\\x9cben, not the\n",
            "########################################################\n",
            "Training Epoch 17\n",
            "\n",
            "########################################################\n",
            "Mina, be said whe he stoct, he\n",
            "oar sond corpic\n",
            "########################################################\n",
            "Training Epoch 18\n",
            "\n",
            "########################################################\n",
            "Mina, moring is\n",
            "saitling want fay\n",
            "and upun cou\n",
            "########################################################\n",
            "Training Epoch 19\n",
            "\n",
            "########################################################\n",
            "Mina, and that I mach!\\xe2\\x80\\x99s rook-his k\n",
            "########################################################\n",
            "Training Epoch 20\n",
            "\n",
            "########################################################\n",
            "Mina, all noh ramering, bim pood:--\\xe2\\x80\\x9\n",
            "########################################################\n",
            "Training Epoch 21\n",
            "\n",
            "########################################################\n",
            "Mina, and eepin and the coke not of srig; fay \n",
            "########################################################\n",
            "Training Epoch 22\n",
            "\n",
            "########################################################\n",
            "Mina, me lay in thereiedaru have hom, wos me_ \n",
            "########################################################\n",
            "Training Epoch 23\n",
            "\n",
            "########################################################\n",
            "Mina, to from, as the fure\\xe2\\x80\\x9conly fam\n",
            "########################################################\n",
            "Training Epoch 24\n",
            "\n",
            "########################################################\n",
            "Mina, I on fee bloul hos exant. I word onib. F\n",
            "########################################################\n",
            "Training Epoch 25\n",
            "\n",
            "########################################################\n",
            "Mina, and hen and hos of her Just asse mectle \n",
            "########################################################\n",
            "Training Epoch 26\n",
            "\n",
            "########################################################\n",
            "Mina, and has streent and space-me, but daveiw\n",
            "########################################################\n",
            "Training Epoch 27\n",
            "\n",
            "########################################################\n",
            "Mina, and waits\n",
            "sountle was nereted and I saz \n",
            "########################################################\n",
            "Training Epoch 28\n",
            "\n",
            "########################################################\n",
            "Mina, an the \\xe2\\x80\\x99t, mad the\n",
            "can that i\n",
            "########################################################\n",
            "Training Epoch 29\n",
            "\n",
            "########################################################\n",
            "Mina, and ould-taketed off is\n",
            "A pecanse ap who\n",
            "########################################################\n",
            "Training Epoch 30\n",
            "\n",
            "########################################################\n",
            "Mina, his gon before uf and wall Pr. as with M\n",
            "########################################################\n",
            "Training Epoch 31\n",
            "\n",
            "########################################################\n",
            "Mina, as sarmed begared me, of as a nect mead \n",
            "########################################################\n",
            "Training Epoch 32\n",
            "\n",
            "########################################################\n",
            "Mina, and somber.\n",
            "\n",
            "        thure I armed to co\n",
            "########################################################\n",
            "Training Epoch 33\n",
            "\n",
            "########################################################\n",
            "Mina, and sling to not and dreakere to such un\n",
            "########################################################\n",
            "Training Epoch 34\n",
            "\n",
            "########################################################\n",
            "Mina, his woed\n",
            "and resthur flarl crochil, not \n",
            "########################################################\n",
            "Training Epoch 35\n",
            "\n",
            "########################################################\n",
            "Mina, but and beath the stand\n",
            "the to ather\n",
            "eve\n",
            "########################################################\n",
            "Training Epoch 36\n",
            "\n",
            "########################################################\n",
            "Mina, for\n",
            "I comy to that, friens and of Ithere\n",
            "########################################################\n",
            "Training Epoch 37\n",
            "\n",
            "########################################################\n",
            "Mina, whiser in time the room, well in a mayte\n",
            "########################################################\n",
            "Training Epoch 38\n",
            "\n",
            "########################################################\n",
            "Mina, thor it nof more and\n",
            "now by bow of Gocas\n",
            "########################################################\n",
            "Training Epoch 39\n",
            "\n",
            "########################################################\n",
            "Mina, it flould erstroly, to which in his cro \n",
            "########################################################\n",
            "Training Epoch 40\n",
            "\n",
            "########################################################\n",
            "Mina, just us to ceal, and ficked and contiof-\n",
            "########################################################\n",
            "Training Epoch 41\n",
            "\n",
            "########################################################\n",
            "Mina, his eversfulak fain, with sleasmed vanti\n",
            "########################################################\n",
            "Training Epoch 42\n",
            "\n",
            "########################################################\n",
            "Mina, betrict intlomer with theirs meced, thom\n",
            "########################################################\n",
            "Training Epoch 43\n",
            "\n",
            "########################################################\n",
            "Mina, hit with gurquct the back. But me teccen\n",
            "########################################################\n",
            "Training Epoch 44\n",
            "\n",
            "########################################################\n",
            "Mina, dew. I how, a faivincera somp askere; of\n",
            "########################################################\n",
            "Training Epoch 45\n",
            "\n",
            "########################################################\n",
            "Mina, and him? The face with screaterfuan inni\n",
            "########################################################\n",
            "Training Epoch 46\n",
            "\n",
            "########################################################\n",
            "Mina, chades an nowave Lorked, of gid should w\n",
            "########################################################\n",
            "Training Epoch 47\n",
            "\n",
            "########################################################\n",
            "Mina, sofuse to us to tell themwie cann it and\n",
            "########################################################\n",
            "Training Epoch 48\n",
            "\n",
            "########################################################\n",
            "Mina, for the rearid was on oven the dound.\n",
            "\n",
            "\\\n",
            "########################################################\n",
            "Training Epoch 49\n",
            "\n",
            "########################################################\n",
            "Mina, but mave to rest.\n",
            "Feetivive\n",
            "sinn must he\n",
            "########################################################\n",
            "Training Epoch 50\n",
            "\n",
            "########################################################\n",
            "Mina, ohat, \\xe2\\x80\\x9d Fow, was dole about p\n",
            "########################################################\n",
            "Training Epoch 51\n",
            "\n",
            "########################################################\n",
            "Mina, are I mussate seemed my I haddor of prec\n",
            "########################################################\n",
            "Training Epoch 52\n",
            "\n",
            "########################################################\n",
            "Mina, un a have to be\n",
            "orionlyact away the\n",
            "; wi\n",
            "########################################################\n",
            "Training Epoch 53\n",
            "\n",
            "########################################################\n",
            "Mina, herder the to head the not of diBthe ear\n",
            "########################################################\n",
            "Training Epoch 54\n",
            "\n",
            "########################################################\n",
            "Mina, who belfide of the Maokiss:--\n",
            "\n",
            "\\xe2\\x80\\\n",
            "########################################################\n",
            "Training Epoch 55\n",
            "\n",
            "########################################################\n",
            "Mina, us have go to\n",
            "the ramik the\n",
            "layy the pas\n",
            "########################################################\n",
            "Training Epoch 56\n",
            "\n",
            "########################################################\n",
            "Mina, he say for I was have all, for making an\n",
            "########################################################\n",
            "Training Epoch 57\n",
            "\n",
            "########################################################\n",
            "Mina, in fat be with and sies. She it ormoany \n",
            "########################################################\n",
            "Training Epoch 58\n",
            "\n",
            "########################################################\n",
            "Mina, bess of her caster of his fixing up whic\n",
            "########################################################\n",
            "Training Epoch 59\n",
            "\n",
            "########################################################\n",
            "Mina, howe\n",
            "that yir hormo!\\xe2\\x80\\x99s ble la\n",
            "########################################################\n",
            "Training Epoch 60\n",
            "\n",
            "########################################################\n",
            "Mina, friend in my have in I calriend to other\n",
            "########################################################\n",
            "Training Epoch 61\n",
            "\n",
            "########################################################\n",
            "Mina, show:-ear, fos I ammound and though the \n",
            "########################################################\n",
            "Training Epoch 62\n",
            "\n",
            "########################################################\n",
            "Mina, blain you horried very all dorpher must \n",
            "########################################################\n",
            "Training Epoch 63\n",
            "\n",
            "########################################################\n",
            "Mina, whe scascsedibe eye with the Hantly spen\n",
            "########################################################\n",
            "Training Epoch 64\n",
            "\n",
            "########################################################\n",
            "Mina, sharres\n",
            "an puricen to wanded feath to de\n",
            "########################################################\n",
            "Training Epoch 65\n",
            "\n",
            "########################################################\n",
            "Mina, he gave, calling. Fe drively all\n",
            "won\\xe2\n",
            "########################################################\n",
            "Training Epoch 66\n",
            "\n",
            "########################################################\n",
            "Mina, the till comencage the ember! Way irs or\n",
            "########################################################\n",
            "Training Epoch 67\n",
            "\n",
            "########################################################\n",
            "Mina, but pucureled freit of can our pont sopp\n",
            "########################################################\n",
            "Training Epoch 68\n",
            "\n",
            "########################################################\n",
            "Mina, and sire e eres to Seenant you pain, he\n",
            "\n",
            "########################################################\n",
            "Training Epoch 69\n",
            "\n",
            "########################################################\n",
            "Mina, that a gould dear you full I have it eve\n",
            "########################################################\n",
            "Training Epoch 70\n",
            "\n",
            "########################################################\n",
            "Mina, but his night of the did eithed notevil.\n",
            "########################################################\n",
            "Training Epoch 71\n",
            "\n",
            "########################################################\n",
            "Mina, before to she are I had bentiricosness t\n",
            "########################################################\n",
            "Training Epoch 72\n",
            "\n",
            "########################################################\n",
            "Mina, shpelvere. \\xe2\\x80\\x9cU have be interpe\n",
            "########################################################\n",
            "Training Epoch 73\n",
            "\n",
            "########################################################\n",
            "Mina, I have oll to you commose take, was graw\n",
            "########################################################\n",
            "Training Epoch 74\n",
            "\n",
            "########################################################\n",
            "Mina, but for turure from speet in cinment, in\n",
            "########################################################\n",
            "Training Epoch 75\n",
            "\n",
            "########################################################\n",
            "Mina, my hest came make, and was make, for tho\n",
            "########################################################\n",
            "Training Epoch 76\n",
            "\n",
            "########################################################\n",
            "Mina, but\n",
            "them. I feftunnash, or we mut of her\n",
            "########################################################\n",
            "Training Epoch 77\n",
            "\n",
            "########################################################\n",
            "Mina, and render. She so melns own was soph\n",
            "sl\n",
            "########################################################\n",
            "Training Epoch 78\n",
            "\n",
            "########################################################\n",
            "Mina, moptible imporing ress out could ran in \n",
            "########################################################\n",
            "Training Epoch 79\n",
            "\n",
            "########################################################\n",
            "Mina, and the rike. But out on done do rouble.\n",
            "########################################################\n",
            "Training Epoch 80\n",
            "\n",
            "########################################################\n",
            "Mina, his shall s caze.   _ap so a dis?\\xe2\\x8\n",
            "########################################################\n",
            "Training Epoch 81\n",
            "\n",
            "########################################################\n",
            "Mina, his\n",
            "frame. I could shinder._--\n",
            "\n",
            "\\xe2\\x80\n",
            "########################################################\n",
            "Training Epoch 82\n",
            "\n",
            "########################################################\n",
            "Mina, to gischury forcold if o\\xiedive\n",
            "excepti\n",
            "########################################################\n",
            "Training Epoch 83\n",
            "\n",
            "########################################################\n",
            "Mina, prodighful--up gorskuthar.\n",
            "As Dyforible \n",
            "########################################################\n",
            "Training Epoch 84\n",
            "\n",
            "########################################################\n",
            "Mina, no eral of\n",
            "he for our man a tope,\n",
            "and hi\n",
            "########################################################\n",
            "Training Epoch 85\n",
            "\n",
            "########################################################\n",
            "Mina, her this through it and not and head wil\n",
            "########################################################\n",
            "Training Epoch 86\n",
            "\n",
            "########################################################\n",
            "Mina, her\n",
            "levite,\n",
            "taber to leard\n",
            "the sain the \n",
            "########################################################\n",
            "Training Epoch 87\n",
            "\n",
            "########################################################\n",
            "Mina, that the dither; mivesterine her beton t\n",
            "########################################################\n",
            "Training Epoch 88\n",
            "\n",
            "########################################################\n",
            "Mina, not we Mirt of\n",
            "the asked the straid they\n",
            "########################################################\n",
            "Training Epoch 89\n",
            "\n",
            "########################################################\n",
            "Mina, and when he same all go is\n",
            "a gow tadd\n",
            "ho\n",
            "########################################################\n",
            "Training Epoch 90\n",
            "\n",
            "########################################################\n",
            "Mina, and by refide dikeres fryik to me.\n",
            "(Ount\n",
            "########################################################\n",
            "Training Epoch 91\n",
            "\n",
            "########################################################\n",
            "Mina, one of\n",
            "him. The fremming-by in be\n",
            "as\\xe2\n",
            "########################################################\n",
            "Training Epoch 92\n",
            "\n",
            "########################################################\n",
            "Mina, we havers._--\n",
            "\n",
            "an with propents his trin\n",
            "########################################################\n",
            "Training Epoch 93\n",
            "\n",
            "########################################################\n",
            "Mina, as againny beay. The onworg up, for agai\n",
            "########################################################\n",
            "Training Epoch 94\n",
            "\n",
            "########################################################\n",
            "Mina, nevided my dread tere. When the commin s\n",
            "########################################################\n",
            "Training Epoch 95\n",
            "\n",
            "########################################################\n",
            "Mina, would grewn\n",
            "there has then I must.\n",
            "\n",
            "\\xe2\n",
            "########################################################\n",
            "Training Epoch 96\n",
            "\n",
            "########################################################\n",
            "Mina, I said:--\n",
            "\n",
            "\\xe2\\x80\\x9cWith I could mant\n",
            "########################################################\n",
            "Training Epoch 97\n",
            "\n",
            "########################################################\n",
            "Mina, befo rertery seem, and a solcess-thing w\n",
            "########################################################\n",
            "Training Epoch 98\n",
            "\n",
            "########################################################\n",
            "Mina, Artes through agined on the casemanty--i\n",
            "########################################################\n",
            "Training Epoch 99\n",
            "\n",
            "########################################################\n",
            "Mina, and as mady give-\\xe2\\x80\\x9cSimime. He \n",
            "########################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DmYTk_xZ7MKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}