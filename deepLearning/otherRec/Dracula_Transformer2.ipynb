{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "csZtRNPikayG",
    "outputId": "c953c107-51af-47e0-9c4f-f1ab8b1a8967"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xef\\xbb\\xbfThe Project Gutenberg eBook of Dracula\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: Dracula\n",
      "\n",
      "Author: Bram Stoker\n",
      "\n",
      "Release date: October 1, 1995 [eBook #345]\n",
      "                Most recently updated: November 12, 2023\n",
      "\n",
      "Language: English\n",
      "\n",
      "Credits: Chuck Greif and the Online Distributed Proofreading Team\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK DRACULA ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                DRACULA\n",
      "\n",
      "                                  _by_\n",
      "\n",
      "                              Bram Stoker\n",
      "\n",
      "                        [Illustration: colophon]\n",
      "\n",
      "                                NEW YORK\n",
      "\n",
      "\n",
      "Tokens: ['[UNK]', '9', 'f', 'u', 'P', 'Y', 'Z', 'c', 'i', 'j']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import bs4\n",
    "import urllib.request\n",
    "\n",
    "webpage = str(urllib.request.urlopen(\"https://www.gutenberg.org/cache/epub/345/pg345.txt\").read())\n",
    "soup = bs4.BeautifulSoup(webpage)\n",
    "\n",
    "text = soup.get_text()\n",
    "text = text.replace(\"\\\\r\\\\n\", '\\n')\n",
    "print(text[0:1000])\n",
    "\n",
    "tokens = [ \"[UNK]\" ] + list(set(text))\n",
    "print(\"Tokens:\", tokens[0:10])\n",
    "\n",
    "\n",
    "## All the above is as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FkH9iC2slBRp",
    "outputId": "0b4f0ea8-ea72-4fee-e6c2-5551c2c47e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l. We only know that the box is somewhere on the water,\\nmoving along. The customs and the octroi, if', ' ')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[23,  3, 67, 76, 10, 23, 81,  8, 31, 47],\n",
       "         [23,  8, 86, 23,  8, 34, 23, 46, 23, 81],\n",
       "         [81,  8, 31, 33, 23, 32, 81, 25, 10, 23],\n",
       "         [46, 86,  8, 76, 10, 23, 76,  2, 23, 52],\n",
       "         [13, 46,  8, 10, 23, 76, 10, 23, 10, 25]]),\n",
       " tensor([23,  3, 81, 76, 13]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_random_substring(text, length):\n",
    "  start_point = random.randint(0, len( text ) - 1 - length - 1)\n",
    "  # We want to get a sequence of length 'length', and the next character\n",
    "  substring = text[start_point:start_point + length]\n",
    "  next_token = text[start_point + length]\n",
    "\n",
    "  return substring, next_token\n",
    "\n",
    "print( get_random_substring(text, 100) )\n",
    "\n",
    "def generate_batch(text, length, batch_size, token_dict):\n",
    "  x_index_lists = []\n",
    "  y_index_list = []\n",
    "\n",
    "  for i in range( batch_size ):\n",
    "    substring, next_token = get_random_substring( text, length )\n",
    "    x_index_lists.append( [ token_dict.index( c ) for c in substring ]  )\n",
    "    y_index_list.append( token_dict.index( next_token ))\n",
    "\n",
    "  return torch.LongTensor(x_index_lists), torch.LongTensor(y_index_list)\n",
    "\n",
    "generate_batch(text, 10, 5, tokens)\n",
    "\n",
    "\n",
    "### All the above, as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "IYosZO8qlpO1"
   },
   "outputs": [],
   "source": [
    "class TokenTransformerModel(nn.Module):\n",
    "  def __init__(self, token_dictionary, embedding_size, n_heads):\n",
    "    super(TokenTransformerModel, self).__init__()\n",
    "\n",
    "\n",
    "    ## Note that, among other things, one thing we need to specify for a transformer model is how many 'attention heads' we want in each attention layer.\n",
    "\n",
    "    self.tokens = token_dictionary\n",
    "    self.n_tokens = len( self.tokens )\n",
    "    self.dim = embedding_size\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    ## As before, we need an embedding matrix, to transform the token indices into representative vectors of a specified dimension\n",
    "    self.embedding = nn.Embedding( self.n_tokens, self.dim, max_norm=True)\n",
    "    self.embedding_dropout = nn.Dropout( 0.1 )\n",
    "\n",
    "    #######################################\n",
    "    ## The below specifies the parameters we need for the first attention layer\n",
    "    ## For each attention head, (n_heads in total), we need a query matrix, for converting the terms of the sequence to query vectors\n",
    "    ## This I am (arbitrarily) setting to be mapped into a space half as large as the embedding (dim // 2), though it could be larger, could be smaller.\n",
    "    ## The dimension of W_Q1 is thus (n_heads) x (dim / 2) x (dim), or rather n_heads many (dim/2)x(dim) matrices\n",
    "\n",
    "    ## The key matrices are set up similarly\n",
    "\n",
    "    ## For this first attention layer, the 'value' vectors will just be the original sequence vectors\n",
    "\n",
    "    self.W_Q1 = nn.Parameter( torch.randn( self.n_heads, self.dim // 2, self.dim ), requires_grad = True ) * 0.1\n",
    "    self.W_K1 = nn.Parameter( torch.randn( self.n_heads, self.dim // 2, self.dim ), requires_grad = True ) * 0.1\n",
    "\n",
    "    ## Once we compute the attention heads, we want to pass the results (and the original sequence items) into a small feed forward network,\n",
    "    ## In this case, just two layers deep. This is as set up in 'Attention Is All You Need'. Note I am again arbitrarily setting the internal dimension\n",
    "    ## of the network, in this case to 200.\n",
    "\n",
    "    ## The input will be ( each attention head + the t-th sequence item), the output will be a computed 'summary' for that sequence item, that factors in the 'global' info\n",
    "    ## from the attention heads.\n",
    "    self.attn_layer_1_layer_1 = nn.Linear( in_features = ( self.n_heads + 1 ) * self.dim, out_features = 200 )\n",
    "    self.attn_layer_1_layer_2 = nn.Linear( in_features = 200, out_features = self.dim )\n",
    "\n",
    "    ## Again following 'attention is all you need', we will batchnorm the result along the sequence\n",
    "    self.batch_norm_1 = nn.BatchNorm1d( num_features = 20 )\n",
    "    #######################################\n",
    "\n",
    "    #######################################\n",
    "    ## Below, similar parameters for the second attention layer\n",
    "\n",
    "    self.W_Q2 = nn.Parameter( torch.randn( self.n_heads, self.dim // 2, self.dim ), requires_grad = True ) * 0.1\n",
    "    self.W_K2 = nn.Parameter( torch.randn( self.n_heads, self.dim // 2, self.dim ), requires_grad = True ) * 0.1\n",
    "\n",
    "    self.attn_layer_2_layer_1 = nn.Linear( in_features = ( self.n_heads + 1 ) * self.dim, out_features = 200 )\n",
    "    self.attn_layer_2_layer_2 = nn.Linear( in_features = 200, out_features = self.dim )\n",
    "\n",
    "    self.batch_norm_2 = nn.BatchNorm1d( num_features = 20 )\n",
    "    #######################################\n",
    "\n",
    "    #######################################\n",
    "    ## Below, parameters for one final attention head\n",
    "    ## But on this third layer, I am only computing a single 'attention head', representing a final 'summary' of the whole sequence\n",
    "    ## So I only need one of each kind of matrix\n",
    "\n",
    "    ## Making an arbitrary decision here that the value vectors for this layer will be twice as large as the original sequence vectors\n",
    "\n",
    "    self.W_Q3 = nn.Parameter( torch.randn( self.dim // 2, self.dim ), requires_grad = True ) * 0.1\n",
    "    self.W_K3 = nn.Parameter( torch.randn( self.dim // 2, self.dim ), requires_grad = True ) * 0.1\n",
    "    self.W_V3 = nn.Parameter( torch.randn( 2 * self.dim, self.dim ) , requires_grad = True ) * 0.1\n",
    "\n",
    "    ## The final vector result of this layer will be a vector of size 2*dim, representing a summary of the whole input sequence.\n",
    "    #######################################\n",
    "\n",
    "\n",
    "    #######################################\n",
    "    ## Again, we convert that vector summary to a vector of logits, so that we can predict the next token in the sequence.\n",
    "    self.logit_layer = nn.Linear( in_features = 2 * self.dim, out_features = self.n_tokens )\n",
    "    #######################################\n",
    "\n",
    "\n",
    "  def forward(self, input_tensor, masking = False):\n",
    "\n",
    "    ######################################################################################\n",
    "    ## As before, mask if requested, removing about 15% of the original tokens (forcing the network)\n",
    "    ## To learn longer term dependencies\n",
    "    embedded = input_tensor.clone()\n",
    "    batch_size, seq_length = embedded.shape\n",
    "\n",
    "    if masking:\n",
    "      for i in range( batch_size ):\n",
    "        for t in range( seq_length ):\n",
    "          if random.random() <= 0.15:\n",
    "            embedded[i, t] = 0\n",
    "    ######################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    embedded = self.embedding( embedded ) ## Convert the tensor of indices into embedded vectors\n",
    "    batch_size, seq_length, feature_count = embedded.shape\n",
    "\n",
    "    ############################################################################################\n",
    "    ## Here I am calculating the positional embedding for every postion in the sequence\n",
    "    ## And once each embedding vector (of size feature count) is calculated, I add the results\n",
    "    ## To the token embeddings, and apply dropout.\n",
    "    pos_embedding = torch.zeros( seq_length, feature_count )\n",
    "    div_term = torch.exp(torch.arange(0, feature_count, 2) * (-np.log(1000.0) / feature_count))\n",
    "\n",
    "    for i in range( seq_length ):\n",
    "      pos_embedding[i, 0:feature_count:2] = torch.sin(i * div_term)\n",
    "      pos_embedding[i, 1:feature_count:2] = torch.cos(i * div_term)\n",
    "\n",
    "    embedded = embedded + pos_embedding\n",
    "    embedded = self.embedding_dropout( embedded )\n",
    "\n",
    "    ## At this point, 'embedded' stores a vector representation of combined information about\n",
    "    ## /what/ token is present, and /where/ that token is in the sequence.\n",
    "    ############################################################################################\n",
    "\n",
    "    ############################################################################################\n",
    "\n",
    "    ## ATTENTION LAYER 1\n",
    "    ## First thing we need to do is compute the 'attention head' for each head.\n",
    "\n",
    "    attention_heads_1 = []\n",
    "    for h in range( self.n_heads ):\n",
    "      test_query_vector_sequence = torch.matmul( embedded, self.W_Q1[h].t() ) # This will multiply every sequence of vectors by W_Q1 for that head, giving a set of (batch_size, seq_length, dim/2) query vectors\n",
    "      test_key_vector_sequence = torch.matmul( embedded, self.W_K1[h].t() ) # Similarly, (batch_size, seq_length, dim/2) key vectors\n",
    "\n",
    "      test_attention_weights = test_query_vector_sequence * test_key_vector_sequence # This uses broadcasting to multiply each term of each query vector by the corresponding term of the corresponding key vector\n",
    "      test_attention_weights = torch.sum( test_attention_weights, dim = 2 ) # This sums each (query * key) vector, essentially computing the dot product for each pair. (batch_size, seq_length) final dot products\n",
    "      test_attention_weights = test_attention_weights  / np.sqrt( self.dim // 2 ) # Scale the dot products by 1/sqrt(dim) - this gives 'scaled dot product attention' weights\n",
    "      test_attention_weights = torch.nn.Softmax(dim = 1)( test_attention_weights ) # Softmax to get the final attention weights, (batch_size, seq_length) - for each seq in the batch, a set of weights for each term of the sequence\n",
    "      test_attention_weights = test_attention_weights.unsqueeze(2) # This final line here is going to expand it to a tensor of size (batch_size, seq_length, 1), with this superfluous component on the end to make dimensions line up\n",
    "\n",
    "      test_attention_head = torch.sum( embedded * test_attention_weights, dim = 1 ) # This multiplies each embedded vector by its corresponding weight, and then adds across each scaled term of the sequence\n",
    "\n",
    "      # Final result here is going to be of size ( batch_size, feature_count ) - for each sequence in the batch, we have an attention vector of size 'feature_count' that summarizes that sequence\n",
    "\n",
    "      attention_heads_1.append( test_attention_head.unsqueeze(1) )\n",
    "      # Add the computed attention head to 'attention_heads_1', modifying it to dimension (batch_size, 1, feature_count)\n",
    "\n",
    "    attention_heads_1 = torch.concat( attention_heads_1, dim = 1 )\n",
    "    # This glues all the attention heads into one tensor of shape (batch_size, n_heads, feature_count)\n",
    "    # We have computed all the attention heads, for each sequence in the batch\n",
    "\n",
    "\n",
    "    # We then process each sequence, based on teh computed attention\n",
    "    processed_sequence_term = []\n",
    "    for i in range( seq_length ):\n",
    "      seq_term = embedded[:, i].unsqueeze(1) # For each sequence in the batch, we get sequence term i, shaping it to be a tensor of size (batch_size, 1, feature_count)\n",
    "      seq_term = torch.concat( [ seq_term, attention_heads_1 ], dim = 1 ) # We glue it to the attention heads, creating a tensor of size (batch_size, n_heads + 1, feature_count)\n",
    "      seq_term = torch.nn.Flatten()( seq_term ) # And flatten it to a tensor of (batch_size, (n_heads+1)*feature_count) - note that this vector (n_heads+1)*feature_count contains information abt sequence element i, /and the entire sequence/ based on the attention vectors\n",
    "\n",
    "      seq_term = self.attn_layer_1_layer_1( seq_term ) # Pass it into a dense layer\n",
    "      seq_term = nn.ReLU()( seq_term ) # Activation function\n",
    "      seq_term = self.attn_layer_1_layer_2( seq_term ) # Pass it into a dense layer\n",
    "\n",
    "      seq_term = self.batch_norm_1( embedded[:, i] + seq_term ) # We add the computed 'residual' back to the original sequence element, and then batch normalize\n",
    "\n",
    "      # Final shape: (batch_size, feature_count)\n",
    "\n",
    "      processed_sequence_term.append( seq_term.unsqueeze(1) ) # Collect it, reshaped as (batch_size, 1, feature_count), so that we can glue each term of the sequence back together\n",
    "\n",
    "    embedded = torch.concat( processed_sequence_term, dim = 1 )\n",
    "\n",
    "    # Embedded now has shape (batch_size, seq_length, feature_count), each term of each sequence in the batch representing 'processed' information about how that sequence item relates to everything else in the sequence\n",
    "    ################################################################################################\n",
    "\n",
    "    ############################################################################################\n",
    "    ## ATTENTION HEAD 2\n",
    "    ## Much the same as attention head 1!\n",
    "\n",
    "    attention_heads_2 = []\n",
    "    for h in range( self.n_heads ):\n",
    "      test_query_vector_sequence = torch.matmul( embedded, self.W_Q2[h].t() )\n",
    "      test_key_vector_sequence = torch.matmul( embedded, self.W_K2[h].t() )\n",
    "\n",
    "      test_attention_weights = torch.sum( test_query_vector_sequence * test_key_vector_sequence, dim = 2 )  / np.sqrt( self.dim // 2 )\n",
    "      test_attention_weights = torch.nn.Softmax(dim = 1)( test_attention_weights )\n",
    "      test_attention_weights = test_attention_weights.unsqueeze(2)\n",
    "\n",
    "      test_attention_head = torch.sum( embedded * test_attention_weights, dim = 1 )\n",
    "\n",
    "      attention_heads_2.append( test_attention_head.unsqueeze(1) )\n",
    "\n",
    "    attention_heads_2 = torch.concat( attention_heads_2, dim = 1 )\n",
    "\n",
    "    processed_sequence_term = []\n",
    "    for i in range( seq_length ):\n",
    "      seq_term = embedded[:, i].unsqueeze(1)\n",
    "      seq_term = torch.concat( [ seq_term, attention_heads_2 ], dim = 1 )\n",
    "      seq_term = torch.nn.Flatten()( seq_term )\n",
    "\n",
    "      seq_term = self.attn_layer_2_layer_1( seq_term )\n",
    "      seq_term = nn.ReLU()( seq_term )\n",
    "      seq_term = self.attn_layer_2_layer_2( seq_term )\n",
    "\n",
    "      seq_term = self.batch_norm_2( embedded[:, i] + seq_term )\n",
    "\n",
    "      processed_sequence_term.append( seq_term.unsqueeze(1) )\n",
    "\n",
    "    embedded = torch.concat( processed_sequence_term, dim = 1 )\n",
    "    ################################################################################################\n",
    "\n",
    "    ############################################################################################\n",
    "    ## Final attention layer\n",
    "\n",
    "    ## Note that at this layer, we only have one attention head\n",
    "    ## The value vectors that we are combining are now mapped versions of the embedded sequence, though, of dimension twice as large\n",
    "\n",
    "    test_query_vector_sequence = torch.matmul( embedded, self.W_Q3.t() )\n",
    "    test_key_vector_sequence = torch.matmul( embedded, self.W_K3.t() )\n",
    "    test_value_vector_sequence = torch.matmul( embedded, self.W_V3.t() )\n",
    "\n",
    "    test_attention_weights = torch.sum( test_query_vector_sequence * test_key_vector_sequence, dim = 2 )  / np.sqrt( self.dim // 2 )\n",
    "    test_attention_weights = torch.nn.Softmax(dim = 1)( test_attention_weights )\n",
    "    test_attention_weights = test_attention_weights.unsqueeze(2)\n",
    "\n",
    "    test_attention_head = torch.sum( test_value_vector_sequence * test_attention_weights, dim = 1 )\n",
    "\n",
    "    # Final dimension here: (batch_size, 2 * feature_count )\n",
    "\n",
    "    logits = self.logit_layer( test_attention_head ) # Convert to probabilities\n",
    "\n",
    "    return logits\n",
    "    ################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "Rd0v2fZQ3c2Y"
   },
   "outputs": [],
   "source": [
    "transformer_model = TokenTransformerModel(tokens, embedding_size = 20, n_heads = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "BXjp4bmR3kmO"
   },
   "outputs": [],
   "source": [
    "batch_size = 7\n",
    "x_batch, y_batch = generate_batch(text, 35, batch_size, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqFu4wTj3sYv",
    "outputId": "59370c3b-a0ea-44bb-ec2c-43f1793f670c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 87])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model( x_batch ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "E2Y2v4Qv3wfY",
    "outputId": "0804b5cf-ab1a-4f57-a0fd-df875eede9b3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Dracula, where is my money?  ,rtlsetid;  ldure,em&tu!;tkts lt  ,t kt'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_next_character(model, substring, tokens):\n",
    "  indices = torch.LongTensor( [ [ tokens.index(c) for c in substring ] ] )\n",
    "  next_token_logits = model( indices ).detach()[0]\n",
    "  next_token_probabilities = torch.nn.Softmax( dim = 0 )( next_token_logits )\n",
    "  next_token_probabilities = np.asarray( next_token_probabilities )\n",
    "\n",
    "  next_character = random.choices( tokens, weights = next_token_probabilities, k = 1 )\n",
    "  return next_character[0]\n",
    "\n",
    "def extend_prompt(model, prompt, tokens, character_count):\n",
    "  model.eval()\n",
    "  output = prompt\n",
    "  for i in range( character_count ):\n",
    "    next_character = predict_next_character(model, output, tokens)\n",
    "    output += next_character\n",
    "  return output\n",
    "\n",
    "extend_prompt(transformer_model, \"Dracula, where is my money? \", tokens, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "D9_mbpfKHsu6"
   },
   "outputs": [],
   "source": [
    "transformer_model = TokenTransformerModel(tokens, embedding_size = 20, n_heads = 4)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam( transformer_model.parameters(), lr = 0.005, weight_decay = 0.001 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JPAGF9mpH-34",
    "outputId": "8b00ec8b-e81d-4924-9a40-83d8c662a78a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0\n",
      "########################################################\n",
      "Mina, t0\\0x9x09xx99e  e  weffplb eanrohd\n",
      " hhe \n",
      "########################################################\n",
      "Training Epoch 1\n",
      "########################################################\n",
      "Mina, \\x9899x29x92e\\990Ser reo9   t xvx\\0\\ c  \n",
      "########################################################\n",
      "Training Epoch 2\n",
      "########################################################\n",
      "Mina, 8888990899o0 \\n0x0Ttxn08y8\\8x9\n",
      "\\\\9xeox8x\n",
      "########################################################\n",
      "Training Epoch 3\n",
      "########################################################\n",
      "Mina, \\98998099\\0999990299\\#\\\\999999\\9x8000x8\\\n",
      "########################################################\n",
      "Training Epoch 4\n",
      "########################################################\n",
      "Mina, 89899x8x8908280x9D\\cxx8880cx0x990txx(x8\\\n",
      "########################################################\n",
      "Training Epoch 5\n",
      "########################################################\n",
      "Mina, \\x9xxx9xxx\\02x\\x8x8x8x98\\x980xx0\\9h2\\229\n",
      "########################################################\n",
      "Training Epoch 6\n",
      "########################################################\n",
      "Mina, 99x49T8tT90x9,e f brnAd0i lT5 &xi\n",
      ".adeo \n",
      "########################################################\n",
      "Training Epoch 7\n",
      "########################################################\n",
      "Mina, \\0xx980\\xxxx\\x\\9\\48x\\\\9000x0Z  0b\\ dta! \n",
      "########################################################\n",
      "Training Epoch 8\n",
      "########################################################\n",
      "Mina, xxxx098x8x90xx8x80x88x800x00208098x0xxD8\n",
      "########################################################\n",
      "Training Epoch 9\n",
      "########################################################\n",
      "Mina, eS9 9e  Y dpirurbyrtgelail ebrohdabt  hl\n",
      "########################################################\n",
      "Training Epoch 10\n",
      "########################################################\n",
      "Mina, e\n",
      "t\n",
      "cl n o oebptqo\n",
      "ser t suamhaw  ei e  \n",
      "########################################################\n",
      "Training Epoch 11\n",
      "########################################################\n",
      "Mina, xxx\\\\00x899ex\n",
      "2\\xxYxxxx\\\n",
      "x\n",
      "Hx\\00W\\2_eI\n",
      "\n",
      "\n",
      "########################################################\n",
      "Training Epoch 12\n",
      "########################################################\n",
      "Mina, gt dcsplysyG hehavn.saiIhuthhom *ea\n",
      "Tuh*\n",
      "########################################################\n",
      "Training Epoch 13\n",
      "########################################################\n",
      "Mina,  xxx9xxxx9x99xx08xXx8x08282x8x0088x8\\0\\2\n",
      "########################################################\n",
      "Training Epoch 14\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "for epoch in range(100):\n",
    "  print(\"Training Epoch\", epoch)\n",
    "\n",
    "  transformer_model.train()\n",
    "\n",
    "  for batch in range(100):\n",
    "    seq_length = random.randint(1,50)\n",
    "    x_batch, y_batch =  generate_batch(text, seq_length, batch_size, tokens)\n",
    "    optimizer.zero_grad()\n",
    "    logits = transformer_model( x_batch, masking = True )\n",
    "    loss = loss_function( logits, y_batch )\n",
    "    loss.backward( retain_graph = True ) #NOTE: I do not know why I needed to add retain_graph, it seems to thing I botched the computation graph somewhere, but I'm pretty sure I did not >.< This fixes it though, just runs a little slower than I'd like.\n",
    "    optimizer.step()\n",
    "    #print( loss.item() )\n",
    "\n",
    "  print(\"########################################################\")\n",
    "  print( extend_prompt(transformer_model, \"Mina, \", tokens, 40) )\n",
    "  print(\"########################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzFrgvapIMot"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
