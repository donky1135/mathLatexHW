{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YUH3WeLBcJh"
   },
   "source": [
    "In the upper right corner, select from the dropdown menu 'Change Runtime Type', and select a GPU as available - this will allow you to run your neural network training on accelerated hardware and run everything faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fKUVG_yVxys",
    "outputId": "474ecf7c-9001-48bf-fd85-4b79b5760ef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOkdVdGsCPNg"
   },
   "source": [
    "This indicates that a GPU has been detected and can be used - the device is saved to 'device' so that we can direct data and models to access memory on that device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnRW_4dyWG12",
    "outputId": "7f23810c-9631-4478-8723-09b5d82cead3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "train_X = torch.Tensor( trainset.data/255.0 - 0.5 )\n",
    "train_X = train_X.permute( 0, 3, 1, 2 )\n",
    "\n",
    "train_X = train_X.to( device ) # This line is different from the previous CIFAR code - it transfers the tensor to the GPU memory\n",
    "\n",
    "test_X = torch.Tensor( testset.data/255.0 - 0.5 )\n",
    "test_X = test_X.permute( 0, 3, 1, 2 )\n",
    "\n",
    "test_X = test_X.to( device ) # Again, transfering the tensor to GPU memory.\n",
    "\n",
    "train_Y = torch.Tensor( np.asarray( trainset.targets ) ).long()\n",
    "train_Y = train_Y.to( device )\n",
    "test_Y = torch.Tensor( np.asarray( testset.targets ) ).long()\n",
    "test_Y = test_Y.to( device )\n",
    "\n",
    "# All the data needs to be loaded into the GPU, as that is where the model processing will occur.\n",
    "\n",
    "def get_batch(x, y, batch_size):\n",
    "  n = x.shape[0]\n",
    "\n",
    "  batch_indices = random.sample( [ i for i in range(n) ], k = batch_size )\n",
    "\n",
    "  x_batch = x[ batch_indices ]\n",
    "  y_batch = y[ batch_indices ]\n",
    "\n",
    "  return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "stRuThG-WK5a"
   },
   "outputs": [],
   "source": [
    "class CIFARModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CIFARModel, self).__init__()\n",
    "\n",
    "    self.conv_layer_1 = nn.Conv2d(in_channels = 3, out_channels = 5, kernel_size = 3, stride = 1, bias=True)\n",
    "    self.conv_layer_2 = nn.Conv2d(in_channels = 5, out_channels = 10, kernel_size = 3, stride = 1, bias=True)\n",
    "    self.conv_layer_3 = nn.Conv2d(in_channels = 10, out_channels = 15, kernel_size = 3, stride = 1, bias=True)\n",
    "\n",
    "    self.linear_layer = torch.nn.Linear( in_features = 15*26*26, out_features = 10, bias=True )\n",
    "    # Note that the output of the last convolutional layer will be 15x26x16 - why?\n",
    "    # So we want to input 15*26*26 values into the last layer, and get 10 output values out (for the class probabilities)\n",
    "\n",
    "  def forward(self, input_tensor):\n",
    "    output = self.conv_layer_1( input_tensor )\n",
    "    output = nn.Sigmoid()( output )\n",
    "    output = self.conv_layer_2( output )\n",
    "    output = nn.Sigmoid()( output )\n",
    "    output = self.conv_layer_3( output )\n",
    "    output = nn.Sigmoid()( output )\n",
    "\n",
    "    # At this point, the block of node values from the convolutional layer is flattened\n",
    "    # So that it can be passed into a standard linear layer\n",
    "    output = nn.Flatten()( output )\n",
    "    output = self.linear_layer( output )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WHQ91OrnWYE0"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix( model, x, y ):\n",
    "  identification_counts = np.zeros( shape = (10,10), dtype = np.int32 )\n",
    "\n",
    "  logits = model( x )\n",
    "  predicted_classes = torch.argmax( logits, dim = 1 )\n",
    "\n",
    "  n = x.shape[0]\n",
    "\n",
    "  for i in range(n):\n",
    "    actual_class = int( y[i].item() )\n",
    "    predicted_class = predicted_classes[i].item()\n",
    "    identification_counts[actual_class, predicted_class] += 1\n",
    "\n",
    "  return identification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pHjBz3GWihM",
    "outputId": "6d5d6a04-6835-4317-ce08-b52ecc2969ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFARModel(\n",
      "  (conv_layer_1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv_layer_2): Conv2d(5, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv_layer_3): Conv2d(10, 15, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (linear_layer): Linear(in_features=10140, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_model = CIFARModel()\n",
    "\n",
    "cifar_model.to( device ) # The only change is that we also send the model to the GPU\n",
    "\n",
    "print( cifar_model )\n",
    "confusion_matrix( cifar_model, test_X, test_Y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHMoVFMXC5vG"
   },
   "source": [
    "At this point, everything else runs as before - just faster. I also increased the bach size, which I might could have done previously. Nevertheless - faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlptfBlvWlxk",
    "outputId": "bebcd9b3-8996-40a1-fd8e-593fd0dffa8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Test Loss: 2.3501369953155518\n"
     ]
    }
   ],
   "source": [
    "cnn_optimizer = optim.Adam(cifar_model.parameters(), lr = 0.01 )\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "print(\"Initial Test Loss:\", loss_function( cifar_model( test_X ), test_Y ).item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uaShVxtDWq4W",
    "outputId": "b01eb0eb-b1f0-44f5-fbc9-1022347ba5fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Total Loss over Batches: 0.15405796080672068\n",
      "[[498  66  59  29  33  19  15  21 190  70]\n",
      " [ 63 581  13  19  13   9  22  20  88 172]\n",
      " [ 93  19 325 127 167 104  65  41  34  25]\n",
      " [ 34  34 108 324  95 159  76  79  34  57]\n",
      " [ 54  20 129 105 381  70  70 130  22  19]\n",
      " [ 17  18  97 204  77 406  46  80  26  29]\n",
      " [ 22  29  88 124  91  68 495  30  20  33]\n",
      " [ 35  25  68  79 107  96  22 481  11  76]\n",
      " [146  89  32  21  17  13   9  17 580  76]\n",
      " [ 52 187  14  26  17  16  13  34  77 564]]\n",
      "Average Total Loss over Batches: 0.17345946182014882\n",
      "[[480  54  71  38  45  33  16  35 149  79]\n",
      " [ 57 546  20  28  19  21  30  30  70 179]\n",
      " [ 70  12 320 129 157 131  97  44  19  21]\n",
      " [ 23  19  97 327  88 207 100  82  21  36]\n",
      " [ 36  14 127 116 369  82  94 131  15  16]\n",
      " [ 11  10  83 189  66 457  53  95  17  19]\n",
      " [ 11  14  72 111  79  84 569  34   6  20]\n",
      " [ 23  12  70  74 108 116  31 519   3  44]\n",
      " [160  79  33  26  26  26  20  24 528  78]\n",
      " [ 45 166  19  38  27  28  20  53  57 547]]\n",
      "Average Total Loss over Batches: 0.15904684369180766\n",
      "[[482  54  81  41  53  34  19  34 138  64]\n",
      " [ 65 526  25  29  24  17  43  28  83 160]\n",
      " [ 82   8 313 132 156 122 102  45  22  18]\n",
      " [ 22  19  93 317 108 191 110  83  22  35]\n",
      " [ 29  10 133 114 397  76 100 115  14  12]\n",
      " [ 11   8  78 207  76 445  62  81  15  17]\n",
      " [  9  10  80 103  80  76 587  31   6  18]\n",
      " [ 20   7  69  77 144 120  39 479   6  39]\n",
      " [166  78  33  31  31  23  26  22 526  64]\n",
      " [ 53 151  30  35  35  31  26  55  65 519]]\n",
      "Average Total Loss over Batches: 0.15567818012032655\n",
      "[[511  44  82  38  45  24  14  39 149  54]\n",
      " [ 74 506  25  32  26  22  33  27  90 165]\n",
      " [ 88  10 330 130 159 126  66  54  23  14]\n",
      " [ 36  12 117 328  83 220  70  83  29  22]\n",
      " [ 39   7 139 111 390  85  70 129  18  12]\n",
      " [ 14   5  96 190  65 452  48  96  22  12]\n",
      " [ 18   9 106 127  83  83 524  27   9  14]\n",
      " [ 25   9  78  74 109 140  29 498   6  32]\n",
      " [171  61  31  27  25  26  13  22 573  51]\n",
      " [ 56 139  28  37  33  42  22  61  75 507]]\n",
      "Average Total Loss over Batches: 0.16687760599461232\n",
      "[[494  60  75  41  37  23  17  38 148  67]\n",
      " [ 66 559  18  23  19  16  28  25  86 160]\n",
      " [ 85  14 332 120 155 115  78  52  28  21]\n",
      " [ 33  25 123 305  80 193  87  80  28  46]\n",
      " [ 39  17 131 118 365  72  84 145  15  14]\n",
      " [ 13  15  88 188  69 438  54  95  18  22]\n",
      " [ 15  14  89 130  83  80 531  29  11  18]\n",
      " [ 29  16  74  74 103 109  29 514   7  45]\n",
      " [188  84  42  26  25  18  18  16 505  78]\n",
      " [ 59 177  30  33  20  30  14  54  57 526]]\n",
      "Average Total Loss over Batches: 0.16712209418204788\n",
      "[[476  74  82  47  42  32  14  40 120  73]\n",
      " [ 54 607  19  22  15  13  30  28  51 161]\n",
      " [ 76  16 308 179 115 119  89  60  20  18]\n",
      " [ 26  26  74 383  68 190  90  85  19  39]\n",
      " [ 33  15 127 171 335  76  83 132  13  15]\n",
      " [ 14  15  82 256  64 410  51  79  11  18]\n",
      " [ 17  16  83 156  63  69 532  37   6  21]\n",
      " [ 24  17  65  95  92 110  29 521   4  43]\n",
      " [166 119  37  42  21  17  20  18 475  85]\n",
      " [ 49 196  20  40  18  20  22  46  42 547]]\n",
      "Average Total Loss over Batches: 0.16165521253109752\n",
      "[[474  56  83  37  50  32  18  34 156  60]\n",
      " [ 64 559  26  32  17  15  27  23  70 167]\n",
      " [ 84  16 327 142 127 120  83  55  29  17]\n",
      " [ 31  20 108 329  84 190 100  75  21  42]\n",
      " [ 33  17 148 124 349  78  90 134  17  10]\n",
      " [ 15  13 101 219  64 407  56  87  21  17]\n",
      " [ 17  16  98 120  74  67 552  28   9  19]\n",
      " [ 24  15  88  90 106 101  31 496   4  45]\n",
      " [159  90  36  29  30  18  18  14 535  71]\n",
      " [ 53 186  19  30  21  30  19  51  57 534]]\n",
      "Average Total Loss over Batches: 0.16411878815743897\n",
      "[[473  62  67  32  37  17  13  26 190  83]\n",
      " [ 57 563  24  21  18  11  22  20  84 180]\n",
      " [ 84  14 346 117 147 102  76  51  35  28]\n",
      " [ 35  24 125 320  87 163  71  86  37  52]\n",
      " [ 37  18 153 111 354  69  85 124  25  24]\n",
      " [ 16  14 101 210  65 407  44  91  25  27]\n",
      " [ 15  18  98 124  68  64 545  25  13  30]\n",
      " [ 24  15  78  80  98  97  29 506  11  62]\n",
      " [141  92  31  21  17  17  13  16 583  69]\n",
      " [ 46 163  21  28  19  22   9  44  73 575]]\n",
      "Average Total Loss over Batches: 0.15943836660983557\n",
      "[[480  44 104  44  34  17  22  25 172  58]\n",
      " [ 72 511  35  34  17  13  42  25  97 154]\n",
      " [ 84  12 386 134 127  83 108  33  21  12]\n",
      " [ 28  12 154 360  85 139 120  47  28  27]\n",
      " [ 37  13 192 131 325  57 120  96  15  14]\n",
      " [ 12  15 131 251  60 360  78  62  17  14]\n",
      " [ 12  10 111 109  54  43 630  15   7   9]\n",
      " [ 26  10 122  97 110  92  45 452   5  41]\n",
      " [159  65  52  35  18  20  30  15 546  60]\n",
      " [ 58 162  46  60  21  31  36  41  72 473]]\n",
      "Average Total Loss over Batches: 0.16630643377221252\n",
      "[[500  61  74  45  40  24  12  34 152  58]\n",
      " [ 64 568  23  25  22  11  29  30  73 155]\n",
      " [ 95  17 326 121 155 113  90  50  23  10]\n",
      " [ 31  26 110 316  92 198  95  72  24  36]\n",
      " [ 38  11 136 117 377  79  94 117  17  14]\n",
      " [ 13  15  98 210  67 425  50  87  19  16]\n",
      " [ 19  11  98 105  78  71 566  29   9  14]\n",
      " [ 26  19  72  84 127 113  32 475   6  46]\n",
      " [170  97  41  27  28  23  18  15 518  63]\n",
      " [ 60 190  22  28  30  31  20  48  49 522]]\n",
      "Average Total Loss over Batches: 0.16514040450977785\n",
      "[[504  56  60  37  39  22  17  37 163  65]\n",
      " [ 63 546  19  21  14  11  32  21  95 178]\n",
      " [109  18 329 115 135 102 100  40  29  23]\n",
      " [ 49  28 107 310  88 153 106  75  40  44]\n",
      " [ 45  20 134 109 362  76 102 106  26  20]\n",
      " [ 24  18  96 206  75 387  59  87  25  23]\n",
      " [ 23  18  95  92  61  52 596  28  11  24]\n",
      " [ 35  22  75  74 115  98  40 465  11  65]\n",
      " [172  80  31  14  18  17  18  15 562  73]\n",
      " [ 57 169  22  27  22  21  18  39  74 551]]\n",
      "Average Total Loss over Batches: 0.1621036385884272\n",
      "[[506  45  61  38  35  17  14  31 183  70]\n",
      " [ 70 546  20  19  22  10  28  21 101 163]\n",
      " [104  13 341 136 127  89  95  43  34  18]\n",
      " [ 35  27 122 347  78 145  93  75  40  38]\n",
      " [ 40  13 153 126 359  55  97 110  31  16]\n",
      " [ 21  13 104 229  62 383  58  87  25  18]\n",
      " [ 19  14  95 110  60  52 601  19  11  19]\n",
      " [ 39  14  80  84 107  97  37 489   7  46]\n",
      " [166  78  29  19  19  16  19  15 574  65]\n",
      " [ 72 146  21  31  24  27  16  53  79 531]]\n",
      "Average Total Loss over Batches: 0.1568211591706083\n",
      "[[450  72  63  40  38  33  17  40 175  72]\n",
      " [ 52 598  21  20  13  16  24  22  67 167]\n",
      " [ 76  17 331 123 157 118  69  55  32  22]\n",
      " [ 29  28  90 318  89 220  76  86  24  40]\n",
      " [ 36  20 135 120 372  86  74 121  23  13]\n",
      " [ 10  14  72 197  64 464  39 100  20  20]\n",
      " [ 11  19  93 124  74  90 516  34  15  24]\n",
      " [ 27  17  67  78  99 121  26 502   7  56]\n",
      " [148  99  34  26  25  18   8  17 556  69]\n",
      " [ 44 191  25  26  24  33  10  53  65 529]]\n",
      "Average Total Loss over Batches: 0.16651863858569554\n",
      "[[473  80  47  38  35  18  11  33 193  72]\n",
      " [ 59 573  13  18  11  15  17  24 100 170]\n",
      " [110  25 273 126 147 113  69  71  42  24]\n",
      " [ 48  33  91 309  91 152  74 110  41  51]\n",
      " [ 49  23  98 108 365  81  74 144  32  26]\n",
      " [ 19  27  71 190  67 401  42 119  39  25]\n",
      " [ 24  27  69 125  88  67 516  35  17  32]\n",
      " [ 46  18  54  67  95  93  21 529  16  61]\n",
      " [158  96  24  17  17  12  10  20 557  89]\n",
      " [ 59 187  12  25  19  22   7  49  76 544]]\n",
      "Average Total Loss over Batches: 0.17015971800412638\n",
      "[[493  56 101  30  45  18  23  25 161  48]\n",
      " [ 76 557  28  23  21  10  45  25  94 121]\n",
      " [ 95  14 375 125 130  72 103  40  32  14]\n",
      " [ 41  19 142 337  81 137 129  61  29  24]\n",
      " [ 39  13 172 115 353  59 111 105  26   7]\n",
      " [ 18  12 133 214  56 390  73  69  25  10]\n",
      " [ 22  15 115  91  57  45 620  15  12   8]\n",
      " [ 36  13 109  97 113  96  50 450   8  28]\n",
      " [176  98  50  21  25  19  28   9 511  63]\n",
      " [ 71 169  34  44  26  32  37  46  70 471]]\n",
      "Average Total Loss over Batches: 0.16715372621599353\n",
      "[[496  70  51  29  34  14  16  26 186  78]\n",
      " [ 51 573  17  18  12  11  21  18  84 195]\n",
      " [102  20 320 141 127 104  75  49  34  28]\n",
      " [ 40  41  95 313  81 163  82  91  38  56]\n",
      " [ 46  23 126 128 326  77  93 124  27  30]\n",
      " [ 20  19  77 204  62 402  56  97  30  33]\n",
      " [ 20  27  79 105  67  59 565  30  13  35]\n",
      " [ 33  21  73  79  96  76  24 501   9  88]\n",
      " [144  91  29  17  19  14  11  14 562  99]\n",
      " [ 47 178  15  24  15  17   7  33  60 604]]\n",
      "Average Total Loss over Batches: 0.15800254811115613\n",
      "[[447  66  68  35  48  21  14  36 199  66]\n",
      " [ 58 553  20  23  18  12  27  24 100 165]\n",
      " [101  20 316 129 153 102  83  36  42  18]\n",
      " [ 31  33 107 299 107 167 101  66  39  50]\n",
      " [ 39  22 127 101 381  74 103 104  32  17]\n",
      " [ 17  16  88 193  65 425  59  87  28  22]\n",
      " [ 15  18  95  93  78  69 576  23  12  21]\n",
      " [ 27  21  82  74 118 108  34 461  19  56]\n",
      " [142  89  27  15  23  17  16  14 572  85]\n",
      " [ 49 185  24  24  21  29  15  45  74 534]]\n",
      "Average Total Loss over Batches: 0.1616684439255216\n",
      "[[527  38  71  44  45  13  16  33 170  43]\n",
      " [ 81 506  28  25  28  12  34  26 117 143]\n",
      " [ 98  13 344 132 156  90  72  47  36  12]\n",
      " [ 43  15 121 314 108 156  84  78  53  28]\n",
      " [ 46  14 131 118 380  68  87 109  34  13]\n",
      " [ 22  12 102 214  76 387  47  90  35  15]\n",
      " [ 24  11 103 114  98  62 526  23  23  16]\n",
      " [ 39  10  70  80 136  94  27 488  17  39]\n",
      " [166  55  36  25  23  18  13  16 607  41]\n",
      " [ 65 152  27  34  34  28  17  50  99 494]]\n",
      "Average Total Loss over Batches: 0.1615239701263696\n",
      "[[495  48  71  43  51  15  16  37 175  49]\n",
      " [ 78 519  32  30  25  15  32  27  95 147]\n",
      " [ 92  10 351 125 163  95  83  40  30  11]\n",
      " [ 31  13 129 326 112 170  96  76  27  20]\n",
      " [ 36  11 153 110 402  62  92 105  19  10]\n",
      " [ 18   9 119 204  65 414  49  89  23  10]\n",
      " [ 18   9 115 112  93  60 548  25  10  10]\n",
      " [ 23  12  83  87 138 104  37 476   9  31]\n",
      " [174  73  32  25  28  18  21  21 553  55]\n",
      " [ 68 168  31  38  35  33  23  60  81 463]]\n",
      "Average Total Loss over Batches: 0.17327016869420164\n",
      "[[476  68  73  40  51  25  13  39 146  69]\n",
      " [ 56 557  24  23  20  13  29  28  73 177]\n",
      " [ 86  11 316 108 174 109 100  54  23  19]\n",
      " [ 31  21 109 280 118 177 111  89  22  42]\n",
      " [ 37  12 121  77 399  74 112 134  17  17]\n",
      " [ 18  14  93 166  75 425  59 107  20  23]\n",
      " [ 13  14  93  97  82  61 587  27   7  19]\n",
      " [ 26  18  72  68 118 102  34 508   5  49]\n",
      " [182 105  32  23  33  16  19  15 495  80]\n",
      " [ 52 178  19  23  30  25  23  51  52 547]]\n",
      "Average Total Loss over Batches: 0.16425757776673994\n",
      "[[485  62  71  47  53  23  21  35 143  60]\n",
      " [ 63 557  20  28  22  17  35  27  83 148]\n",
      " [ 77  13 325 133 169  96  99  52  19  17]\n",
      " [ 28  20  97 338 113 154 124  78  20  28]\n",
      " [ 31  16 117 125 380  57 117 128  15  14]\n",
      " [ 13  15  91 215  74 397  62 100  18  15]\n",
      " [ 13  13  77 105  71  47 620  30   7  17]\n",
      " [ 19  14  59  92 118  98  38 514   8  40]\n",
      " [149  85  38  37  26  24  28  21 530  62]\n",
      " [ 44 184  23  37  30  23  28  42  69 520]]\n",
      "Average Total Loss over Batches: 0.16133708730122634\n",
      "[[481  70  65  36  36  17  14  30 171  80]\n",
      " [ 55 565  17  20   8  15  25  20  75 200]\n",
      " [ 92  20 341 132 122  95  86  51  30  31]\n",
      " [ 29  33 104 342  76 153  94  86  26  57]\n",
      " [ 41  24 139 132 326  72  94 134  17  21]\n",
      " [ 16  21  99 209  60 404  48  98  19  26]\n",
      " [ 16  22  90 121  65  60 556  24  12  34]\n",
      " [ 27  23  70  77  88  94  31 518  10  62]\n",
      " [150  95  26  18  16  17  14  16 545 103]\n",
      " [ 40 179  15  23  19  18  19  37  54 596]]\n",
      "Average Total Loss over Batches: 0.15863059226097256\n",
      "[[515  63  64  37  41  10  19  29 156  66]\n",
      " [ 64 553  18  20  20   7  32  23  87 176]\n",
      " [ 97  16 334 138 147  72 102  54  21  19]\n",
      " [ 38  22 108 342  96 112 116  97  22  47]\n",
      " [ 37  17 121 126 372  43 115 134  17  18]\n",
      " [ 20  19  91 226  81 353  61 102  21  26]\n",
      " [ 17  16  80 110  72  40 606  28  10  21]\n",
      " [ 27  18  76  73 108  73  31 535   5  54]\n",
      " [159  82  30  24  21  12  20  17 559  76]\n",
      " [ 47 171  18  28  22  18  19  43  62 572]]\n",
      "Average Total Loss over Batches: 0.1716133601554919\n",
      "[[512  57  63  37  36  16  12  29 180  58]\n",
      " [ 74 555  23  21  15  12  28  19 109 144]\n",
      " [106  16 360 114 136 102  74  46  30  16]\n",
      " [ 49  25 119 303  82 188  83  79  33  39]\n",
      " [ 49  17 138 115 363  74  89 116  24  15]\n",
      " [ 27  16 102 201  71 418  46  82  21  16]\n",
      " [ 18  22 103 114  85  71 527  28  14  18]\n",
      " [ 40  19  84  72 106 102  32 482  12  51]\n",
      " [174  87  31  19  20  20  14  17 546  72]\n",
      " [ 72 172  26  26  26  28  12  47  69 522]]\n",
      "Average Total Loss over Batches: 0.15250417138652436\n",
      "[[423  76  64  44  53  29  17  53 156  85]\n",
      " [ 47 576  18  23  15  16  28  26  51 200]\n",
      " [ 79  20 315 113 159 123  80  68  21  22]\n",
      " [ 19  34  98 294  92 209  80 102  19  53]\n",
      " [ 26  20 109 109 375  90  92 145  12  22]\n",
      " [ 11  19  77 156  67 456  50 123  20  21]\n",
      " [ 14  21  84 114  85  75 530  42   5  30]\n",
      " [ 17  18  60  57 116 108  27 535   7  55]\n",
      " [132  96  42  28  25  26  22  24 498 107]\n",
      " [ 32 187  15  25  26  33  18  45  41 578]]\n",
      "Average Total Loss over Batches: 0.16628168946850408\n",
      "[[472  58  70  52  53  22  10  41 153  69]\n",
      " [ 69 533  25  29  17  13  27  31  80 176]\n",
      " [ 86  10 344 140 158 102  61  55  27  17]\n",
      " [ 28  18 112 349 102 171  65  83  30  42]\n",
      " [ 35  15 133 133 399  73  61 118  17  16]\n",
      " [ 11  17  99 213  76 403  36 103  26  16]\n",
      " [ 16  17 111 134 101  70 487  31   9  24]\n",
      " [ 23  12  75  78 116 102  19 520   6  49]\n",
      " [176  83  35  30  30  15  14  22 522  73]\n",
      " [ 55 157  22  33  30  25   9  60  56 553]]\n",
      "Average Total Loss over Batches: 0.16372350192522178\n",
      "[[498  64  74  45  34  25  17  28 161  54]\n",
      " [ 64 590  25  30  12  19  32  23  90 115]\n",
      " [ 89  18 325 142 137 103 100  43  30  13]\n",
      " [ 37  17 101 360  75 181 101  70  32  26]\n",
      " [ 41  12 129 152 329  78  96 123  23  17]\n",
      " [ 17  11  95 225  51 428  62  79  18  14]\n",
      " [ 15  14  79 127  67  67 583  24  10  14]\n",
      " [ 33  18  88  95  93 121  42 470   6  34]\n",
      " [169  90  41  34  18  17  20  19 532  60]\n",
      " [ 66 191  26  38  26  33  20  53  60 487]]\n",
      "Average Total Loss over Batches: 0.16726934173314512\n",
      "[[441  65  66  53  39  24  17  32 186  77]\n",
      " [ 66 544  20  26  22  14  26  19 101 162]\n",
      " [ 74  17 312 163 142 122  70  44  42  14]\n",
      " [ 25  17 100 367  90 188  73  67  33  40]\n",
      " [ 26  11 133 148 360  83  81 120  22  16]\n",
      " [ 13  11  83 238  54 431  44  83  26  17]\n",
      " [ 15  14 100 141  92  74 518  16  10  20]\n",
      " [ 25  10  66 103 108 118  23 491  12  44]\n",
      " [146  70  43  23  24  21  18  17 561  77]\n",
      " [ 47 174  21  35  26  27  16  48  71 535]]\n",
      "Average Total Loss over Batches: 0.16266858935970033\n",
      "[[538  51  66  35  24  20  22  20 165  59]\n",
      " [ 65 580  17  23  16  13  33  23  90 140]\n",
      " [102  13 349 119 133 101 107  35  23  18]\n",
      " [ 43  25 117 317  86 159 110  74  31  38]\n",
      " [ 51  16 154 113 351  79 101 100  18  17]\n",
      " [ 18  15 101 198  57 434  57  79  24  17]\n",
      " [ 17  16  85  94  61  55 621  18  12  21]\n",
      " [ 34  19  84  84 119 100  37 471   5  47]\n",
      " [163  85  39  20  19  17  22  11 571  53]\n",
      " [ 59 206  22  30  25  23  22  40  67 506]]\n",
      "Average Total Loss over Batches: 0.16842263814018413\n",
      "[[487  52  69  44  36  17  13  31 186  65]\n",
      " [ 64 535  26  22  14  14  27  21 104 173]\n",
      " [ 90  13 371 126 135  89  79  46  34  17]\n",
      " [ 32  20 124 347  75 160  81  84  37  40]\n",
      " [ 47  12 156 142 344  75  75 113  16  20]\n",
      " [ 19  13 111 214  63 384  50 100  22  24]\n",
      " [ 20  15 100 121  78  65 534  27  10  30]\n",
      " [ 34  13  91  83  92  97  23 502   9  56]\n",
      " [178  79  37  23  15  18  16  17 543  74]\n",
      " [ 62 163  23  28  23  26  13  53  68 541]]\n",
      "Average Total Loss over Batches: 0.16439115291147172\n",
      "[[475  52  72  47  46  21  15  34 177  61]\n",
      " [ 62 560  25  25  19  17  28  18  96 150]\n",
      " [ 80  12 350 148 132 104  93  42  23  16]\n",
      " [ 30  19 124 346  87 161 101  70  35  27]\n",
      " [ 32  15 155 136 350  76  93 108  23  12]\n",
      " [ 19  10 106 241  64 393  52  78  24  13]\n",
      " [ 15  12  90 132  69  61 568  21  15  17]\n",
      " [ 21  15  91  92 113  87  33 502   7  39]\n",
      " [165  78  37  27  23  18  15  13 563  61]\n",
      " [ 54 183  29  37  29  26  23  50  78 491]]\n",
      "Average Total Loss over Batches: 0.1672904663446137\n",
      "[[498  58  60  39  51  20  12  32 160  70]\n",
      " [ 62 553  18  19  24  19  26  22  89 168]\n",
      " [ 92  19 302 129 183 112  68  52  27  16]\n",
      " [ 26  25  94 330 124 176  66  82  37  40]\n",
      " [ 38  17 110 113 422  76  69 122  18  15]\n",
      " [ 19  12  79 185  85 441  44  96  20  19]\n",
      " [ 17  13 101 123 110  73 501  28  12  22]\n",
      " [ 26  15  60  78 130 109  22 501  14  45]\n",
      " [185  74  36  25  30  18  12  16 525  79]\n",
      " [ 53 173  21  33  26  29  13  48  70 534]]\n",
      "Average Total Loss over Batches: 0.1753617752258233\n",
      "[[505  75  84  41  29  16  18  18 160  54]\n",
      " [ 57 583  24  22  12  14  32  18 101 137]\n",
      " [ 94  19 377 120 137  78  99  38  22  16]\n",
      " [ 46  28 142 321  69 143 124  60  34  33]\n",
      " [ 52  21 175 107 331  63 124  92  14  21]\n",
      " [ 23  19 135 212  46 378  66  76  27  18]\n",
      " [ 18  17 106 102  63  50 600  18  10  16]\n",
      " [ 36  23 112  85  95  90  33 465   8  53]\n",
      " [183  95  39  29  18  14  19  18 535  50]\n",
      " [ 69 220  30  35  19  26  22  36  68 475]]\n",
      "Average Total Loss over Batches: 0.17227994054409937\n",
      "[[456  67  62  40  42  22  21  30 182  78]\n",
      " [ 53 577  16  17  17  10  30  20  88 172]\n",
      " [ 86  15 323 125 151  85 112  50  33  20]\n",
      " [ 28  28 117 304 100 142 128  76  28  49]\n",
      " [ 34  22 111 117 377  58 122 112  26  21]\n",
      " [ 18  17  88 213  78 361  77  99  28  21]\n",
      " [ 18  16  80  98  72  44 624  21   7  20]\n",
      " [ 24  19  77  82 121  76  44 492   5  60]\n",
      " [154  90  32  21  14  13  21  26 563  66]\n",
      " [ 45 179  18  25  24  22  24  41  62 560]]\n",
      "Average Total Loss over Batches: 0.1634992618735944\n",
      "[[446  40  79  45  65  32  26  45 158  64]\n",
      " [ 64 477  22  26  41  20  48  33  91 178]\n",
      " [ 81   8 303 133 164 114 110  56  19  12]\n",
      " [ 15  12 102 320 116 172 123  86  25  29]\n",
      " [ 24  10  99 112 401  79 126 118  17  14]\n",
      " [ 16   6  80 201  82 418  76  91  18  12]\n",
      " [ 10   6  81  96  99  62 610  28   3   5]\n",
      " [ 17   8  64  85 134 108  48 496   6  34]\n",
      " [145  63  30  26  37  25  32  28 551  63]\n",
      " [ 49 132  26  40  43  39  33  66  71 501]]\n",
      "Average Total Loss over Batches: 0.15955804776237523\n",
      "[[466  64  70  40  48  21  17  26 176  72]\n",
      " [ 55 570  19  25  17  18  27  27  87 155]\n",
      " [ 86  14 336 140 138 113  71  46  35  21]\n",
      " [ 29  22 110 360  79 183  73  76  29  39]\n",
      " [ 35  14 130 134 362  97  79 109  23  17]\n",
      " [ 13  12  85 215  55 445  47  88  22  18]\n",
      " [ 18  14  96 131  82  82 523  27   9  18]\n",
      " [ 25  16  71  80 110 118  27 495  12  46]\n",
      " [146  84  40  22  22  22  13  22 555  74]\n",
      " [ 46 180  26  34  22  27  13  52  67 533]]\n",
      "Average Total Loss over Batches: 0.16454400393908353\n",
      "[[513  59  69  41  37  19  16  35 155  56]\n",
      " [ 61 585  18  22  18  12  25  26  81 152]\n",
      " [102  19 352 113 140  82  91  55  30  16]\n",
      " [ 42  26 134 323  84 136 100  78  29  48]\n",
      " [ 46  18 154 120 349  62  96 116  22  17]\n",
      " [ 21  16 116 208  61 381  57  94  25  21]\n",
      " [ 21  18 111 101  68  53 575  26   9  18]\n",
      " [ 33  20  80  77 112  82  30 511   9  46]\n",
      " [183  98  35  20  15  15  17  21 522  74]\n",
      " [ 54 186  26  29  23  21  16  51  55 539]]\n",
      "Average Total Loss over Batches: 0.1705092240171819\n",
      "[[482  52  73  38  45  20  13  29 179  69]\n",
      " [ 74 517  22  23  17  16  33  16 106 176]\n",
      " [ 99  13 348 115 146 109  70  43  35  22]\n",
      " [ 36  19 134 306  94 172  75  73  42  49]\n",
      " [ 44  16 167 108 344  86  73 113  28  21]\n",
      " [ 21   9 110 204  71 413  44  78  28  22]\n",
      " [ 15  14 126 112  95  77 501  23  11  26]\n",
      " [ 35  16  91  67 122 102  25 471  10  61]\n",
      " [181  69  32  12  21  17  12  15 572  69]\n",
      " [ 58 143  31  31  21  20  14  48  80 554]]\n",
      "Average Total Loss over Batches: 0.16255985456053562\n",
      "[[472  62  69  42  39  18  18  35 189  56]\n",
      " [ 68 551  23  28  15  14  30  21  96 154]\n",
      " [ 89  13 347 118 143  89  99  53  32  17]\n",
      " [ 36  22 123 321  83 155 104  85  35  36]\n",
      " [ 40  18 147 122 338  70 103 130  18  14]\n",
      " [ 19  14 104 199  59 394  63  96  34  18]\n",
      " [ 17  12 103 116  71  59 574  25  11  12]\n",
      " [ 33  19  80  75 100  91  36 513  12  41]\n",
      " [177  78  40  18  21  16  19  15 546  70]\n",
      " [ 66 184  28  31  20  24  21  53  70 503]]\n",
      "Average Total Loss over Batches: 0.17704485077366564\n",
      "[[498  59  67  24  33  16  15  21 205  62]\n",
      " [ 59 569  15  13  12  15  27  22 107 161]\n",
      " [ 92  19 361 113 129 105  82  39  38  22]\n",
      " [ 39  29 126 301  74 166  95  77  51  42]\n",
      " [ 44  17 165 119 335  78  85 102  33  22]\n",
      " [ 21  17 109 189  62 406  49  87  40  20]\n",
      " [ 21  17 107 113  65  66 544  27  20  20]\n",
      " [ 39  19  83  75  95  95  30 484  19  61]\n",
      " [142  82  26  15  16  16  11  14 626  52]\n",
      " [ 54 180  23  19  11  20  12  43  91 547]]\n",
      "Average Total Loss over Batches: 0.15559732799141682\n",
      "[[481  61  82  43  41  26  18  30 160  58]\n",
      " [ 63 557  21  26  17  17  34  24  91 150]\n",
      " [ 79  15 355 128 123 109  99  44  31  17]\n",
      " [ 31  25 118 354  67 171  98  70  26  40]\n",
      " [ 38  21 159 127 323  74 110 117  16  15]\n",
      " [ 15  11  96 216  59 427  53  86  18  19]\n",
      " [ 14  13  95 118  57  63 587  25  11  17]\n",
      " [ 30  17  82  91  98 112  31 487  10  42]\n",
      " [166  90  41  29  20  27  24  14 526  63]\n",
      " [ 51 185  28  35  16  33  19  46  66 521]]\n",
      "Average Total Loss over Batches: 0.17024373156166497\n",
      "[[503  67  65  36  32  21  15  28 169  64]\n",
      " [ 61 571  20  24  12  12  21  20  95 164]\n",
      " [ 95  22 332 129 123 107  83  49  38  22]\n",
      " [ 41  35 113 322  80 163  76  88  32  50]\n",
      " [ 46  24 145 128 330  73  90 119  21  24]\n",
      " [ 22  19 102 210  69 394  44  84  27  29]\n",
      " [ 21  26  88 123  77  54 529  36  16  30]\n",
      " [ 34  21  77  74  98  90  29 498  13  66]\n",
      " [165  85  29  15  18  18  15  15 575  65]\n",
      " [ 60 195  17  26  14  17  13  40  74 544]]\n",
      "Average Total Loss over Batches: 0.16081598115403897\n",
      "[[507  58  77  31  38  17  16  23 175  58]\n",
      " [ 71 543  30  22  18  13  33  18  97 155]\n",
      " [103  14 387  98 147  93  80  39  26  13]\n",
      " [ 38  26 147 292  97 163  99  67  33  38]\n",
      " [ 52  12 160  94 394  75  93  90  14  16]\n",
      " [ 15  16 123 193  68 412  57  73  25  18]\n",
      " [ 17  15 111 100  77  57 576  18  12  17]\n",
      " [ 35  15 106  78 125 100  34 456  10  41]\n",
      " [187  89  38  20  16  17  18  12 539  64]\n",
      " [ 71 179  28  29  28  22  20  48  60 515]]\n",
      "Average Total Loss over Batches: 0.17442966615614422\n",
      "[[467  54  98  38  44  13  12  32 185  57]\n",
      " [ 60 546  30  26  15  16  35  23 110 139]\n",
      " [ 74  14 405 122 138  89  83  35  27  13]\n",
      " [ 31  22 149 336  97 130  92  74  41  28]\n",
      " [ 36  12 184 123 359  57  75 117  20  17]\n",
      " [ 11  12 141 218  76 360  57  82  30  13]\n",
      " [ 14  16 127 117  82  48 545  24  10  17]\n",
      " [ 25  13 117  86 131  78  32 474  10  34]\n",
      " [141  69  38  22  16  16  17  17 606  58]\n",
      " [ 60 164  35  32  28  24  18  42  93 504]]\n",
      "Average Total Loss over Batches: 0.16885363813926252\n",
      "[[503  61  60  34  42  20  16  23 172  69]\n",
      " [ 60 539  17  14  15  14  30  25 104 182]\n",
      " [ 94  13 332 128 141 115  85  41  28  23]\n",
      " [ 33  23 111 333  95 148  98  71  37  51]\n",
      " [ 38  19 129 111 379  76  94 113  20  21]\n",
      " [ 18  14  89 217  72 420  53  76  21  20]\n",
      " [ 18  12  90 118  80  62 560  27  12  21]\n",
      " [ 28  15  77  77 123  88  37 492  10  53]\n",
      " [156  81  27  15  22  16  18  15 580  70]\n",
      " [ 52 166  18  27  25  17  15  47  73 560]]\n",
      "Average Total Loss over Batches: 0.15994443090460495\n",
      "[[520  54  67  41  42  22  17  28 146  63]\n",
      " [ 69 562  21  20  20  14  36  26  81 151]\n",
      " [ 99  11 329 130 147  84 114  47  23  16]\n",
      " [ 36  20 117 343  91 144 122  74  25  28]\n",
      " [ 39  10 130 113 388  61 116 112  16  15]\n",
      " [ 18  12 102 224  71 387  63  83  20  20]\n",
      " [ 15  12  83  96  76  50 625  19  11  13]\n",
      " [ 29  18  82  86 117  89  36 500   8  35]\n",
      " [198  78  35  30  24  15  22  18 515  65]\n",
      " [ 59 168  27  35  24  27  23  43  57 537]]\n",
      "Average Total Loss over Batches: 0.1640392748452295\n",
      "[[494  43  95  44  57  29  21  34 127  56]\n",
      " [ 70 506  28  34  28  20  41  24  71 178]\n",
      " [ 89  10 351 138 146 103  87  41  22  13]\n",
      " [ 28  19 120 368  93 153 100  64  23  32]\n",
      " [ 38   9 142 109 387  64 107 120  13  11]\n",
      " [ 18  13  93 230  72 393  57  83  21  20]\n",
      " [ 12  17 105 117  85  58 560  23   9  14]\n",
      " [ 27  13  77  86 123  99  29 496   9  41]\n",
      " [168  73  47  43  36  24  27  17 498  67]\n",
      " [ 56 154  24  38  32  28  25  51  52 540]]\n",
      "Average Total Loss over Batches: 0.1637229757206596\n",
      "[[476  50  79  46  38  21  14  32 179  65]\n",
      " [ 61 533  21  30  20  13  36  25  96 165]\n",
      " [ 75  11 354 141 141 102  85  46  27  18]\n",
      " [ 28  19 131 358  81 163  85  72  32  31]\n",
      " [ 29  11 140 146 358  71  86 119  19  21]\n",
      " [ 13  10 104 232  76 388  52  86  21  18]\n",
      " [ 14   9 103 131  76  60 548  28  12  19]\n",
      " [ 25  14  79  88 104 100  33 506   6  45]\n",
      " [156  73  31  29  25  18  19  18 564  67]\n",
      " [ 54 152  31  38  22  27  21  54  66 535]]\n",
      "Average Total Loss over Batches: 0.1747172140892265\n",
      "[[487  59  73  32  32  16  18  21 213  49]\n",
      " [ 65 557  20  19  21  14  29  19 116 140]\n",
      " [ 95  13 364 111 126 106  91  41  39  14]\n",
      " [ 40  26 144 295  81 166  91  69  57  31]\n",
      " [ 48  16 187 102 330  75  91 109  26  16]\n",
      " [ 21  15 122 184  66 407  59  76  29  21]\n",
      " [ 20  19 118 109  77  64 538  27  17  11]\n",
      " [ 38  17 102  77 112 101  34 471  14  34]\n",
      " [145  74  31  18  12  15  14  18 630  43]\n",
      " [ 65 180  31  31  26  30  25  43 109 460]]\n",
      "Average Total Loss over Batches: 0.16899356314807104\n",
      "[[471  65  84  42  47  32  18  26 149  66]\n",
      " [ 56 562  21  26  19  26  26  25  74 165]\n",
      " [ 87  15 320 135 132 138  77  47  30  19]\n",
      " [ 31  21  92 324  93 207  76  82  28  46]\n",
      " [ 34  14 137 128 366  93  82 107  23  16]\n",
      " [ 13  13  94 213  71 435  45  73  16  27]\n",
      " [ 23  18  90 126  81  84 508  28  14  28]\n",
      " [ 31  14  79  88 109 131  23 463  11  51]\n",
      " [144  94  38  30  27  26  15  12 540  74]\n",
      " [ 46 188  18  36  23  34  17  42  64 532]]\n",
      "Average Total Loss over Batches: 0.1595771921639141\n",
      "[[507  44  71  40  48  27  18  37 152  56]\n",
      " [ 75 520  24  31  23  19  33  28  85 162]\n",
      " [ 97  10 316 132 156 117  83  50  24  15]\n",
      " [ 29  20  99 324  99 201  86  74  29  39]\n",
      " [ 35  11 126 123 387  82  88 117  19  12]\n",
      " [ 17  10  80 212  78 438  49  80  19  17]\n",
      " [ 14  15  81 114  97  70 551  31   9  18]\n",
      " [ 29  15  75  85 101 119  31 500   7  38]\n",
      " [186  74  31  28  29  22  18  13 538  61]\n",
      " [ 62 174  14  30  33  39  18  56  70 504]]\n",
      "Average Total Loss over Batches: 0.18796335436493847\n",
      "[[435  54  81  40  50  34  18  30 183  75]\n",
      " [ 56 544  21  24  22  19  35  26  95 158]\n",
      " [ 70  10 318 134 153 117 103  48  30  17]\n",
      " [ 21  19 109 337  98 187 102  68  31  28]\n",
      " [ 23  11 127 110 382  87 110 112  22  16]\n",
      " [ 10  12  91 216  66 425  62  76  25  17]\n",
      " [ 12  11  85 104  65  65 609  22  13  14]\n",
      " [ 21  14  81  99 122 110  32 474   8  39]\n",
      " [149  78  39  31  29  20  28  18 531  77]\n",
      " [ 46 160  28  35  29  41  23  47  63 528]]\n",
      "Average Total Loss over Batches: 0.1768022741361852\n",
      "[[482  53  71  39  54  30  16  30 162  63]\n",
      " [ 59 537  16  26  19  17  32  30  88 176]\n",
      " [ 86  15 344 135 148  97  84  51  23  17]\n",
      " [ 32  20 115 331  93 171  90  79  30  39]\n",
      " [ 37  12 136 116 380  67  95 125  15  17]\n",
      " [ 13  14  97 208  71 403  54  97  23  20]\n",
      " [ 14  16  94 131  84  67 540  25  11  18]\n",
      " [ 21  13  82  86 109 107  34 503   8  37]\n",
      " [172  70  36  31  27  17  16  21 539  71]\n",
      " [ 48 154  25  36  25  33  17  48  63 551]]\n",
      "Average Total Loss over Batches: 0.16770985713362552\n",
      "[[474  60  77  47  39  23  19  35 167  59]\n",
      " [ 51 554  25  30  26  17  36  26  89 146]\n",
      " [ 73  13 339 127 154 100 107  46  27  14]\n",
      " [ 27  20 119 336  97 150 114  75  29  33]\n",
      " [ 30  10 128 118 361  79 125 117  20  12]\n",
      " [  9  11  95 207  72 403  71  95  21  16]\n",
      " [ 14   9  81  88  73  56 633  25   9  12]\n",
      " [ 22  12  76  84 120 111  43 491   7  34]\n",
      " [145  88  39  28  22  20  23  19 561  55]\n",
      " [ 47 150  28  37  30  32  30  50  76 520]]\n",
      "Average Total Loss over Batches: 0.17619807477512642\n",
      "[[486  59  76  33  47  23  17  34 155  70]\n",
      " [ 62 542  25  19  22  15  31  25  74 185]\n",
      " [100  19 341 111 144 107  91  42  27  18]\n",
      " [ 35  23 112 298 109 162 103  80  34  44]\n",
      " [ 40  17 147 103 377  67  97 118  14  20]\n",
      " [ 21  15 103 190  75 401  59  91  22  23]\n",
      " [ 19  15  94  87  79  60 588  24  10  24]\n",
      " [ 19  14  78  76 122 103  34 496   7  51]\n",
      " [174  76  38  22  22  21  25  18 520  84]\n",
      " [ 55 170  24  26  24  23  24  49  56 549]]\n",
      "Average Total Loss over Batches: 0.16718869573149145\n",
      "[[510  51  77  38  41  25  16  28 159  55]\n",
      " [ 70 530  30  27  22  11  30  23  89 168]\n",
      " [ 95  11 355 122 139 103  85  48  28  14]\n",
      " [ 38  23 128 319  96 167  91  77  28  33]\n",
      " [ 45  15 145 114 371  77  85 113  22  13]\n",
      " [ 20  12 108 208  68 393  60  87  28  16]\n",
      " [ 18  15 105 108  80  56 569  28   8  13]\n",
      " [ 31  17  84  74 117 109  32 491  10  35]\n",
      " [175  75  29  24  25  14  14  20 573  51]\n",
      " [ 66 159  24  32  29  29  25  48  78 510]]\n",
      "Average Total Loss over Batches: 0.16613951061656124\n",
      "[[488  57  66  40  50  20  12  31 164  72]\n",
      " [ 72 558  18  19  18   8  29  27  81 170]\n",
      " [ 94  14 321 137 154  80  77  68  31  24]\n",
      " [ 39  31  95 340  93 149  91  80  32  50]\n",
      " [ 48  20 129 111 389  54  86 128  18  17]\n",
      " [ 23  22  89 225  73 347  55 113  25  28]\n",
      " [ 18  16  98 121  84  50 541  33  14  25]\n",
      " [ 27  19  74  80 117  76  28 512   8  59]\n",
      " [170  88  26  19  26  14  15  20 526  96]\n",
      " [ 57 179  18  27  24  17  13  46  61 558]]\n",
      "Average Total Loss over Batches: 0.16750992415453045\n",
      "[[499  61  75  41  37  22  12  29 164  60]\n",
      " [ 80 553  21  24  16  14  28  21  86 157]\n",
      " [103  18 337 144 122 101  79  43  34  19]\n",
      " [ 35  24 110 362  84 150  82  76  34  43]\n",
      " [ 52  17 148 147 326  71  75 121  27  16]\n",
      " [ 22  13  99 229  58 383  46 101  27  22]\n",
      " [ 19  17  98 147  82  56 523  24  11  23]\n",
      " [ 29  16  88 101  93  91  25 504   9  44]\n",
      " [184  72  35  26  23  17  14  17 545  67]\n",
      " [ 63 174  26  34  23  24  16  49  64 527]]\n",
      "Average Total Loss over Batches: 0.17806707372676167\n",
      "[[440  57  68  50  44  29  16  38 182  76]\n",
      " [ 51 525  23  26  24  17  33  27  91 183]\n",
      " [ 70  10 312 157 148  98 108  53  29  15]\n",
      " [ 18  20 111 356  84 170  98  84  21  38]\n",
      " [ 32  12 136 139 342  73 110 123  16  17]\n",
      " [ 12  15  86 230  64 394  62  99  18  20]\n",
      " [ 14  12  97 130  69  57 575  25  10  11]\n",
      " [ 21  13  79 101 101 115  32 496   4  38]\n",
      " [152  80  28  31  22  17  19  19 542  90]\n",
      " [ 44 164  28  37  21  25  28  52  64 537]]\n",
      "Average Total Loss over Batches: 0.17118755759711213\n",
      "[[456  62  86  47  48  29  16  37 140  79]\n",
      " [ 61 539  25  32  21  18  30  30  70 174]\n",
      " [ 79  11 350 142 142 102  85  51  21  17]\n",
      " [ 23  19 116 376  79 166  94  77  16  34]\n",
      " [ 33  13 148 128 361  68  99 124  13  13]\n",
      " [  9  12 104 235  62 401  51  93  15  18]\n",
      " [ 11  15  93 138  80  62 551  28   5  17]\n",
      " [ 22  11  75 104  98 108  29 508   3  42]\n",
      " [174  75  46  43  27  24  16  25 497  73]\n",
      " [ 48 172  30  41  22  28  19  56  46 538]]\n",
      "Average Total Loss over Batches: 0.17732200001305623\n",
      "[[498  48  66  35  48  18   9  27 182  69]\n",
      " [ 76 541  18  16  22  14  20  24 101 168]\n",
      " [108  13 338 121 162  82  73  57  27  19]\n",
      " [ 43  26 121 302 107 157  78  82  38  46]\n",
      " [ 50  11 152 105 396  57  72 118  20  19]\n",
      " [ 23  15 110 203  82 373  49  88  31  26]\n",
      " [ 20  15 118 120 102  58 503  28  14  22]\n",
      " [ 26  12  88  74 125  90  24 504   7  50]\n",
      " [187  74  23  25  26  13  11  14 560  67]\n",
      " [ 61 169  25  23  23  15  14  49  83 538]]\n",
      "Average Total Loss over Batches: 0.16237381708264711\n",
      "[[533  48  57  36  27  20  19  27 172  61]\n",
      " [ 67 580  14  24  15  16  31  22  98 133]\n",
      " [103  18 317 141 106  98 106  60  37  14]\n",
      " [ 36  27  91 336  70 164 108  91  35  42]\n",
      " [ 53  15 108 145 312  71 120 129  31  16]\n",
      " [ 18  14  80 218  46 398  73 102  31  20]\n",
      " [ 23  14  75 116  42  53 609  29  15  24]\n",
      " [ 31  20  61  97  78 102  44 522   9  36]\n",
      " [183  76  23  26  12  16  16  16 576  56]\n",
      " [ 66 182  23  36  15  23  22  39  73 521]]\n",
      "Average Total Loss over Batches: 0.1791197479266928\n",
      "[[482  52  92  34  50  21  19  27 167  56]\n",
      " [ 69 511  36  26  28  16  39  24 103 148]\n",
      " [ 86  10 373 107 161  86  98  39  26  14]\n",
      " [ 24  17 150 292 127 148 108  63  36  35]\n",
      " [ 35   8 168  99 384  58 111 108  17  12]\n",
      " [ 14  10 119 198  87 375  64  88  28  17]\n",
      " [ 13  11 115  93  79  47 589  27   9  17]\n",
      " [ 27   7  99  82 144  98  33 468   6  36]\n",
      " [159  65  52  23  21  20  22  21 557  60]\n",
      " [ 57 150  37  33  31  28  26  54  74 510]]\n",
      "Average Total Loss over Batches: 0.16944954086591685\n",
      "[[508  58  75  44  37  25  13  25 154  61]\n",
      " [ 72 543  21  31  20  12  29  16  90 166]\n",
      " [104  14 349 131 132 107  87  40  23  13]\n",
      " [ 36  23 117 356  81 169  91  65  27  35]\n",
      " [ 41  13 149 141 346  77  96 104  19  14]\n",
      " [ 24  12  98 226  54 421  53  75  19  18]\n",
      " [ 21  13 101 122  58  72 563  23   9  18]\n",
      " [ 31  10  85 103 102 108  34 477   7  43]\n",
      " [181  78  41  33  18  19  14  18 545  53]\n",
      " [ 62 158  25  46  17  31  17  45  69 530]]\n",
      "Average Total Loss over Batches: 0.16984837322435717\n",
      "[[492  60  65  38  34  25  20  21 187  58]\n",
      " [ 64 555  18  16  17  16  30  23 105 156]\n",
      " [ 87  16 345 120 127 110  97  38  41  19]\n",
      " [ 37  23 102 300  87 189 106  84  35  37]\n",
      " [ 39  17 146 100 353  73 117 111  29  15]\n",
      " [ 17  14  94 191  59 426  64  87  28  20]\n",
      " [ 14  19 100  99  50  62 604  27  10  15]\n",
      " [ 31  12  73  85 106 106  39 498   7  43]\n",
      " [161  83  35  20  16  19  19  11 577  59]\n",
      " [ 61 172  23  36  17  25  26  35  78 527]]\n",
      "Average Total Loss over Batches: 0.17610655548313406\n",
      "[[514  61  77  25  28  15  19  23 177  61]\n",
      " [ 67 545  16  18  18  11  31  20 102 172]\n",
      " [ 99  15 359 110 136  83 111  42  26  19]\n",
      " [ 34  27 140 278 107 133 131  68  35  47]\n",
      " [ 45  15 165 100 350  52 117 112  21  23]\n",
      " [ 21  17 118 193  74 371  73  93  23  17]\n",
      " [ 17  15 103  86  70  42 615  21  12  19]\n",
      " [ 25  14  88  86 113  79  40 487  12  56]\n",
      " [158  83  26  20  24  16  19  15 587  52]\n",
      " [ 54 179  20  31  20  18  21  41  73 543]]\n",
      "Average Total Loss over Batches: 0.1690797260375339\n",
      "[[481  49  82  54  39  22  17  30 175  51]\n",
      " [ 71 505  24  33  26  16  40  24 112 149]\n",
      " [ 80  13 328 143 146  93 103  49  33  12]\n",
      " [ 28  13 100 367  89 164 111  69  33  26]\n",
      " [ 36   7 124 142 378  62 112 107  21  11]\n",
      " [ 16   8  97 226  66 405  62  78  30  12]\n",
      " [ 13  10  91 116  75  55 597  23  10  10]\n",
      " [ 29   7  71 121 123 108  46 459   7  29]\n",
      " [166  64  32  39  24  12  20  18 570  55]\n",
      " [ 63 141  31  50  31  30  27  53  74 500]]\n",
      "Average Total Loss over Batches: 0.17604555049524198\n",
      "[[460  66  79  45  36  26  19  38 168  63]\n",
      " [ 58 560  22  24  21  15  34  25  83 158]\n",
      " [ 80  14 337 126 139 102 108  51  27  16]\n",
      " [ 26  24 110 341  89 159 107  73  27  44]\n",
      " [ 29  18 133 116 366  70 119 117  15  17]\n",
      " [ 13  15  95 202  70 408  65  90  20  22]\n",
      " [ 14  12  92 114  64  62 584  27  10  21]\n",
      " [ 24  12  70  92 111 106  32 503   4  46]\n",
      " [148  98  38  24  25  22  23  14 535  73]\n",
      " [ 43 180  22  36  20  33  27  51  55 533]]\n",
      "Average Total Loss over Batches: 0.16705022919298415\n",
      "[[483  62  67  43  34  23  12  34 183  59]\n",
      " [ 65 551  19  27  18  14  27  31  94 154]\n",
      " [ 95  17 349 141 127 110  65  51  30  15]\n",
      " [ 34  26 112 355  82 161  70  84  33  43]\n",
      " [ 39  15 136 148 339  81  72 124  29  17]\n",
      " [ 21  17  92 212  54 406  52 100  30  16]\n",
      " [ 22  16 106 143  71  69 501  39  15  18]\n",
      " [ 29  19  70  92 101 112  23 499   8  47]\n",
      " [176  91  29  22  19  17   9  27 544  66]\n",
      " [ 49 184  20  39  20  25  15  52  71 525]]\n",
      "Average Total Loss over Batches: 0.16743319841452126\n",
      "[[522  59  75  44  35  17  17  32 153  46]\n",
      " [ 69 562  27  23  21  11  32  24  89 142]\n",
      " [ 85  10 362 126 136  88 100  51  25  17]\n",
      " [ 35  21 117 374  82 128  97  85  25  36]\n",
      " [ 38  11 145 125 353  53 112 132  16  15]\n",
      " [ 16  16 110 233  60 364  65  93  23  20]\n",
      " [ 19  13  91 108  63  53 605  24  13  11]\n",
      " [ 34  12  88  94  90  89  31 511   6  45]\n",
      " [189  92  37  26  18  12  18  17 521  70]\n",
      " [ 57 171  31  38  25  18  24  52  59 525]]\n",
      "Average Total Loss over Batches: 0.17976057539292006\n",
      "[[469  61  87  41  48  21  15  24 173  61]\n",
      " [ 61 569  31  31  17  14  26  22  84 145]\n",
      " [ 80  11 371 128 133  92  91  45  35  14]\n",
      " [ 26  22 116 365  92 159  82  70  37  31]\n",
      " [ 38   8 174 126 361  59  94 115  15  10]\n",
      " [ 16  14 116 213  76 390  48  84  27  16]\n",
      " [ 14  14 116 129  65  61 551  26  10  14]\n",
      " [ 25  13 100  84 120  99  32 485   5  37]\n",
      " [154  88  45  32  23  14  18  13 549  64]\n",
      " [ 60 181  27  38  23  27  19  45  74 506]]\n",
      "Average Total Loss over Batches: 0.16545792108405882\n",
      "[[483  49  78  35  41  24  13  28 184  65]\n",
      " [ 68 505  32  28  20  14  32  28 105 168]\n",
      " [ 92  13 341 134 141  94  88  51  32  14]\n",
      " [ 26  15 118 351  93 148 105  78  35  31]\n",
      " [ 39   9 139 130 365  61  96 124  23  14]\n",
      " [ 15  12 101 218  75 381  53  96  32  17]\n",
      " [ 15  12 102 120  75  60 561  27  11  17]\n",
      " [ 24   9  78  94  98 105  37 505   9  41]\n",
      " [177  69  37  28  23  17  22  19 547  61]\n",
      " [ 62 146  26  43  27  29  23  57  78 509]]\n",
      "Average Total Loss over Batches: 0.17309389947282428\n",
      "[[490  46  76  55  48  33  15  46 135  56]\n",
      " [ 75 503  24  32  31  22  40  32  68 173]\n",
      " [ 86  10 326 131 157 108  96  52  19  15]\n",
      " [ 26  14 103 339 101 194  99  76  18  30]\n",
      " [ 32   7 123 117 393  71 105 125  14  13]\n",
      " [ 11  11  79 213  65 432  57 109  11  12]\n",
      " [ 15  10  94 114  83  64 582  26   5   7]\n",
      " [ 24   7  67  93 101 114  33 528   3  30]\n",
      " [196  75  42  35  35  28  31  18 462  78]\n",
      " [ 61 137  27  46  33  47  25  66  48 510]]\n",
      "Average Total Loss over Batches: 0.18018713151141777\n",
      "[[510  54  66  41  40  21  15  32 162  59]\n",
      " [ 67 532  20  26  23  12  33  29  92 166]\n",
      " [101  16 328 138 152  77  93  54  26  15]\n",
      " [ 33  21 113 335 123 120 107  76  36  36]\n",
      " [ 46  12 138 111 387  48 111 118  16  13]\n",
      " [ 17  17 104 224  81 349  55 102  29  22]\n",
      " [ 20  15  94 109  82  46 574  31  12  17]\n",
      " [ 33   9  76  90 127  89  34 492   5  45]\n",
      " [196  77  29  29  25  16  18  17 525  68]\n",
      " [ 62 161  21  31  33  18  23  56  68 527]]\n",
      "Average Total Loss over Batches: 0.1718252884490762\n",
      "[[493  52  71  43  32  28  19  32 158  72]\n",
      " [ 55 533  27  23  15  17  36  23  99 172]\n",
      " [ 83   8 341 116 140 126  94  46  29  17]\n",
      " [ 34  13 114 325  82 211  94  74  22  31]\n",
      " [ 33   9 148 116 336  94 112 116  20  16]\n",
      " [ 14  11  99 193  59 448  53  84  22  17]\n",
      " [ 16   9  88  99  68  74 604  24   8  10]\n",
      " [ 32  11  85  98  93 126  33 483   8  31]\n",
      " [164  81  40  27  18  19  20  22 545  64]\n",
      " [ 59 152  26  31  21  37  26  48  65 535]]\n",
      "Average Total Loss over Batches: 0.16901392130782697\n",
      "[[514  58  69  39  40  26  16  35 137  66]\n",
      " [ 60 549  18  28  20  13  32  25  77 178]\n",
      " [ 95  16 293 147 158 100  98  52  22  19]\n",
      " [ 37  28  97 346  96 158  99  75  23  41]\n",
      " [ 44  16 113 126 357  78 101 124  20  21]\n",
      " [ 16  15  78 223  70 416  54  81  22  25]\n",
      " [ 17  16  70 120  70  70 576  28  12  21]\n",
      " [ 33  15  67  91 114  99  32 489   5  55]\n",
      " [175  94  30  32  25  22  20  19 509  74]\n",
      " [ 50 152  19  34  21  31  17  53  57 566]]\n",
      "Average Total Loss over Batches: 0.17978878473836143\n",
      "[[516  64  58  30  38  18  19  26 168  63]\n",
      " [ 62 565  17  19  19  11  22  24  84 177]\n",
      " [105  15 310 104 157 101  94  47  37  30]\n",
      " [ 40  27 100 283 116 158  90  86  39  61]\n",
      " [ 46  17 121  86 400  67  94 117  24  28]\n",
      " [ 18  21  80 189  96 401  56  87  21  31]\n",
      " [ 17  16  85 103  89  55 555  32  15  33]\n",
      " [ 34  21  70  73 128  85  30 480  10  69]\n",
      " [168  90  23  14  23  19  14  14 563  72]\n",
      " [ 48 177  17  24  19  15  17  45  72 566]]\n",
      "Average Total Loss over Batches: 0.17389506235993732\n",
      "[[489  54  75  42  45  28  13  27 163  64]\n",
      " [ 61 540  25  32  18  17  26  25  84 172]\n",
      " [ 91  14 304 161 163 118  61  43  31  14]\n",
      " [ 30  17  94 389  95 173  63  70  27  42]\n",
      " [ 41  11 117 149 371  86  68 107  29  21]\n",
      " [ 14  14  84 243  68 403  43  83  31  17]\n",
      " [ 20  13  92 160  84  79 488  27  16  21]\n",
      " [ 28  14  67 105 113 117  25 471   6  54]\n",
      " [168  70  32  35  17  19  13  21 560  65]\n",
      " [ 53 156  25  39  28  27  18  48  71 535]]\n",
      "Average Total Loss over Batches: 0.1730462263827635\n",
      "[[536  72  47  27  31  17  15  24 164  67]\n",
      " [ 49 642  11  13  13   7  17  19  84 145]\n",
      " [111  24 328 114 127  97  94  50  29  26]\n",
      " [ 43  44 105 289  89 149  96  86  42  57]\n",
      " [ 51  30 129 114 338  62  96 125  29  26]\n",
      " [ 31  22  93 188  65 390  61  93  30  27]\n",
      " [ 19  30  83 101  68  47 582  29  13  28]\n",
      " [ 37  31  66  74  98  78  33 498  13  72]\n",
      " [174 104  13  13  17  13  14  11 584  57]\n",
      " [ 55 213  14  19  13  16  16  32  72 550]]\n",
      "Average Total Loss over Batches: 0.17942778236767362\n",
      "[[498  56  72  37  44  20  14  28 183  48]\n",
      " [ 69 519  26  22  23  13  29  27 119 153]\n",
      " [ 90  13 349 112 156 101  83  48  31  17]\n",
      " [ 38  17 126 320 107 158  87  69  41  37]\n",
      " [ 39  15 142 108 376  62  98 119  25  16]\n",
      " [ 16  17 106 201  69 404  55  82  32  18]\n",
      " [ 14  10 101 119  81  62 549  27  19  18]\n",
      " [ 30  15  88  85 115 103  33 477   8  46]\n",
      " [166  71  23  20  28  19  16  16 578  63]\n",
      " [ 61 166  24  31  17  18  20  51  86 526]]\n",
      "Average Total Loss over Batches: 0.18016375567102613\n",
      "[[510  68  70  28  43  24  17  25 164  51]\n",
      " [ 65 570  25  17  20  18  30  25  94 136]\n",
      " [ 98  19 324 124 125 134  83  45  32  16]\n",
      " [ 39  22 112 308  85 190  91  90  28  35]\n",
      " [ 46  14 140 111 337  97 104 116  22  13]\n",
      " [ 19  17  95 181  59 450  61  77  23  18]\n",
      " [ 17  16  81 109  63  79 574  28  14  19]\n",
      " [ 35  15  84  81  91 120  37 494   8  35]\n",
      " [178  92  31  18  16  18  19  17 548  63]\n",
      " [ 62 183  23  34  22  33  21  49  87 486]]\n",
      "Average Total Loss over Batches: 0.17506904074155558\n",
      "[[504  60  68  41  44  24  13  26 164  56]\n",
      " [ 76 524  25  26  21  17  27  26 104 154]\n",
      " [ 96  13 336 138 144 119  56  42  40  16]\n",
      " [ 32  14 121 334 105 182  64  75  39  34]\n",
      " [ 41   9 147 126 375  87  66 105  33  11]\n",
      " [ 18  10 103 208  63 433  37  77  32  19]\n",
      " [ 19  13 104 143  96  86 468  30  17  24]\n",
      " [ 31   7  82  92 113 121  23 480   9  42]\n",
      " [173  65  31  28  20  25  13  15 577  53]\n",
      " [ 59 154  29  38  28  25  14  48  94 511]]\n",
      "Average Total Loss over Batches: 0.17269953207903163\n",
      "[[518  82  72  32  32  14  19  31 142  58]\n",
      " [ 56 609  20  16  16  15  29  20  72 147]\n",
      " [ 97  18 373 100 121 111  97  43  22  18]\n",
      " [ 33  31 134 293  75 169 113  85  24  43]\n",
      " [ 41  18 161 100 345  70 109 116  20  20]\n",
      " [ 16  22 119 173  59 413  70  91  16  21]\n",
      " [ 14  23  92  91  62  62 600  25  12  19]\n",
      " [ 33  17  84  73  90 107  33 506   5  52]\n",
      " [182 109  34  17  17  17  20  16 519  69]\n",
      " [ 51 214  30  26  17  22  24  40  53 523]]\n",
      "Average Total Loss over Batches: 0.18536039528238368\n",
      "[[497  68  67  35  45  36  12  25 152  63]\n",
      " [ 58 577  19  28  11  18  22  20  79 168]\n",
      " [ 84  18 295 153 127 167  60  41  33  22]\n",
      " [ 30  28  95 335  87 227  56  78  26  38]\n",
      " [ 45  17 122 147 329 114  68 112  24  22]\n",
      " [ 17  13  73 191  61 497  38  66  24  20]\n",
      " [ 18  24  77 149  75 115 467  32  17  26]\n",
      " [ 28  14  66  90  98 145  19 470   9  61]\n",
      " [163  87  33  22  23  28   8  16 553  67]\n",
      " [ 56 169  22  33  21  34  11  39  70 545]]\n",
      "Average Total Loss over Batches: 0.18088175191816258\n",
      "[[478  52  84  39  58  22  18  30 160  59]\n",
      " [ 58 524  33  28  22  16  36  28  90 165]\n",
      " [ 83  14 346 121 154 109  82  46  25  20]\n",
      " [ 25  21 127 321 101 168  96  75  29  37]\n",
      " [ 36   9 137 121 374  69 105 116  17  16]\n",
      " [ 10  13 110 201  73 405  62  85  25  16]\n",
      " [ 14  11 102 107  70  64 584  24  10  14]\n",
      " [ 25  11  94  81 111  99  36 494   5  44]\n",
      " [156  78  32  23  30  23  20  17 556  65]\n",
      " [ 53 151  24  40  26  23  26  55  71 531]]\n",
      "Average Total Loss over Batches: 0.19467958427248042\n",
      "[[487  69  67  34  44  20  11  28 172  68]\n",
      " [ 64 572  16  20  17  12  24  21  93 161]\n",
      " [ 91  14 325 145 149  92  72  53  43  16]\n",
      " [ 35  23 108 347  97 155  70  89  39  37]\n",
      " [ 44  14 127 133 372  57  73 140  28  12]\n",
      " [ 19  14  90 224  72 389  48  96  29  19]\n",
      " [ 18  17 107 140  95  50 499  34  20  20]\n",
      " [ 28  16  70  90 120  83  22 521   7  43]\n",
      " [161  86  28  16  20  20  11  15 581  62]\n",
      " [ 54 173  22  37  22  21  20  45  81 525]]\n",
      "Average Total Loss over Batches: 0.1637009830508847\n",
      "[[476  57  79  43  53  24  16  40 140  72]\n",
      " [ 63 524  28  25  27  12  36  27  81 177]\n",
      " [ 87  12 343 116 154  89  97  59  27  16]\n",
      " [ 29  17 115 315 107 155 107  85  24  46]\n",
      " [ 37  12 129 109 391  57 109 117  17  22]\n",
      " [ 15  14  96 207  78 392  53  98  24  23]\n",
      " [ 16  14  97  93  96  47 576  27  12  22]\n",
      " [ 20  15  82  77 123  95  32 505   7  44]\n",
      " [169  84  40  27  28  21  17  21 517  76]\n",
      " [ 54 145  27  28  27  29  22  49  54 565]]\n",
      "Average Total Loss over Batches: 0.16604497461553605\n",
      "[[497  69  69  33  45  17  17  27 155  71]\n",
      " [ 57 582  16  20  15  13  24  23  66 184]\n",
      " [105  15 336 144 139  85  81  47  27  21]\n",
      " [ 29  32 105 353  85 142  88  80  29  57]\n",
      " [ 40  17 139 131 375  58  87 109  21  23]\n",
      " [ 14  15  87 238  73 380  57  86  24  26]\n",
      " [ 18  18  94 121  76  51 557  22  16  27]\n",
      " [ 27  19  74  87 120  88  21 496   6  62]\n",
      " [161  91  31  24  21  16  18  14 551  73]\n",
      " [ 43 172  16  25  18  16  18  42  71 579]]\n",
      "Average Total Loss over Batches: 0.16990921375456514\n",
      "[[506  84  66  38  35  24  14  32 127  74]\n",
      " [ 59 617  22  18  17  11  22  24  67 143]\n",
      " [102  23 349 146 122  91  79  47  23  18]\n",
      " [ 35  35 117 357  80 133  98  76  27  42]\n",
      " [ 43  23 151 138 329  69  86 124  18  19]\n",
      " [ 19  22 103 235  60 371  53  92  24  21]\n",
      " [ 20  23  97 133  72  51 545  25  11  23]\n",
      " [ 30  19  83 100  93  89  27 498   7  54]\n",
      " [178 117  39  26  17  17  24  19 488  75]\n",
      " [ 49 206  24  40  17  14  19  51  47 533]]\n",
      "Average Total Loss over Batches: 0.17276650681051178\n",
      "[[489  61  77  50  51  27  16  37 129  63]\n",
      " [ 62 551  26  30  20  21  32  27  66 165]\n",
      " [ 82   9 339 140 144 107  95  50  17  17]\n",
      " [ 27  16 110 355  89 176  93  82  18  34]\n",
      " [ 32  11 137 121 365  71 112 126  12  13]\n",
      " [ 12  13 103 224  61 413  53  87  15  19]\n",
      " [ 16   9  86 129  68  62 581  28   6  15]\n",
      " [ 18   8  84  91 104  99  30 525   4  37]\n",
      " [182  85  33  39  28  24  19  17 494  79]\n",
      " [ 53 164  24  49  25  28  25  47  53 532]]\n",
      "Average Total Loss over Batches: 0.16894770857108243\n",
      "[[501  53  55  31  40  23  10  25 197  65]\n",
      " [ 72 527  18  20  19  14  21  27 112 170]\n",
      " [109  12 307 125 142 116  65  61  41  22]\n",
      " [ 34  20 102 333  97 173  72  90  36  43]\n",
      " [ 42  13 126 121 363  85  75 121  30  24]\n",
      " [ 17  11  88 189  77 419  40 104  35  20]\n",
      " [ 20  14  96 147 102  67 477  41  16  20]\n",
      " [ 30   9  67  82 103 108  20 515  10  56]\n",
      " [152  77  24  21  20  20  14  19 599  54]\n",
      " [ 58 156  18  29  25  24  10  50  86 544]]\n",
      "Average Total Loss over Batches: 0.1684858246659826\n",
      "[[475  47  74  39  45  30  12  31 180  67]\n",
      " [ 75 514  27  24  22  16  30  29 103 160]\n",
      " [ 92  13 337 125 129 121  81  55  32  15]\n",
      " [ 32  20 115 317  88 214  86  71  28  29]\n",
      " [ 41  17 134 118 357  92  94 111  24  12]\n",
      " [ 15   9  93 192  55 447  51  96  27  15]\n",
      " [ 19  13  95 119  69  80 555  28   8  14]\n",
      " [ 27  11  74  85  99 132  33 498   7  34]\n",
      " [172  80  32  28  29  25  17  21 522  74]\n",
      " [ 64 144  31  39  26  43  21  53  70 509]]\n",
      "Average Total Loss over Batches: 0.19130200334358832\n",
      "[[487  55  63  41  40  13  13  30 190  68]\n",
      " [ 69 537  25  19  18   9  21  21  96 185]\n",
      " [ 92  12 358 132 134  74  72  67  32  27]\n",
      " [ 34  23 130 364  85 124  78  85  28  49]\n",
      " [ 46  12 153 127 360  46  83 132  19  22]\n",
      " [ 19  15 113 237  70 350  44 102  30  20]\n",
      " [ 15  17 109 138  85  54 513  31  15  23]\n",
      " [ 28  11  77  96 102  78  27 528   9  44]\n",
      " [170  82  33  27  12  11  16  19 557  73]\n",
      " [ 55 160  25  33  14  15  13  47  85 553]]\n",
      "Average Total Loss over Batches: 0.1760595463710268\n",
      "[[478  70  77  41  36  25  14  28 170  61]\n",
      " [ 57 563  32  26  18  14  22  20 102 146]\n",
      " [ 85  14 355 107 130 125  92  47  30  15]\n",
      " [ 24  23 128 312  89 185 100  70  35  34]\n",
      " [ 39  10 154 120 351  78 102 102  25  19]\n",
      " [ 17  14 111 198  56 439  62  72  16  15]\n",
      " [ 15  14 102 101  71  77 564  26  14  16]\n",
      " [ 28  15  96  87 102 120  37 466   7  42]\n",
      " [173  95  38  22  16  17  18  16 539  66]\n",
      " [ 56 191  33  33  20  25  27  46  71 498]]\n",
      "Average Total Loss over Batches: 0.17719584581600437\n",
      "[[474  55  83  47  43  28  13  33 172  52]\n",
      " [ 72 514  33  33  20  14  32  28 106 148]\n",
      " [ 78   9 342 137 151 113  79  54  26  11]\n",
      " [ 25  16 116 381  97 160  83  79  21  22]\n",
      " [ 31   9 150 135 374  66  86 119  17  13]\n",
      " [ 14  10 109 235  70 387  47  96  22  10]\n",
      " [ 16   9 114 133  82  65 534  28  11   8]\n",
      " [ 23  12  91  94 103 100  25 523   7  22]\n",
      " [155  81  43  36  26  20  18  17 540  64]\n",
      " [ 62 150  38  46  35  33  21  51  79 485]]\n",
      "Average Total Loss over Batches: 0.15862296662027772\n",
      "[[538  62  69  26  28  15  16  22 170  54]\n",
      " [ 71 593  20  19   9  12  26  18  99 133]\n",
      " [111  18 360 115 109 105  87  39  33  23]\n",
      " [ 49  31 134 319  63 150  88  79  40  47]\n",
      " [ 65  19 167 110 309  71  98 112  32  17]\n",
      " [ 32  16 112 204  55 385  66  78  34  18]\n",
      " [ 22  22  91 119  52  67 558  29  19  21]\n",
      " [ 44  17  91  81  75  96  35 499  10  52]\n",
      " [175  89  32  13  19  18  14  12 575  53]\n",
      " [ 63 204  27  33  12  19  18  41  90 493]]\n",
      "Average Total Loss over Batches: 0.19658942334721927\n",
      "[[505  61  75  35  46  21  16  32 156  53]\n",
      " [ 68 562  23  22  16  12  31  25  77 164]\n",
      " [103  16 353 125 137 106  82  40  22  16]\n",
      " [ 32  25 121 329  95 174  90  71  23  40]\n",
      " [ 44  14 141 114 364  76  96 115  23  13]\n",
      " [ 18  11 106 205  67 415  56  82  24  16]\n",
      " [ 18  19  95 112  73  73 555  29   9  17]\n",
      " [ 35  12  86  97 114 116  33 465   4  38]\n",
      " [159  84  37  22  26  19  23  25 543  62]\n",
      " [ 53 165  28  36  26  22  18  56  61 535]]\n",
      "Average Total Loss over Batches: 0.16990437751182727\n",
      "[[536  48  53  35  42  19  17  29 156  65]\n",
      " [ 70 525  27  27  20  10  27  24  95 175]\n",
      " [106  13 294 153 156  89  94  54  19  22]\n",
      " [ 41  17  98 341 105 154  98  70  35  41]\n",
      " [ 44   9 115 136 394  55 102 107  25  13]\n",
      " [ 24  15  82 242  84 371  57  86  19  20]\n",
      " [ 19  13  81 130  79  54 561  26  13  24]\n",
      " [ 32  13  61  98 117  88  34 500  11  46]\n",
      " [206  72  18  28  25  16  18  12 543  62]\n",
      " [ 65 152  16  35  27  18  22  39  78 548]]\n",
      "Average Total Loss over Batches: 0.17896864101715168\n",
      "[[501  78  69  36  38  25  16  27 154  56]\n",
      " [ 62 558  28  27  20  14  30  26  90 145]\n",
      " [ 93  12 328 120 146 115  95  48  27  16]\n",
      " [ 39  20 118 334  96 166  95  75  23  34]\n",
      " [ 43  16 136 123 360  71  96 119  22  14]\n",
      " [ 22  18  99 200  77 412  56  80  21  15]\n",
      " [ 21  16  95 110  76  64 568  26   9  15]\n",
      " [ 32  13  80  90 118 113  37 476   6  35]\n",
      " [176  96  42  28  20  19  19  19 517  64]\n",
      " [ 59 186  31  36  26  30  21  51  60 500]]\n",
      "Average Total Loss over Batches: 0.18830884516753563\n",
      "[[532  58  89  29  37  19  17  19 152  48]\n",
      " [ 66 563  30  25  18  15  27  24 103 129]\n",
      " [101  15 386 113 114 109  77  44  21  20]\n",
      " [ 40  22 161 286  82 175  91  69  32  42]\n",
      " [ 44  14 184 109 331  92  90  96  27  13]\n",
      " [ 19   9 119 187  60 436  54  78  24  14]\n",
      " [ 25  10 106 106  69  69 556  23  17  19]\n",
      " [ 38  16  94  79  92 127  33 473  12  36]\n",
      " [175  89  35  24  17  21  16  14 566  43]\n",
      " [ 67 191  33  30  19  18  18  43  87 494]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "for epochs in range(100):\n",
    "  total_loss = 0\n",
    "  for batch in range( train_X.shape[0] // batch_size ):\n",
    "    x_batch, y_batch = get_batch(train_X, train_Y, batch_size)\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "    logits = cifar_model( x_batch )\n",
    "    loss = loss_function( logits, y_batch )\n",
    "\n",
    "    loss.backward()\n",
    "    cnn_optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  print( \"Average Total Loss over Batches:\", total_loss / ( train_X.shape[0] // batch_size ) )\n",
    "  print( confusion_matrix( cifar_model, test_X, test_Y ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlMvskNtGWDq"
   },
   "source": [
    "Similar results - but much faster."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
