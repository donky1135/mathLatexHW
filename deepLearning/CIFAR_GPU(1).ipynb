{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YUH3WeLBcJh"
   },
   "source": [
    "In the upper right corner, select from the dropdown menu 'Change Runtime Type', and select a GPU as available - this will allow you to run your neural network training on accelerated hardware and run everything faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fKUVG_yVxys",
    "outputId": "474ecf7c-9001-48bf-fd85-4b79b5760ef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOkdVdGsCPNg"
   },
   "source": [
    "This indicates that a GPU has been detected and can be used - the device is saved to 'device' so that we can direct data and models to access memory on that device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnRW_4dyWG12",
    "outputId": "7f23810c-9631-4478-8723-09b5d82cead3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "train_X = torch.Tensor( trainset.data/255.0 - 0.5 )\n",
    "train_X = train_X.permute( 0, 3, 1, 2 )\n",
    "\n",
    "train_X = train_X.to( device ) # This line is different from the previous CIFAR code - it transfers the tensor to the GPU memory\n",
    "\n",
    "test_X = torch.Tensor( testset.data/255.0 - 0.5 )\n",
    "test_X = test_X.permute( 0, 3, 1, 2 )\n",
    "\n",
    "test_X = test_X.to( device ) # Again, transfering the tensor to GPU memory.\n",
    "\n",
    "train_Y = torch.Tensor( np.asarray( trainset.targets ) ).long()\n",
    "train_Y = train_Y.to( device )\n",
    "test_Y = torch.Tensor( np.asarray( testset.targets ) ).long()\n",
    "test_Y = test_Y.to( device )\n",
    "\n",
    "# All the data needs to be loaded into the GPU, as that is where the model processing will occur.\n",
    "\n",
    "def get_batch(x, y, batch_size):\n",
    "  n = x.shape[0]\n",
    "\n",
    "  batch_indices = random.sample( [ i for i in range(n) ], k = batch_size )\n",
    "\n",
    "  x_batch = x[ batch_indices ]\n",
    "  y_batch = y[ batch_indices ]\n",
    "\n",
    "  return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "stRuThG-WK5a"
   },
   "outputs": [],
   "source": [
    "class CIFARModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CIFARModel, self).__init__()\n",
    "\n",
    "    self.conv_layer_1 = nn.Conv2d(in_channels = 3, out_channels = 5, kernel_size = 3, stride = 1, bias=True)\n",
    "    self.conv_layer_2 = nn.Conv2d(in_channels = 5, out_channels = 10, kernel_size = 3, stride = 1, bias=True)\n",
    "    self.conv_layer_3 = nn.Conv2d(in_channels = 10, out_channels = 15, kernel_size = 3, stride = 1, bias=True)\n",
    "\n",
    "    self.linear_layer = torch.nn.Linear( in_features = 15*26*26, out_features = 10, bias=True )\n",
    "    # Note that the output of the last convolutional layer will be 15x26x16 - why?\n",
    "    # So we want to input 15*26*26 values into the last layer, and get 10 output values out (for the class probabilities)\n",
    "\n",
    "  def forward(self, input_tensor):\n",
    "    output = self.conv_layer_1( input_tensor )\n",
    "    output = nn.Sigmoid()( output )\n",
    "    output = self.conv_layer_2( output )\n",
    "    output = nn.Sigmoid()( output )\n",
    "    output = self.conv_layer_3( output )\n",
    "    output = nn.Sigmoid()( output )\n",
    "\n",
    "    # At this point, the block of node values from the convolutional layer is flattened\n",
    "    # So that it can be passed into a standard linear layer\n",
    "    output = nn.Flatten()( output )\n",
    "    output = self.linear_layer( output )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WHQ91OrnWYE0"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix( model, x, y ):\n",
    "  identification_counts = np.zeros( shape = (10,10), dtype = np.int32 )\n",
    "\n",
    "  logits = model( x )\n",
    "  predicted_classes = torch.argmax( logits, dim = 1 )\n",
    "\n",
    "  n = x.shape[0]\n",
    "\n",
    "  for i in range(n):\n",
    "    actual_class = int( y[i].item() )\n",
    "    predicted_class = predicted_classes[i].item()\n",
    "    identification_counts[actual_class, predicted_class] += 1\n",
    "\n",
    "  return identification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pHjBz3GWihM",
    "outputId": "6d5d6a04-6835-4317-ce08-b52ecc2969ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFARModel(\n",
      "  (conv_layer_1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv_layer_2): Conv2d(5, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv_layer_3): Conv2d(10, 15, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (linear_layer): Linear(in_features=10140, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0, 1000,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar_model = CIFARModel()\n",
    "\n",
    "cifar_model.to( device ) # The only change is that we also send the model to the GPU\n",
    "\n",
    "print( cifar_model )\n",
    "confusion_matrix( cifar_model, test_X, test_Y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHMoVFMXC5vG"
   },
   "source": [
    "At this point, everything else runs as before - just faster. I also increased the bach size, which I might could have done previously. Nevertheless - faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlptfBlvWlxk",
    "outputId": "bebcd9b3-8996-40a1-fd8e-593fd0dffa8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Test Loss: 2.353330135345459\n"
     ]
    }
   ],
   "source": [
    "cnn_optimizer = optim.Adam(cifar_model.parameters(), lr = 0.01 )\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "print(\"Initial Test Loss:\", loss_function( cifar_model( test_X ), test_Y ).item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uaShVxtDWq4W",
    "outputId": "b01eb0eb-b1f0-44f5-fbc9-1022347ba5fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Total Loss over Batches: 2.331151395301819\n",
      "[[246  38  31  30  12  76  71  32 378  86]\n",
      " [ 25 267  10  22  44 147 173  46 175  91]\n",
      " [ 31  39  91  34  33  95 513  31  85  48]\n",
      " [ 23  23  62  98  57 164 437  53  32  51]\n",
      " [ 16   5  35  35  44  55 681  40  55  34]\n",
      " [ 20  39  47  61  66 264 383  52  41  27]\n",
      " [  8  14  31  52  33  27 772  34   7  22]\n",
      " [ 15  75  24  38  46  89 383 132  49 149]\n",
      " [ 87  42  17  29  11  96  48  29 483 158]\n",
      " [ 20  85  12  35  26  56 155  73 196 342]]\n",
      "Average Total Loss over Batches: 1.9741747277832031\n",
      "[[276  55  71  87  28  17  99  62  90 215]\n",
      " [  5 307  12  50  75  38 264  51  21 177]\n",
      " [ 25  25 114  60  50  27 602  46  11  40]\n",
      " [  5  21  22 107  41  57 650  54   8  35]\n",
      " [ 12   8  29  38  38  15 764  42   9  45]\n",
      " [  6  54  27 110  69  74 597  48   0  15]\n",
      " [  1   4   9  27  10  16 880  35   1  17]\n",
      " [  4  29  22  42  57  19 517 242   3  65]\n",
      " [120  71  35  74  27  37  70  55 176 335]\n",
      " [  4  93  13  36  50  14 262 143  20 365]]\n",
      "Average Total Loss over Batches: 1.8247202014923096\n",
      "[[346  73  25   5  15  16  13  17 438  52]\n",
      " [ 22 599   3   4   3  25   9  11 193 131]\n",
      " [ 63  50 174  43 117  96 230  77 112  38]\n",
      " [ 30  56  58 112  73 210 234  60  78  89]\n",
      " [ 44  30  85  45 221  88 269 112  74  32]\n",
      " [ 31  53  80  75  70 285 220  89  54  43]\n",
      " [ 11  42  59  44  68  91 523  81  24  57]\n",
      " [ 38  49  45  46  71 115  89 394  47 106]\n",
      " [ 86  66  10   6   2  11   7  12 729  71]\n",
      " [ 22 212  10   9   5  23  20  27 244 428]]\n",
      "Average Total Loss over Batches: 1.5699449006652832\n",
      "[[448  35  72  24  15  14  12  52 234  94]\n",
      " [ 15 618  10  14   1  17  11  29  77 208]\n",
      " [ 64  16 295  63 192  91  91 117  33  38]\n",
      " [ 22  23  88 209  96 210 146 131  11  64]\n",
      " [ 35   5 156  60 382  76  82 168  18  18]\n",
      " [ 17  14 138 114  96 339  84 161   8  29]\n",
      " [  2  12  92  94 107  60 478 104   2  49]\n",
      " [ 12  15  42  57  84  95  19 617  11  48]\n",
      " [100  70  31  12   6  10   5  31 643  92]\n",
      " [ 24 130  21  34   8  26  16  56  62 623]]\n",
      "Average Total Loss over Batches: 1.3740450782012938\n",
      "[[627  34  83  22  18  21  11  32 108  44]\n",
      " [ 69 678   7  19   4  18  20  14  64 107]\n",
      " [ 88  13 352  64 163 133  76  75  20  16]\n",
      " [ 21  14  86 290  91 268  96  74  11  49]\n",
      " [ 56   4 153  57 416 102  87 109  10   6]\n",
      " [ 13   8  84 121  81 495  65 106  11  16]\n",
      " [  7  13  55  86 111  68 596  40   5  19]\n",
      " [ 25   7  63  65  73 124  27 576   9  31]\n",
      " [243  80  29  14   8  21   8  17 540  40]\n",
      " [ 86 134  19  50  13  20  30  46  36 566]]\n",
      "Average Total Loss over Batches: 1.256501373023987\n",
      "[[600  35  50  46  19   5  11  24 147  63]\n",
      " [ 42 654   5  34   3   5  17  17  48 175]\n",
      " [102   9 312 198  98  77  71  82  29  22]\n",
      " [ 27  17  48 566  40  85  68  75  14  60]\n",
      " [ 51   7 118 180 294  58 110 149  20  13]\n",
      " [ 17   9  58 316  52 349  47 122   6  24]\n",
      " [  6  20  34 177  43  36 587  55   9  33]\n",
      " [ 23   4  30 143  38  64  24 619   5  50]\n",
      " [146  66  11  37  10   2  11  10 650  57]\n",
      " [ 45 112   6  77   4   7  25  34  43 647]]\n",
      "Average Total Loss over Batches: 1.1409088867282868\n",
      "[[535  34  86   8  51   6  12  23 214  31]\n",
      " [ 31 680  11   8  20   2  13  12 101 122]\n",
      " [ 81   9 399  28 243  68  72  57  31  12]\n",
      " [ 30  20 133 219 170 145 107  91  41  44]\n",
      " [ 39   6 118  24 591  29  84  69  33   7]\n",
      " [ 12   8 132 102 143 370  80 106  32  15]\n",
      " [  6  16  62  38 165  34 598  40  14  27]\n",
      " [ 20   7  58  36 167  55  35 581  10  31]\n",
      " [100  53  14   5  13   3   7  13 763  29]\n",
      " [ 67 140  17  13  24   7  22  44  73 593]]\n",
      "Average Total Loss over Batches: 1.0600274228668214\n",
      "[[555  22  84  29  32   7  11  20 213  27]\n",
      " [ 40 667  10  29  13   1  22  12 122  84]\n",
      " [ 87   6 355  87 227  74  73  46  40   5]\n",
      " [ 24  16  97 426 115 130  78  63  32  19]\n",
      " [ 35   4  90  96 535  37 105  70  24   4]\n",
      " [ 15   4 114 237  82 372  70  86  16   4]\n",
      " [  4  15  49 103 112  33 624  34  14  12]\n",
      " [ 28   5  66 102 131  65  26 553   9  15]\n",
      " [ 99  44  15  21  13   5   5  13 767  18]\n",
      " [ 73 132  16  57  18   9  31  42  99 523]]\n",
      "Average Total Loss over Batches: 0.9961368780612946\n",
      "[[565  26  56  10  28  10  22  31 153  99]\n",
      " [ 20 611   3   9   6   5  22  16  37 271]\n",
      " [ 85  11 334  56 141 117 106  90  23  37]\n",
      " [ 27  22  54 264  80 196 130 131  17  79]\n",
      " [ 39   5  97  61 405  65 129 159  12  28]\n",
      " [ 16  10  58 126  77 432  79 157  13  32]\n",
      " [  5  21  34  43  61  50 668  66   4  48]\n",
      " [ 11  10  27  41  55  75  33 683   5  60]\n",
      " [134  59   9  15  12   6  10  20 631 104]\n",
      " [ 29  99   3  15  10   6  20  38  28 752]]\n",
      "Average Total Loss over Batches: 0.9352724336433411\n",
      "[[603  44  81  22  26   5   6  13 178  22]\n",
      " [ 51 770  13  17   3   2  12  13  60  59]\n",
      " [118  13 481  81 105  73  54  41  24  10]\n",
      " [ 47  33 129 359  86 145  78  68  35  20]\n",
      " [ 66  11 224  78 377  56  69  91  22   6]\n",
      " [ 21  11 144 202  64 386  48  99  16   9]\n",
      " [ 22  37 109 108  89  51 516  35  15  18]\n",
      " [ 41  22  81  93  65  87  15 564  12  20]\n",
      " [135  87  18  17   6   3   5   7 704  18]\n",
      " [ 82 221  20  32  13   9  10  39  74 500]]\n",
      "Average Total Loss over Batches: 0.8849980319499969\n",
      "[[505  50  82  31  50  15  23  43 149  52]\n",
      " [ 31 722   9  20   4   4  29  20  37 124]\n",
      " [ 58   8 339 111 166 128  80  79  17  14]\n",
      " [ 19  22  69 353  80 210 104 103  13  27]\n",
      " [ 22   9  83 100 420  87 117 142  10  10]\n",
      " [  7   8  68 195  61 457  49 136   8  11]\n",
      " [  5  14  36  99  79  73 601  65   4  24]\n",
      " [ 11   7  25  83  67 110  21 654   3  19]\n",
      " [128  86  26  34  16  14   9  20 613  54]\n",
      " [ 45 145  10  41  15  23  19  62  34 606]]\n",
      "Average Total Loss over Batches: 0.8408936443829537\n",
      "[[558  40 108  26  31   2  11  12 166  46]\n",
      " [ 33 664  27  18  14   4  16  18  70 136]\n",
      " [ 79   8 464  74 143  77  76  44  21  14]\n",
      " [ 29  21 161 338  84 177  74  63  29  24]\n",
      " [ 33   5 209  80 433  52  84  75  19  10]\n",
      " [ 13  11 154 188  74 390  54  86  19  11]\n",
      " [ 11  14  95  88 105  52 569  34   9  23]\n",
      " [ 19   9  93  87 107  81  21 545   8  30]\n",
      " [122  61  25  19   9   6   5  11 700  42]\n",
      " [ 56 134  21  33  20   5  15  44  67 605]]\n",
      "Average Total Loss over Batches: 0.7825791534471512\n",
      "[[554  48 138  25  43   7  17  10 141  17]\n",
      " [ 54 702  24  23   4   4  35  12  71  71]\n",
      " [ 78   7 453  85 144  73  93  41  18   8]\n",
      " [ 31  13 152 316  94 163 145  47  23  16]\n",
      " [ 34   7 174  69 441  48 138  71  12   6]\n",
      " [ 20   9 149 190  73 357  95  84  13  10]\n",
      " [ 12   8  88  68  83  34 667  20   8  12]\n",
      " [ 29   9 107  89 102  78  37 528   6  15]\n",
      " [144  81  34  25  15   7   9   8 657  20]\n",
      " [ 89 178  35  50  25  14  29  27  71 482]]\n",
      "Average Total Loss over Batches: 0.7383986696243287\n",
      "[[533  28 102  48  39  24  21  23 127  55]\n",
      " [ 43 641  20  22   5   9  37  22  35 166]\n",
      " [ 67   5 357 106 139 136 116  48  15  11]\n",
      " [ 19  20  75 370  57 247 126  52  10  24]\n",
      " [ 23   9 106  98 404 100 160  81   8  11]\n",
      " [ 11   8  77 203  53 470  71  84   8  15]\n",
      " [  6   9  49 100  67  72 638  30   4  25]\n",
      " [ 13   6  53 122  78 141  27 524   3  33]\n",
      " [147  65  29  56  11  19  11   8 595  59]\n",
      " [ 45 110  15  55  19  16  22  33  40 645]]\n",
      "Average Total Loss over Batches: 0.6983276265072823\n",
      "[[498  53 119  27  38   6  17  20 176  46]\n",
      " [ 19 724  19  16   9   5  20  18  43 127]\n",
      " [ 75  13 445  89 116 110  69  45  26  12]\n",
      " [ 24  33 119 279  80 242  96  64  31  32]\n",
      " [ 33  12 169  69 413  80 113  85  15  11]\n",
      " [ 12  11 121 143  72 471  48  91  11  20]\n",
      " [  8  27  82  94  76  84 543  40  11  35]\n",
      " [ 17  12  79  71 108 137  18 514   7  37]\n",
      " [111  90  28  28   7  14   5   9 666  42]\n",
      " [ 38 177  16  27  14  21  16  27  55 609]]\n",
      "Average Total Loss over Batches: 0.6599487050008774\n",
      "[[533  24 105  45  28  14  28  14 179  30]\n",
      " [ 55 634  19  28   5   9  39   9  90 112]\n",
      " [ 76   6 388 105 108 122 124  36  28   7]\n",
      " [ 26  10 117 332  62 244 140  28  25  16]\n",
      " [ 30   5 146 109 338 102 178  66  17   9]\n",
      " [ 13   5  95 196  54 459  91  70  10   7]\n",
      " [  5  10  63  92  57  69 668  15   8  13]\n",
      " [ 22   8  76 125  87 140  46 463   9  24]\n",
      " [121  56  24  32  12  19  13   6 686  31]\n",
      " [ 65 141  23  58  13  29  29  22  75 545]]\n",
      "Average Total Loss over Batches: 0.6259329641211033\n",
      "[[557  26  78  24  42   2  11  23 177  60]\n",
      " [ 53 562  12  14   9   4  18  14 100 214]\n",
      " [100   8 353  84 194  55  89  59  37  21]\n",
      " [ 35  23 108 287 117 135 101  95  43  56]\n",
      " [ 41   7 101  64 508  34  95 100  26  24]\n",
      " [ 26   7 115 208 105 290  61 130  22  36]\n",
      " [ 12  13  87  85 120  37 548  41  12  45]\n",
      " [ 25  10  65  78 127  63  24 546  10  52]\n",
      " [126  37  18  15  17   5   4  10 705  63]\n",
      " [ 44 111  12  20  20   8  16  34  73 662]]\n",
      "Average Total Loss over Batches: 0.5996371446466446\n",
      "[[614  36 121  20  40   0   9  20 116  24]\n",
      " [ 75 666  26  12  10   6  27  15  60 103]\n",
      " [109  10 492  76 124  54  57  48  21   9]\n",
      " [ 39  26 176 267  97 158 112  66  32  27]\n",
      " [ 44   7 215  59 419  48  93  86  17  12]\n",
      " [ 22  10 168 171  90 339  62 111  16  11]\n",
      " [ 13  26 138  66 103  47 547  25  12  23]\n",
      " [ 32  10 112  74 107  79  24 530   6  26]\n",
      " [177  70  36  17  13   7   5   7 631  37]\n",
      " [ 76 172  30  33  17  13  21  37  64 537]]\n",
      "Average Total Loss over Batches: 0.558077157560587\n",
      "[[508  40 100  29  35   4  19  19 207  39]\n",
      " [ 37 698  16   9   7   7  31  19  61 115]\n",
      " [ 77   9 384  68 146  90 117  61  37  11]\n",
      " [ 25  33 110 258  86 195 157  69  33  34]\n",
      " [ 27  11 128  66 422  62 154  88  27  15]\n",
      " [ 17  14 117 151  87 365 108 103  19  19]\n",
      " [  9  21  86  59  78  47 632  32  12  24]\n",
      " [ 22  15  70  70 105  94  38 543   8  35]\n",
      " [107  70  24  20  16   7  12   8 693  43]\n",
      " [ 43 174  16  26  11  17  31  46  76 560]]\n",
      "Average Total Loss over Batches: 0.535716081289053\n",
      "[[598  43  58  35  37   6  14  21 146  42]\n",
      " [ 58 678  15  20   7   5  23  12  58 124]\n",
      " [118  16 279 100 159 105 107  60  35  21]\n",
      " [ 38  30  71 328  81 198 127  62  23  42]\n",
      " [ 54  11  91  89 419  80 127  86  22  21]\n",
      " [ 28  11  73 197  83 405  70 102  11  20]\n",
      " [ 13  23  58  96  86  61 597  27  12  27]\n",
      " [ 39  14  53 102 103 102  29 507   7  44]\n",
      " [163  81  11  31  14  12  13   9 605  61]\n",
      " [ 59 166  11  28  21  14  19  28  57 597]]\n",
      "Average Total Loss over Batches: 0.5170891344475746\n",
      "[[640  37  68  29  30   7  17  18 111  43]\n",
      " [ 48 660  14  25   7   9  29  16  67 125]\n",
      " [129  10 281 121 150 111 110  48  29  11]\n",
      " [ 39  22  83 345  68 203 137  49  30  24]\n",
      " [ 53   8 110 112 386  85 134  81  19  12]\n",
      " [ 30   9  98 207  75 393  68  89  15  16]\n",
      " [ 13  16  46 106  81  76 602  31  11  18]\n",
      " [ 29  14  51 127  95 114  34 496   7  33]\n",
      " [199  73  14  26  11  12  11   5 595  54]\n",
      " [ 66 159  13  43  15  18  21  44  51 570]]\n",
      "Average Total Loss over Batches: 0.4807018278706074\n",
      "[[557  27  77  33  39   1  19  21 184  42]\n",
      " [ 81 559  14  17  14   7  21  19  87 181]\n",
      " [121   8 310 100 175  74  99  55  44  14]\n",
      " [ 46  19  92 309 110 147 126  76  40  35]\n",
      " [ 49   6 117  84 427  53 113 105  31  15]\n",
      " [ 30   8 105 195  94 330  87 109  24  18]\n",
      " [ 15   9  54  97 125  49 565  39  15  32]\n",
      " [ 36   9  59 104 117  72  28 521  11  43]\n",
      " [158  48  19  24  17   7  10   8 663  46]\n",
      " [ 71 126  14  38  21  13  17  37  77 586]]\n",
      "Average Total Loss over Batches: 0.4579634172856808\n",
      "[[480  77  70  29  39   8  20  23 196  58]\n",
      " [ 32 726   8  16   4   4  13  17  42 138]\n",
      " [101  28 311  96 126 104  88  70  49  27]\n",
      " [ 26  51  73 327  60 193  86  84  36  64]\n",
      " [ 39  29 112  91 351  86 113 118  29  32]\n",
      " [ 20  21  78 192  58 405  53 118  19  36]\n",
      " [ 10  60  55 113  71  71 519  45  13  43]\n",
      " [ 18  36  40  89  73 104  22 552   9  57]\n",
      " [ 97 121  14  25   6  13   6   6 651  61]\n",
      " [ 23 214   4  22   3  14  15  32  58 615]]\n",
      "Average Total Loss over Batches: 0.45068081755638123\n",
      "[[588  34  52  38  30   3  12  24 181  38]\n",
      " [ 69 623  12  17   6   6  15  19  77 156]\n",
      " [131   9 310 109 137 106  82  57  44  15]\n",
      " [ 42  33  84 332  81 180  92  81  38  37]\n",
      " [ 54  15 121  99 387  80  96 104  29  15]\n",
      " [ 26  11 108 189  81 386  60 100  22  17]\n",
      " [ 14  29  82 125 111  74 480  43  16  26]\n",
      " [ 36  13  59 114  92 106  24 500  16  40]\n",
      " [145  65   8  22   8  12   6  12 675  47]\n",
      " [ 58 150  12  31   8  19  18  40  89 575]]\n",
      "Average Total Loss over Batches: 0.4236898729097843\n",
      "[[509  40  82  45  44  10  19  26 180  45]\n",
      " [ 47 631  15  23   7  13  25  17  71 151]\n",
      " [102   9 307 121 121 124  99  57  41  19]\n",
      " [ 32  27  93 352  76 200  90  67  29  34]\n",
      " [ 37  14 135 122 341 113 107  93  24  14]\n",
      " [ 19   9  99 203  78 411  53  98  17  13]\n",
      " [ 11  21  66 160  79  83 500  41  15  24]\n",
      " [ 22  15  62 121  89 122  30 496  10  33]\n",
      " [120  71  18  24  13  16  10  11 670  47]\n",
      " [ 48 157  14  38   7  28  18  44  66 580]]\n",
      "Average Total Loss over Batches: 0.4056096258878708\n",
      "[[478  29 150  38  50  10  20  26 167  32]\n",
      " [ 63 597  32  21  18  12  32  27  76 122]\n",
      " [ 60   3 442  76 155  91  96  42  28   7]\n",
      " [ 24  15 159 280  96 203 101  76  25  21]\n",
      " [ 27   6 188  72 404  79 108  90  18   8]\n",
      " [ 12   5 162 157  85 379  69 106  17   8]\n",
      " [  9  11 117 115 103  72 530  19   9  15]\n",
      " [ 19   8 106  85 124 104  32 494   7  21]\n",
      " [125  61  44  26  26  15  12  19 629  43]\n",
      " [ 60 139  31  44  20  30  29  54  80 513]]\n",
      "Average Total Loss over Batches: 0.3927128293278813\n",
      "[[532  24  88  42  54  10  22  15 164  49]\n",
      " [ 48 553  15  21  14  12  37  21  70 209]\n",
      " [ 93   8 314  89 156 112 130  44  40  14]\n",
      " [ 29  20  99 251  98 203 165  72  26  37]\n",
      " [ 34  11 131  86 392  87 136  87  21  15]\n",
      " [ 20   6 104 162  87 407  97  90  12  15]\n",
      " [ 11  15  68  88  96  72 589  28  11  22]\n",
      " [ 25  12  66  93 119 127  49 448  10  51]\n",
      " [138  59  22  25  19  16  22   9 627  63]\n",
      " [ 55 120  17  27  23  25  25  34  57 617]]\n",
      "Average Total Loss over Batches: 0.38120714947104456\n",
      "[[533  44  89  41  53  11  19  28 145  37]\n",
      " [ 45 651  18  27  16  10  23  29  54 127]\n",
      " [ 84  12 299 120 182 112  84  69  32   6]\n",
      " [ 21  22  84 342  81 222  75 100  24  29]\n",
      " [ 27  15 125 108 406  88  94 106  19  12]\n",
      " [ 14   8  98 199  82 415  44 119  11  10]\n",
      " [  7  18  63 139 123  92 484  47  10  17]\n",
      " [ 16  10  61 107 106 120  25 525   5  25]\n",
      " [136  93  25  31  22  18  12  15 598  50]\n",
      " [ 56 171  19  39  19  32  18  56  51 539]]\n",
      "Average Total Loss over Batches: 0.3635789569941163\n",
      "[[524  49  86  38  38  11  16  19 165  54]\n",
      " [ 47 637  13  16   9   6  17  18  58 179]\n",
      " [115  11 335 101 130 100  75  62  44  27]\n",
      " [ 32  33 106 288  82 187  88  95  39  50]\n",
      " [ 45  19 176  90 332  82 100 109  22  25]\n",
      " [ 26  11 115 181  76 371  49 121  23  27]\n",
      " [ 14  31  78 129  89  72 498  42  11  36]\n",
      " [ 31  15  58  92  91 113  27 505  13  55]\n",
      " [145  85  21  18  13   9   6  11 628  64]\n",
      " [ 49 156  15  32  11  13  15  35  57 617]]\n",
      "Average Total Loss over Batches: 0.35282070506036284\n",
      "[[503  32  71  37  40   6  27  19 231  34]\n",
      " [ 61 611  17  17  13   9  26  22  81 143]\n",
      " [106   6 315 101 149  94 108  55  50  16]\n",
      " [ 26  26  84 286  84 193 135  91  43  32]\n",
      " [ 41  12 142  88 363  78 129  99  31  17]\n",
      " [ 21   7  99 176  81 378  91 114  20  13]\n",
      " [ 12  22  67  92  95  66 561  43  16  26]\n",
      " [ 27  10  60  98 109 103  31 516  14  32]\n",
      " [105  60  18  24  16  12  14   9 704  38]\n",
      " [ 56 185  13  33  15  22  21  47  71 537]]\n",
      "Average Total Loss over Batches: 0.3429688258123398\n",
      "[[534  43  57  41  35   8  13  26 203  40]\n",
      " [ 38 676  11  20   9   9  13  14  80 130]\n",
      " [120  17 293 141 135 101  66  48  60  19]\n",
      " [ 34  44  76 339  59 190  81  89  46  42]\n",
      " [ 55  22 141 106 329  86  96 106  39  20]\n",
      " [ 24  13  94 214  63 383  40 118  29  22]\n",
      " [ 16  36  74 157  75  82 462  36  24  38]\n",
      " [ 31  22  56 110  82 108  26 505  17  43]\n",
      " [129  82   9  24  11  13   4   8 672  48]\n",
      " [ 52 203  11  36   6  19  14  27  91 541]]\n",
      "Average Total Loss over Batches: 0.33364106782704595\n",
      "[[487  36  69  26  53  17  25  12 236  39]\n",
      " [ 50 584  24  13  19   9  35  13 113 140]\n",
      " [ 97   7 327  86 156 100 106  52  50  19]\n",
      " [ 27  21 104 262 104 207 125  66  47  37]\n",
      " [ 31  10 131  68 411  82 137  76  43  11]\n",
      " [ 26   8 114 171 101 364  84  88  24  20]\n",
      " [ 17  14  80  97 102  70 552  20  19  29]\n",
      " [ 27  14  71  86 139 109  34 448  22  50]\n",
      " [101  65  16  20  14   8  12   7 709  48]\n",
      " [ 60 140  20  28  26  22  23  35  94 552]]\n",
      "Average Total Loss over Batches: 0.3237775545451045\n",
      "[[521  50  73  33  44  17  16  30 179  37]\n",
      " [ 50 657  14  20  17   4  18  24  54 142]\n",
      " [112  19 301  83 173 116  68  67  41  20]\n",
      " [ 37  33  87 247 110 222  78 107  32  47]\n",
      " [ 47  15 121  73 404  86  80 128  28  18]\n",
      " [ 23  11  93 183  87 393  46 127  15  22]\n",
      " [ 19  32  64 104 128  90 460  54  14  35]\n",
      " [ 25  12  53  89 106 112  25 529   9  40]\n",
      " [140  98  20  27  16  12   6  15 611  55]\n",
      " [ 59 178  12  21  18  27  15  47  56 567]]\n",
      "Average Total Loss over Batches: 0.3140893542975187\n",
      "[[568  32  68  34  29   6  29  24 170  40]\n",
      " [ 61 623  13  15   9   6  32  20  64 157]\n",
      " [136  15 301  86 134  82 118  72  35  21]\n",
      " [ 42  38  96 280  73 144 148  96  35  48]\n",
      " [ 51  15 124  81 347  62 152 120  25  23]\n",
      " [ 27  11 106 205  91 302  98 109  22  29]\n",
      " [ 17  29  72 110  77  56 545  42  15  37]\n",
      " [ 34  18  54  82 102  82  45 519  12  52]\n",
      " [150  77  15  23  14   7  12  13 636  53]\n",
      " [ 57 168  14  25  10  13  28  30  63 592]]\n",
      "Average Total Loss over Batches: 0.30403188918471336\n",
      "[[499  50  85  37  55  16  25  28 151  54]\n",
      " [ 47 639  15  19  16  14  25  26  42 157]\n",
      " [ 90  12 295  97 172 119 105  63  32  15]\n",
      " [ 31  29  80 279  99 220 102  99  19  42]\n",
      " [ 25  12 101  83 425  96 129  94  17  18]\n",
      " [ 20   9  87 174 100 394  62 114  16  24]\n",
      " [ 10  27  59 102 124  84 512  46   9  27]\n",
      " [ 23  12  49  85 118 131  40 495   7  40]\n",
      " [135  91  30  27  28  20  15  15 581  58]\n",
      " [ 50 172  18  29  18  33  21  46  50 563]]\n",
      "Average Total Loss over Batches: 0.29849242922872304\n",
      "[[537  36  68  45  33  17  19  16 178  51]\n",
      " [ 50 628  14  15   6   7  20  18  67 175]\n",
      " [130  15 267 133 124 101  90  68  48  24]\n",
      " [ 35  37  75 323  70 176  99  99  29  57]\n",
      " [ 42  16 120 110 349  92 102 114  29  26]\n",
      " [ 18  12  88 235  71 354  56 120  16  30]\n",
      " [ 21  38  62 131  85  70 479  51  15  48]\n",
      " [ 37  16  47 117  96 101  25 504  11  46]\n",
      " [141  93  14  25  14  11   7   7 625  63]\n",
      " [ 54 158  10  34  11  21  15  29  57 611]]\n",
      "Average Total Loss over Batches: 0.28688237985149023\n",
      "[[578  29  91  48  27  12  16  17 141  41]\n",
      " [ 81 587  21  30  11   8  19  13  73 157]\n",
      " [128  11 311 132 123 108  82  53  33  19]\n",
      " [ 43  21 100 354  68 195  79  71  30  39]\n",
      " [ 50  12 160 128 338  83 100  87  22  20]\n",
      " [ 26   8 117 238  60 368  48  98  15  22]\n",
      " [ 21  23  94 163  80  80 466  29  15  29]\n",
      " [ 34  11  68 132  92 118  28 464  12  41]\n",
      " [183  59  26  30  13  18   8  10 589  64]\n",
      " [ 77 135  23  48  15  26  14  35  53 574]]\n",
      "Average Total Loss over Batches: 0.28390500726774337\n",
      "[[488  60  95  38  54  20  17  29 157  42]\n",
      " [ 44 709  19  17  10   8  22  24  43 104]\n",
      " [105  18 331  84 136 119  79  84  32  12]\n",
      " [ 35  34  86 263  84 221  99 108  24  46]\n",
      " [ 44  19 131  74 375  93  92 139  21  12]\n",
      " [ 17  11 102 162  89 382  66 135  17  19]\n",
      " [ 16  31  66 101  96  95 497  53  12  33]\n",
      " [ 30  18  47  78  87 114  29 550  10  37]\n",
      " [166 111  33  24  17  22   9  16 559  43]\n",
      " [ 60 203  16  28  13  23  26  45  49 537]]\n",
      "Average Total Loss over Batches: 0.27990222217842936\n",
      "[[518  43  72  34  52  13  21  40 161  46]\n",
      " [ 45 630  16  20  12   9  28  29  64 147]\n",
      " [104  10 302  95 148 109 100  72  47  13]\n",
      " [ 36  31  97 263  97 199 114  88  33  42]\n",
      " [ 35  17 143  79 347  88 131 117  29  14]\n",
      " [ 16   9 100 183  92 365  76 114  24  21]\n",
      " [ 16  27  74  97  92  86 509  46  19  34]\n",
      " [ 25  15  52  89 106 107  42 509  11  44]\n",
      " [142  75  24  23  24  13  12  17 613  57]\n",
      " [ 49 177  14  26  18  37  26  49  57 547]]\n",
      "Average Total Loss over Batches: 0.2706350837543607\n",
      "[[493  37 109  52  47  12  19  19 174  38]\n",
      " [ 55 609  25  30  18  10  24  22  66 141]\n",
      " [ 79   5 360 110 150 104  92  54  37   9]\n",
      " [ 29  19 112 314  89 201  94  81  27  34]\n",
      " [ 27  13 173  95 378  72 113  94  25  10]\n",
      " [ 14   7 122 210  92 352  74 105  14  10]\n",
      " [ 11  22  83 139 103  73 502  32  15  20]\n",
      " [ 22   9  71 117 120 118  33 470  10  30]\n",
      " [138  66  37  27  28  18  13  14 610  49]\n",
      " [ 65 156  29  46  22  33  22  42  60 525]]\n",
      "Average Total Loss over Batches: 0.26740780694648625\n",
      "[[489  39  77  48  57  28  20  25 173  44]\n",
      " [ 35 611  20  36  16  14  29  24  72 143]\n",
      " [ 99  10 286 134 146 129  78  63  43  12]\n",
      " [ 24  23  87 348  75 217  85  85  24  32]\n",
      " [ 35  12 128 112 346 106 106 118  27  10]\n",
      " [ 11   8  89 225  81 400  49 107  14  16]\n",
      " [ 12  26  79 158  85 105 461  36  10  28]\n",
      " [ 13  10  52 116 101 143  20 498  11  36]\n",
      " [137  66  24  39  24  25  12  15 603  55]\n",
      " [ 47 161  18  56  22  38  22  42  62 532]]\n",
      "Average Total Loss over Batches: 0.26084831118270757\n",
      "[[541  57  71  35  50  10  14  19 153  50]\n",
      " [ 59 647  17  14  15   6  23  17  49 153]\n",
      " [121  19 300 112 154 103  78  56  38  19]\n",
      " [ 44  33  96 306  89 173  88  86  31  54]\n",
      " [ 48  16 145  89 363  86 100 109  25  19]\n",
      " [ 26  10 116 196  95 346  59 108  20  24]\n",
      " [ 20  36  85 121 110  68 468  40  21  31]\n",
      " [ 36  18  64  99 118 104  26 467  12  56]\n",
      " [158  97  20  19  23  10   9  12 592  60]\n",
      " [ 79 177  20  35  21  20  14  32  55 547]]\n",
      "Average Total Loss over Batches: 0.24807630498252808\n",
      "[[524  43  68  43  38  10  20  21 173  60]\n",
      " [ 50 614  13  17  19   7  23  18  48 191]\n",
      " [130  13 282 104 153 103  91  61  42  21]\n",
      " [ 38  36  84 302  95 167 110  87  28  53]\n",
      " [ 55  20 126  94 356  70 120 105  30  24]\n",
      " [ 25  12  99 189  89 340  79 117  16  34]\n",
      " [ 18  33  70 129  94  73 495  36  15  37]\n",
      " [ 32  20  49  97 115  94  36 487  10  60]\n",
      " [131  79  14  26  17   7  12  11 636  67]\n",
      " [ 59 150  13  22  15  26  19  33  55 608]]\n",
      "Average Total Loss over Batches: 0.25222974389106034\n",
      "[[581  42  78  39  25   8  19  33 136  39]\n",
      " [ 59 625  23  30  13  10  32  22  55 131]\n",
      " [137  14 299 105 124  98 115  68  31   9]\n",
      " [ 35  27  97 312  83 173 116  95  29  33]\n",
      " [ 48  14 149 101 344  67 140 106  20  11]\n",
      " [ 24   8 118 217  73 319  85 118  17  21]\n",
      " [ 16  17  90 123  90  53 538  41  10  22]\n",
      " [ 29  15  61 108  94  95  45 514   7  32]\n",
      " [176  86  27  28  16  17  11  13 574  52]\n",
      " [ 82 159  22  46  16  26  33  53  61 502]]\n",
      "Average Total Loss over Batches: 0.24414778704229742\n",
      "[[396  52  99  46  59  13  24  34 225  52]\n",
      " [ 33 652  17  16  17   6  24  23  44 168]\n",
      " [ 77  13 310  90 167 111 106  66  43  17]\n",
      " [ 24  28  93 271 104 175 129  91  29  56]\n",
      " [ 31  13 131  78 379  79 123 119  29  18]\n",
      " [ 13   8 112 170  94 350  91 124  17  21]\n",
      " [  9  28  71  99 108  74 512  48  16  35]\n",
      " [ 18  18  49  88 115 104  46 508   8  46]\n",
      " [ 95  91  26  26  31  16  14  11 629  61]\n",
      " [ 39 182  16  32  18  24  25  34  53 577]]\n",
      "Average Total Loss over Batches: 0.24150767158161848\n",
      "[[544  39  80  38  39  11  16  28 171  34]\n",
      " [ 60 605  26  21  17  12  23  25  72 139]\n",
      " [108  10 304 102 151 125  80  67  40  13]\n",
      " [ 34  20  90 318  83 226  59  95  36  39]\n",
      " [ 48  14 127 106 362  92 105 104  28  14]\n",
      " [ 24   9 112 187  78 397  45 114  17  17]\n",
      " [ 12  27  77 146 110  98 447  39  16  28]\n",
      " [ 29  13  63 109  97 121  29 481  10  48]\n",
      " [157  62  22  26  20  20  11  16 613  53]\n",
      " [ 77 145  18  45  20  38  18  47  75 517]]\n",
      "Average Total Loss over Batches: 0.23769525891661644\n",
      "[[514  39  79  46  37  10  18  21 197  39]\n",
      " [ 59 602  19  19  17   8  17  16  91 152]\n",
      " [119  13 273 122 170  98  89  48  51  17]\n",
      " [ 40  33  81 325 106 175  88  84  36  32]\n",
      " [ 50  12 119 115 399  61 114  89  29  12]\n",
      " [ 20  11 100 234  96 331  62 100  27  19]\n",
      " [ 19  32  83 134 118  63 478  34  20  19]\n",
      " [ 42  17  61 125 109 106  38 445  16  41]\n",
      " [134  87  10  21  16  11  10   9 656  46]\n",
      " [ 72 166  21  39  22  25  19  32  81 523]]\n",
      "Average Total Loss over Batches: 0.23906666039481758\n",
      "[[509  34  86  48  49  21  26  17 164  46]\n",
      " [ 63 574  20  24  19   9  32  28  64 167]\n",
      " [ 98   7 301 109 147 124 104  60  37  13]\n",
      " [ 24  17  87 319  92 215  92  90  24  40]\n",
      " [ 38  11 124  95 387  90 125  98  19  13]\n",
      " [ 16   8 101 194  87 384  65 114  12  19]\n",
      " [ 12  19  80 117 104  72 514  44   9  29]\n",
      " [ 22  12  59 118 108 118  42 467  10  44]\n",
      " [155  66  25  32  28  18  15  14 594  53]\n",
      " [ 64 142  15  43  23  32  23  43  63 552]]\n",
      "Average Total Loss over Batches: 0.22922825365886093\n",
      "[[583  31  69  50  37  17  16  14 149  34]\n",
      " [ 74 567  24  39  20  12  35  21  74 134]\n",
      " [130   9 321 119 123 119  87  49  38   5]\n",
      " [ 35  18 111 314  83 224  94  67  29  25]\n",
      " [ 49  10 156 112 358  90 112  81  22  10]\n",
      " [ 21   5 131 205  75 389  59  85  17  13]\n",
      " [ 16  13  87 150  94  82 503  20  16  19]\n",
      " [ 34  10  78 121 108 123  33 452  11  30]\n",
      " [176  61  23  27  26  16  13  13 605  40]\n",
      " [ 81 134  24  56  24  48  29  43  72 489]]\n",
      "Average Total Loss over Batches: 0.228673297460936\n",
      "[[486  32  85  54  40  13  22  24 205  39]\n",
      " [ 57 580  20  26  19  10  25  17  97 149]\n",
      " [ 91  10 311 136 136 113  93  51  46  13]\n",
      " [ 23  21  96 354  88 186  79  80  39  34]\n",
      " [ 38  11 149 129 343  79 117  90  30  14]\n",
      " [ 17   7 110 230  76 356  60 105  21  18]\n",
      " [ 15  19  88 148  96  75 483  34  16  26]\n",
      " [ 26   9  73 128 106 109  34 466  11  38]\n",
      " [126  67  17  28  17  15  12  11 663  44]\n",
      " [ 65 149  18  54  23  30  23  41  73 524]]\n",
      "Average Total Loss over Batches: 0.22177178550936283\n",
      "[[516  34  74  31  48  11  25  26 186  49]\n",
      " [ 43 572  15  23  19  10  37  24  64 193]\n",
      " [103  16 274  99 148 103 116  70  56  15]\n",
      " [ 30  24  79 271  98 183 138  92  37  48]\n",
      " [ 42  13 113  80 343  77 154 121  34  23]\n",
      " [ 12   7  94 182  88 337  99 124  27  30]\n",
      " [ 12  14  59 107  94  69 554  44  15  32]\n",
      " [ 21  15  49  86 103 101  47 521  12  45]\n",
      " [128  69  17  17  24  12  16  10 647  60]\n",
      " [ 40 150  16  27  18  24  26  48  72 579]]\n",
      "Average Total Loss over Batches: 0.21872879572775214\n",
      "[[492  49 102  36  59  12  24  30 141  55]\n",
      " [ 47 621  14  16  14   7  35  20  59 167]\n",
      " [ 94  12 312  96 146  93 134  67  34  12]\n",
      " [ 35  34  99 265 105 165 135  89  28  45]\n",
      " [ 32  14 141  80 366  69 149 110  21  18]\n",
      " [ 17  11 111 167  97 335  99 120  19  24]\n",
      " [ 12  23  71  94  94  65 556  41  11  33]\n",
      " [ 22  13  67  93 113  98  51 486   9  48]\n",
      " [155  93  27  19  29  15  19  14 568  61]\n",
      " [ 50 168  18  34  19  24  29  44  54 560]]\n",
      "Average Total Loss over Batches: 0.21802194573547692\n",
      "[[518  39 107  34  60  16  18  28 150  30]\n",
      " [ 79 613  21  18  26  10  23  20  53 137]\n",
      " [106  10 295 103 182 104  88  66  32  14]\n",
      " [ 34  25 101 270 113 202  89 101  25  40]\n",
      " [ 40  15 112  84 428  86 100 105  19  11]\n",
      " [ 16   9 112 180 106 376  62 111  13  15]\n",
      " [ 15  23  78 110 140  90 477  38  11  18]\n",
      " [ 26  18  54  94 121 118  28 497  10  34]\n",
      " [175  86  34  22  33  18   7  15 563  47]\n",
      " [ 73 172  21  39  23  31  16  55  52 518]]\n",
      "Average Total Loss over Batches: 0.21462857751265169\n",
      "[[502  42 120  30  35  10  18  32 175  36]\n",
      " [ 55 631  27  19  14  12  29  20  76 117]\n",
      " [100  11 371  89 124  94  92  60  44  15]\n",
      " [ 33  25 126 273  80 193 113  84  39  34]\n",
      " [ 45  14 176  76 332  83 121 103  33  17]\n",
      " [ 16   9 144 180  82 349  75 110  18  17]\n",
      " [ 14  26 114 104 101  75 493  30  19  24]\n",
      " [ 28  17  81  92 106 112  32 484  14  34]\n",
      " [141  77  34  18  15  18  10  12 627  48]\n",
      " [ 60 184  26  38  18  32  24  47  73 498]]\n",
      "Average Total Loss over Batches: 0.20781677596889436\n",
      "[[464  47  74  47  55  20  15  31 183  64]\n",
      " [ 41 607  14  17  19  11  23  22  63 183]\n",
      " [106  16 262 115 162 126  81  70  42  20]\n",
      " [ 31  27  71 295 106 191  80 115  34  50]\n",
      " [ 38  13 104  87 381  96 114 120  27  20]\n",
      " [ 20  11  86 174  95 393  51 118  18  34]\n",
      " [ 16  27  71 132 113  84 457  46  17  37]\n",
      " [ 23  15  44 107 111 110  29 502   9  50]\n",
      " [126  80  20  23  29  19  12  13 611  67]\n",
      " [ 47 158  17  29  20  35  15  42  62 575]]\n",
      "Average Total Loss over Batches: 0.21045466394938528\n",
      "[[458  42 104  52  46  13  27  26 187  45]\n",
      " [ 52 606  23  32  20   9  44  21  60 133]\n",
      " [102   8 325 114 131 108 109  60  34   9]\n",
      " [ 29  24 103 302  90 174 145  75  28  30]\n",
      " [ 36  11 140  96 348  78 143 104  29  15]\n",
      " [ 12  10 112 189  87 358 100 109  13  10]\n",
      " [ 15  20  83 111  87  72 548  28  15  21]\n",
      " [ 29  14  63 115 101 115  52 470   8  33]\n",
      " [120  85  30  30  24  15  17   9 622  48]\n",
      " [ 58 168  19  44  24  37  35  46  58 511]]\n",
      "Average Total Loss over Batches: 0.20352143881302326\n",
      "[[476  38  98  47  58  11  22  22 181  47]\n",
      " [ 53 578  21  20  23  10  35  24  64 172]\n",
      " [ 90   7 306 118 165 104 102  56  38  14]\n",
      " [ 33  18  92 304 113 183 119  74  31  33]\n",
      " [ 42   7 135  84 391  80 127  89  26  19]\n",
      " [ 16   9 126 210  97 320  74 107  21  20]\n",
      " [ 13  20  79 126 117  64 510  28  17  26]\n",
      " [ 25  13  74 110 118 114  46 455  10  35]\n",
      " [129  74  29  23  29  20  16  11 611  58]\n",
      " [ 54 161  25  42  26  33  28  38  59 534]]\n",
      "Average Total Loss over Batches: 0.20491911988563835\n",
      "[[539  47 105  34  40   9  18  25 142  41]\n",
      " [ 62 612  26  23  16   8  27  20  73 133]\n",
      " [108  12 381  81 121  91  97  58  37  14]\n",
      " [ 34  27 141 280  91 168 113  85  30  31]\n",
      " [ 46  16 201  75 319  67 124 111  24  17]\n",
      " [ 17  10 150 175  79 350  75 110  19  15]\n",
      " [ 15  22 121 114  89  64 495  36  16  28]\n",
      " [ 32  17  89 102  97  96  40 484  11  32]\n",
      " [162  86  34  17  19  22  11  13 585  51]\n",
      " [ 73 180  25  42  19  31  22  47  59 502]]\n",
      "Average Total Loss over Batches: 0.20843087718382478\n",
      "[[488  54  89  25  50  11  27  22 181  53]\n",
      " [ 55 616  18  11  17   5  23  17  89 149]\n",
      " [119  15 295  86 171  75 111  60  51  17]\n",
      " [ 38  33  98 258 122 138 126  85  47  55]\n",
      " [ 52  14 123  60 387  57 147  98  36  26]\n",
      " [ 25   9 120 176 113 298 100 104  27  28]\n",
      " [ 18  37  77  87 119  50 523  29  22  38]\n",
      " [ 37  25  64  89 126  83  48 444  23  61]\n",
      " [136  82  25  17  26   9  11  12 632  50]\n",
      " [ 58 181  19  24  20  17  25  36  84 536]]\n",
      "Average Total Loss over Batches: 0.19782685893334448\n",
      "[[563  39  84  36  50  10  21  26 135  36]\n",
      " [ 79 629  18  21  19   7  29  18  50 130]\n",
      " [124  12 299  96 172  89 101  65  28  14]\n",
      " [ 39  27  90 280 115 184 115  86  25  39]\n",
      " [ 48  12 126  82 392  71 126 109  19  15]\n",
      " [ 16   7 111 187 103 345  80 116  18  17]\n",
      " [ 16  24  81 115 112  60 526  28  11  27]\n",
      " [ 33  14  61 105 120 104  38 486   7  32]\n",
      " [195  86  21  22  26  17  14  12 556  51]\n",
      " [ 78 183  18  43  25  31  26  43  44 509]]\n",
      "Average Total Loss over Batches: 0.1977796895459667\n",
      "[[488  54 104  38  51  16  26  33 148  42]\n",
      " [ 47 638  19  27  17  11  26  26  52 137]\n",
      " [ 90  14 325 114 142 101  87  74  36  17]\n",
      " [ 28  30  98 306  94 207  79 100  23  35]\n",
      " [ 35  15 128 107 347  95 105 129  21  18]\n",
      " [ 11   9 111 195  89 369  52 124  18  22]\n",
      " [ 12  27  82 143 109  88 451  40  15  33]\n",
      " [ 16  18  58 107  99 120  28 517   9  28]\n",
      " [143  95  33  28  29  17  11  14 575  55]\n",
      " [ 54 197  19  50  19  34  18  47  60 502]]\n",
      "Average Total Loss over Batches: 0.19698146067856812\n",
      "[[466  44  93  44  52  16  22  29 176  58]\n",
      " [ 51 598  16  16  15  11  26  23  73 171]\n",
      " [ 97  13 291 107 169  97  99  68  44  15]\n",
      " [ 29  27  83 305 105 177  93 105  33  43]\n",
      " [ 41  17 122  94 376  73 118 117  24  18]\n",
      " [ 16   9 106 196 100 341  65 119  21  27]\n",
      " [ 12  33  69 119 111  71 489  40  18  38]\n",
      " [ 28  17  56 108 112  96  34 493  10  46]\n",
      " [134  74  26  21  30  12  12  12 614  65]\n",
      " [ 50 170  18  33  19  25  16  43  58 568]]\n",
      "Average Total Loss over Batches: 0.19432870016859843\n",
      "[[493  43 105  43  54  14  15  36 145  52]\n",
      " [ 55 605  26  18  16   9  22  26  58 165]\n",
      " [ 94  15 304 116 155 107  83  71  36  19]\n",
      " [ 29  24 101 305  88 202  68 101  27  55]\n",
      " [ 37  13 137 102 363  93  95 121  17  22]\n",
      " [ 15   7 109 185  88 384  45 118  21  28]\n",
      " [ 13  27  82 127 133  88 433  47  11  39]\n",
      " [ 20  18  50 104 107 116  21 503   7  54]\n",
      " [165  76  35  26  33  16  11  19 550  69]\n",
      " [ 47 157  19  37  27  32  13  51  47 570]]\n",
      "Average Total Loss over Batches: 0.19475424309655093\n",
      "[[550  46 104  35  47  17  17  18 119  47]\n",
      " [ 58 641  23  17  14  10  24  19  59 135]\n",
      " [123  12 328  99 136 105  96  57  31  13]\n",
      " [ 39  26 126 287  97 192 109  63  26  35]\n",
      " [ 51  18 154  88 361  90 119  91  15  13]\n",
      " [ 20  11 132 167  95 373  73  93  16  20]\n",
      " [ 17  30 100 120 101  78 484  24  13  33]\n",
      " [ 39  15  78 108 110 124  37 436  10  43]\n",
      " [178  89  40  22  30  18  13  11 535  64]\n",
      " [ 68 199  22  34  24  33  24  38  51 507]]\n",
      "Average Total Loss over Batches: 0.18668207211084664\n",
      "[[481  37 100  53  66  16  23  30 154  40]\n",
      " [ 55 557  27  30  18   9  29  31  70 174]\n",
      " [ 89   7 305 120 172  98  93  71  31  14]\n",
      " [ 24  22  94 326  99 181 101  94  21  38]\n",
      " [ 32  11 124  96 379 100 106 118  22  12]\n",
      " [  8   5 111 195  97 370  66 114  18  16]\n",
      " [ 14  21  81 147 120  79 461  35  13  29]\n",
      " [ 24  13  58 114 117 113  36 477   7  41]\n",
      " [131  75  36  35  37  18  15  15 584  54]\n",
      " [ 54 151  22  45  29  33  16  52  63 535]]\n",
      "Average Total Loss over Batches: 0.1988591767323576\n",
      "[[513  40 119  28  48  13  19  24 159  37]\n",
      " [ 71 609  26  15  12   9  24  17  79 138]\n",
      " [106  14 370  86 121  96  96  59  41  11]\n",
      " [ 41  35 144 253  98 186  94  81  28  40]\n",
      " [ 47  15 186  72 350  78 113 103  23  13]\n",
      " [ 16   8 144 178  87 359  66 103  19  20]\n",
      " [ 18  34 115 100 109  77 466  30  19  32]\n",
      " [ 40  18  91  92 101 109  41 456  13  39]\n",
      " [155  87  33  18  27  17  11  10 595  47]\n",
      " [ 72 191  24  30  26  26  20  46  76 489]]\n",
      "Average Total Loss over Batches: 0.18817978635515087\n",
      "[[508  29 101  37  62  13  26  30 149  45]\n",
      " [ 76 520  29  18  32   9  47  22  66 181]\n",
      " [101   8 325  84 157  87 129  60  36  13]\n",
      " [ 32  17 118 247 121 165 148  83  29  40]\n",
      " [ 39   8 144  66 396  67 163  85  20  12]\n",
      " [ 17   4 145 172 102 311 116  94  20  19]\n",
      " [ 11  13  96  76 133  68 534  28  15  26]\n",
      " [ 28  13  84  89 131 103  54 444  10  44]\n",
      " [171  61  34  20  34  17  20  15 561  67]\n",
      " [ 60 142  21  33  34  28  36  49  55 542]]\n",
      "Average Total Loss over Batches: 0.19050475785157645\n",
      "[[512  40 116  37  43   8  13  28 166  37]\n",
      " [ 67 594  27  22  14  10  31  21  82 132]\n",
      " [ 99  10 342 107 139 105  78  63  41  16]\n",
      " [ 34  16 116 294  98 190  74 104  34  40]\n",
      " [ 43  14 180 100 334  86  89 114  29  11]\n",
      " [ 19   8 131 191  89 354  45 125  18  20]\n",
      " [ 15  28 112 127 112  86 436  39  18  27]\n",
      " [ 35  16  82 107 110 101  32 474   8  35]\n",
      " [139  72  43  27  25  15  13  15 605  46]\n",
      " [ 67 175  27  38  25  30  17  55  68 498]]\n",
      "Average Total Loss over Batches: 0.18260869101566263\n",
      "[[407  44 101  34  50  13  20  32 247  52]\n",
      " [ 39 619  17  15  10   8  20  15 102 155]\n",
      " [ 86  18 338 101 138  95  90  62  52  20]\n",
      " [ 35  27 113 297  82 160 107  85  41  53]\n",
      " [ 35  17 142  83 339  98 120 113  36  17]\n",
      " [ 17   9 121 185  85 354  70 111  25  23]\n",
      " [ 12  38  91 121  98  78 475  27  26  34]\n",
      " [ 30  22  68 103  97 103  32 481  14  50]\n",
      " [ 90  83  27  17  21  14  14   9 683  42]\n",
      " [ 51 195  18  35  14  22  20  33  93 519]]\n",
      "Average Total Loss over Batches: 0.18500840822651982\n",
      "[[473  44 115  41  67  19  21  31 137  52]\n",
      " [ 46 632  25  26  19  14  28  22  45 143]\n",
      " [ 73   9 325 103 163 110  90  76  32  19]\n",
      " [ 20  23 103 297 103 220  72 103  19  40]\n",
      " [ 30  12 127  83 393 107  99 115  20  14]\n",
      " [  8  10  99 173  95 390  53 132  16  24]\n",
      " [ 10  33  69 124 133  90 452  44  14  31]\n",
      " [ 16  19  57 111 104 113  25 508   2  45]\n",
      " [136  95  37  29  31  25  13  20 555  59]\n",
      " [ 45 180  22  40  27  33  16  55  44 538]]\n",
      "Average Total Loss over Batches: 0.18597799366444348\n",
      "[[497  43 103  33  45  13  15  26 177  48]\n",
      " [ 49 631  19  13  15  10  21  18  69 155]\n",
      " [105  12 338  84 134 108  96  58  45  20]\n",
      " [ 34  32 112 269 102 174 109  91  30  47]\n",
      " [ 33  21 160  77 345  90 113 110  29  22]\n",
      " [ 11  12 123 162  90 370  78 113  23  18]\n",
      " [ 16  43  93 103 104  76 487  29  18  31]\n",
      " [ 28  24  68  85 105 102  40 488  11  49]\n",
      " [129  93  20  18  13  16  14  13 623  61]\n",
      " [ 52 181  16  26  15  26  12  46  62 564]]\n",
      "Average Total Loss over Batches: 0.1856225778456591\n",
      "[[520  40  84  39  51  10  25  43 140  48]\n",
      " [ 64 589  25  21  17  11  41  23  61 148]\n",
      " [102  11 314 104 144 105  96  81  32  11]\n",
      " [ 32  21 103 295  94 188  99 103  24  41]\n",
      " [ 38  11 138  89 369  90 113 120  22  10]\n",
      " [ 17   9 123 179  87 353  73 119  18  22]\n",
      " [ 15  18  85 120 119  85 493  29  14  22]\n",
      " [ 25  14  64 101 109 111  35 502   3  36]\n",
      " [164  89  30  20  26  18  16  16 570  51]\n",
      " [ 63 158  24  48  26  34  24  54  55 514]]\n",
      "Average Total Loss over Batches: 0.17720188136114273\n",
      "[[527  36  85  44  56  14  22  27 158  31]\n",
      " [ 73 597  23  22  22  11  33  23  75 121]\n",
      " [109  13 291 110 191  89  95  59  34   9]\n",
      " [ 43  19 101 279 127 164 122  83  29  33]\n",
      " [ 36  11 138  90 414  75 108  96  21  11]\n",
      " [ 16   8 118 188 123 331  85  98  21  12]\n",
      " [ 16  16  93 118 133  67 490  34  13  20]\n",
      " [ 29  15  75  99 138 106  43 461   6  28]\n",
      " [153  85  25  28  31  14  16  14 598  36]\n",
      " [ 65 159  24  38  37  26  26  48  74 503]]\n",
      "Average Total Loss over Batches: 0.1828705333809089\n",
      "[[514  42 101  47  59  15  15  29 141  37]\n",
      " [ 52 587  29  32  25  10  34  22  63 146]\n",
      " [ 99  11 317 116 168 108  82  59  29  11]\n",
      " [ 26  20 108 322 102 183  84  98  25  32]\n",
      " [ 44  12 150  99 368  97 101 108  13   8]\n",
      " [ 16   9 121 186  91 375  53 111  20  18]\n",
      " [ 17  16 102 131 116  96 454  35  14  19]\n",
      " [ 21  12  66 114 102 114  35 491  10  35]\n",
      " [156  74  37  29  34  15  16  15 573  51]\n",
      " [ 64 146  22  47  29  32  23  55  58 524]]\n",
      "Average Total Loss over Batches: 0.18353089116610585\n",
      "[[541  42  92  49  51  19  16  27 137  26]\n",
      " [ 66 612  30  36  23  14  35  26  51 107]\n",
      " [111   9 301 127 149 112  88  69  28   6]\n",
      " [ 35  24  91 336  89 197  90  95  18  25]\n",
      " [ 46  13 153 111 328 110 105 115  12   7]\n",
      " [ 18   8 113 206  75 389  55 111  11  14]\n",
      " [ 16  21  99 146 109  91 452  38  13  15]\n",
      " [ 32  12  74 123  85 133  36 480   5  20]\n",
      " [191  88  33  37  31  18  12  15 529  46]\n",
      " [ 77 174  27  61  31  44  28  51  54 453]]\n",
      "Average Total Loss over Batches: 0.17563578363016247\n",
      "[[486  40 111  48  59  18  23  26 162  27]\n",
      " [ 60 570  40  31  31  11  34  22  76 125]\n",
      " [ 88   7 369 115 132  92  93  58  39   7]\n",
      " [ 30  17 136 310  92 192  98  77  24  24]\n",
      " [ 31  13 189  95 346  78 121  99  21   7]\n",
      " [ 13   4 153 186  89 361  66  98  19  11]\n",
      " [ 13  12 121 142 116  89 447  29  14  17]\n",
      " [ 25  10 100 113 107 118  34 464   8  21]\n",
      " [136  74  41  29  37  18  16  18 592  39]\n",
      " [ 58 155  33  52  33  40  32  57  73 467]]\n",
      "Average Total Loss over Batches: 0.18417613790338858\n",
      "[[499  46 100  34  52  14  18  28 172  37]\n",
      " [ 60 622  23  14  15  13  29  21  60 143]\n",
      " [112  14 321  72 134 108 108  78  38  15]\n",
      " [ 39  26 104 272  89 178 110  91  38  53]\n",
      " [ 46  16 141  70 349  87 147 107  20  17]\n",
      " [ 21  12 112 167  85 358  84 115  21  25]\n",
      " [ 19  31  73  99 108  71 505  42  19  33]\n",
      " [ 35  18  70  91  98 110  44 480  13  41]\n",
      " [152  76  38  18  23  15  11  18 602  47]\n",
      " [ 59 182  17  26  21  25  26  51  60 533]]\n",
      "Average Total Loss over Batches: 0.17390472279427574\n",
      "[[490  37 106  48  62  16  22  36 128  55]\n",
      " [ 45 581  26  35  23  12  40  26  54 158]\n",
      " [ 89   9 314 118 148 103  98  82  27  12]\n",
      " [ 26  15  91 325 105 195  84 101  20  38]\n",
      " [ 31  12 131  98 383  82 118 117  14  14]\n",
      " [ 12   7 110 208  91 362  56 121  14  19]\n",
      " [ 11  18  84 130 115  91 478  38  11  24]\n",
      " [ 17  12  61 108 106 125  29 511   4  27]\n",
      " [164  79  40  36  31  20  19  20 536  55]\n",
      " [ 52 142  22  57  26  41  21  57  47 535]]\n",
      "Average Total Loss over Batches: 0.18342034621859435\n",
      "[[515  50  86  43  49  14  21  29 160  33]\n",
      " [ 59 636  25  27  14   9  31  18  69 112]\n",
      " [109  11 301 116 135 112  95  68  44   9]\n",
      " [ 41  23  86 314  85 199  95  92  30  35]\n",
      " [ 42  18 135 102 355  95 110 107  23  13]\n",
      " [ 25   9 118 183  81 379  59 111  21  14]\n",
      " [ 13  25  88 128 111  86 473  31  20  25]\n",
      " [ 36  17  66 116 104 119  35 473   7  27]\n",
      " [145  84  32  24  26  18  16  10 606  39]\n",
      " [ 67 189  24  45  24  35  25  50  68 473]]\n",
      "Average Total Loss over Batches: 0.17404897492066027\n",
      "[[533  38  97  42  52  16  14  24 144  40]\n",
      " [ 61 596  26  29  23   8  24  21  67 145]\n",
      " [122  16 330 103 148  98  76  58  36  13]\n",
      " [ 40  23 119 301  97 181  84  82  29  44]\n",
      " [ 50  16 162  81 358  84 100 114  19  16]\n",
      " [ 20  12 126 191  85 363  60  99  20  24]\n",
      " [ 20  26 103 119 116  90 443  39  18  26]\n",
      " [ 36  17  78 112 101 107  32 467   7  43]\n",
      " [174  77  33  22  33  17  14  11 571  48]\n",
      " [ 68 171  19  40  26  30  26  45  65 510]]\n",
      "Average Total Loss over Batches: 0.18091016467103735\n",
      "[[516  37  81  36  52  17  20  20 187  34]\n",
      " [ 72 585  28  21  18  14  31  13  71 147]\n",
      " [116  11 331 101 130 103  98  49  51  10]\n",
      " [ 36  21 105 294 103 173 121  80  32  35]\n",
      " [ 48  10 165  88 343  88 138  80  24  16]\n",
      " [ 27  10 124 190  81 346  94  90  25  13]\n",
      " [ 18  18  96 110 106  77 514  23  20  18]\n",
      " [ 37  12  82 100 120 107  51 447  14  30]\n",
      " [153  71  31  24  27  13  16   9 614  42]\n",
      " [ 73 163  19  37  26  35  29  50  78 490]]\n",
      "Average Total Loss over Batches: 0.17346448198501022\n",
      "[[560  32  89  50  34  19  19  30 136  31]\n",
      " [ 73 578  32  36  22  14  41  23  67 114]\n",
      " [108   7 338 119 121 107  97  56  40   7]\n",
      " [ 42  20 111 315  89 180 120  71  25  27]\n",
      " [ 51  11 165 100 323  85 126 108  19  12]\n",
      " [ 12   6 119 186  80 382  84 101  21   9]\n",
      " [ 16  16 103 129 100  78 489  32  16  21]\n",
      " [ 39  11  77 121  93 126  43 463   7  20]\n",
      " [176  83  37  35  18  15  15  14 569  38]\n",
      " [ 73 162  24  46  27  41  39  49  57 482]]\n",
      "Average Total Loss over Batches: 0.16924689058079267\n",
      "[[430  46 122  49  71  18  31  33 154  46]\n",
      " [ 34 616  30  28  18  12  41  23  43 155]\n",
      " [ 69  11 327 107 153 104 115  72  28  14]\n",
      " [ 24  17  95 303  99 182 133  88  18  41]\n",
      " [ 32  15 126  80 371 100 142 103  18  13]\n",
      " [ 10   6 103 184  94 370  94 106  18  15]\n",
      " [ 10  20  84 114  94  79 531  30   9  29]\n",
      " [ 19  12  70 104 121 119  51 466   5  33]\n",
      " [120 101  66  40  29  17  20  18 534  55]\n",
      " [ 43 166  24  43  30  42  35  55  44 518]]\n",
      "Average Total Loss over Batches: 0.17339192814073526\n",
      "[[526  44 101  34  59  10  23  24 139  40]\n",
      " [ 61 605  25  21  13   8  45  17  67 138]\n",
      " [116  11 326  89 139  85 120  66  36  12]\n",
      " [ 47  22  99 277 107 143 141  90  30  44]\n",
      " [ 49  15 140  72 366  69 142 108  22  17]\n",
      " [ 20  10 134 176  98 317 104 104  21  16]\n",
      " [ 13  29  82 100  98  57 541  31  17  32]\n",
      " [ 38  14  81 104 104  92  52 465  10  40]\n",
      " [177  80  34  20  28  13  18  12 565  53]\n",
      " [ 71 171  22  37  24  30  32  43  59 511]]\n",
      "Average Total Loss over Batches: 0.16901392387248576\n",
      "[[485  41  77  38  45  16  17  25 206  50]\n",
      " [ 58 569  13  17  15   7  22  19  87 193]\n",
      " [119  10 288  96 148 106  92  69  52  20]\n",
      " [ 42  29  88 269  90 172 109 102  37  62]\n",
      " [ 46  17 125  79 359  93 119 110  27  25]\n",
      " [ 21  10 107 179  91 356  64 112  29  31]\n",
      " [ 15  34  93 103 111  80 464  39  24  37]\n",
      " [ 35  19  66 101 104 102  42 465  15  51]\n",
      " [123  76  22  18  15  13  11  14 645  63]\n",
      " [ 57 153  18  27  20  26  22  45  68 564]]\n",
      "Average Total Loss over Batches: 0.1719981557827443\n",
      "[[483  58  74  33  57  17  31  34 146  67]\n",
      " [ 40 610  11  20  20   9  30  20  49 191]\n",
      " [112  17 262 104 157 102  99  81  46  20]\n",
      " [ 35  31  75 279 105 170 116  97  34  58]\n",
      " [ 34  19 113  82 374  87 130 112  24  25]\n",
      " [ 19  14  91 182  95 354  74 116  19  36]\n",
      " [ 12  34  68 105 112  71 504  38  16  40]\n",
      " [ 30  25  50  94 108 106  38 492   6  51]\n",
      " [143  98  25  27  29  11  16  14 554  83]\n",
      " [ 47 184  12  32  20  25  19  39  46 576]]\n",
      "Average Total Loss over Batches: 0.17112719639655202\n",
      "[[505  38 100  53  41  19  18  36 137  53]\n",
      " [ 55 569  18  26  15  11  38  26  58 184]\n",
      " [102   6 310 123 127  92  93  94  33  20]\n",
      " [ 30  18  96 340  74 176  86 113  24  43]\n",
      " [ 34  13 139 113 336  86 104 134  21  20]\n",
      " [ 17   7  99 202  74 366  55 134  17  29]\n",
      " [ 12  21  77 153  96  71 467  52  15  36]\n",
      " [ 25  10  44 118  88 110  36 518  10  41]\n",
      " [163  71  36  34  24  10  19  19 547  77]\n",
      " [ 41 148  19  55  20  30  13  51  47 576]]\n",
      "Average Total Loss over Batches: 0.1760783395530563\n",
      "[[525  35  73  37  51  13  14  27 196  29]\n",
      " [ 81 588  17  21  17   5  22  18  91 140]\n",
      " [134  13 291  98 151  84  86  74  54  15]\n",
      " [ 51  26 104 280 120 145 105  86  36  47]\n",
      " [ 57  16 138  75 374  73 112 107  31  17]\n",
      " [ 31   8 111 194 105 317  75 112  23  24]\n",
      " [ 24  27  83 112 140  67 462  34  24  27]\n",
      " [ 50  21  67  95 120  91  40 458  17  41]\n",
      " [149  81  27  23  25   9  12  11 624  39]\n",
      " [ 83 172  20  31  22  25  21  44  80 502]]\n",
      "Average Total Loss over Batches: 0.1745740616133064\n",
      "[[452  37 114  38  60  15  25  34 183  42]\n",
      " [ 57 565  27  19  16  10  34  22  75 175]\n",
      " [ 93   8 349  90 147  99  88  74  34  18]\n",
      " [ 31  18 111 277 105 178 109 105  26  40]\n",
      " [ 38  10 159  73 359  82 118 117  25  19]\n",
      " [ 17   8 129 168  88 354  81 115  18  22]\n",
      " [ 13  17  83 106 107  80 505  43  16  30]\n",
      " [ 33  13  68  93 104 117  40 485  10  37]\n",
      " [125  74  40  22  25  13  19  15 605  62]\n",
      " [ 64 142  24  40  23  26  28  56  61 536]]\n",
      "Average Total Loss over Batches: 0.17464980991325807\n",
      "[[451  45  98  55  52  18  18  33 158  72]\n",
      " [ 39 549  14  28  12  13  28  23  70 224]\n",
      " [ 81  11 308 120 129 116  89  83  41  22]\n",
      " [ 29  15  98 304  75 206  75 111  25  62]\n",
      " [ 33  16 143 100 331 111 104 121  16  25]\n",
      " [ 15   8  97 188  80 408  47 114  17  26]\n",
      " [ 12  18  89 132  96 101 449  38  16  49]\n",
      " [ 25  13  53 119  77 128  34 490  10  51]\n",
      " [119  80  35  29  22  17  13  20 580  85]\n",
      " [ 38 130  15  41  14  37  18  49  47 611]]\n",
      "Average Total Loss over Batches: 0.17208288169034758\n",
      "[[472  56 107  39  65  21  17  26 141  56]\n",
      " [ 48 620  16  23   8  12  40  17  38 178]\n",
      " [104  17 299 104 134 118 109  63  30  22]\n",
      " [ 31  26  84 293  88 205 107  91  22  53]\n",
      " [ 40  17 121  90 354  99 135 108  11  25]\n",
      " [ 14   9  89 177  80 423  67  97  15  29]\n",
      " [ 14  31  69 115 101  83 498  37  12  40]\n",
      " [ 32  17  54  99  99 136  42 470   9  42]\n",
      " [142 102  36  26  31  18  18  16 535  76]\n",
      " [ 51 169  18  36  19  32  22  42  40 571]]\n",
      "Average Total Loss over Batches: 0.15946351456316188\n",
      "[[521  56  80  43  45  16  24  33 142  40]\n",
      " [ 55 643  19  19  14   9  32  21  48 140]\n",
      " [118  15 282  94 145 102 115  81  33  15]\n",
      " [ 39  30  86 271  98 179 123  96  28  50]\n",
      " [ 44  18 137  75 356  85 138 112  20  15]\n",
      " [ 25  11 103 172  98 350  85 112  21  23]\n",
      " [ 18  35  82 100  99  66 524  32  15  29]\n",
      " [ 38  17  55  93 102 101  43 503   9  39]\n",
      " [182 102  28  20  19  12  14  16 549  58]\n",
      " [ 64 183  19  33  19  29  28  52  54 519]]\n",
      "Average Total Loss over Batches: 0.16514549333865755\n",
      "[[468  53 100  46  59  18  20  33 152  51]\n",
      " [ 56 597  16  17  18  11  24  27  66 168]\n",
      " [ 90  17 317 102 144 105  91  75  42  17]\n",
      " [ 31  21 104 289  88 191 104  99  29  44]\n",
      " [ 33  17 138  89 353  86 117 128  19  20]\n",
      " [ 14   7 109 199  80 365  68 119  19  20]\n",
      " [ 13  27  83 135 110  73 464  49  14  32]\n",
      " [ 31  15  61 115  97 114  37 485   8  37]\n",
      " [132  96  34  29  24  13  11  21 578  62]\n",
      " [ 49 177  17  37  21  30  21  48  51 549]]\n",
      "Average Total Loss over Batches: 0.1650735733505222\n",
      "[[492  34  86  34  45  19  19  28 192  51]\n",
      " [ 60 589  18  18  18  13  27  25  73 159]\n",
      " [116  14 289  92 143  98 106  74  51  17]\n",
      " [ 36  28  90 269  90 156 151  93  43  44]\n",
      " [ 45  18 127  83 349  83 125 119  27  24]\n",
      " [ 29  12 120 176  91 307 101 111  29  24]\n",
      " [ 20  27  78 104 115  76 485  40  23  32]\n",
      " [ 43  17  60  97 104  99  50 471  15  44]\n",
      " [131  85  23  20  18  13  14  14 627  55]\n",
      " [ 73 165  19  23  22  25  27  40  63 543]]\n",
      "Average Total Loss over Batches: 0.16815137710006442\n",
      "[[445  56  89  56  63  14  15  28 186  48]\n",
      " [ 40 606  18  21  20  10  34  19  74 158]\n",
      " [ 98  13 300 106 149 105 100  71  45  13]\n",
      " [ 30  32  97 286 100 181 106  89  37  42]\n",
      " [ 28  15 137  92 352  93 134 107  25  17]\n",
      " [ 12   9 119 190  87 357  82  99  25  20]\n",
      " [ 17  30  81 115 110  92 472  37  18  28]\n",
      " [ 26  19  68 115  93 117  44 463  11  44]\n",
      " [116  85  29  25  26  16  12  18 621  52]\n",
      " [ 61 186  14  34  24  30  29  43  68 511]]\n",
      "Average Total Loss over Batches: 0.16640327414738015\n",
      "[[537  45  78  42  50  14  13  33 151  37]\n",
      " [ 64 610  29  24  16  14  23  23  63 134]\n",
      " [117  11 301 134 139  95  76  79  33  15]\n",
      " [ 36  21  92 335  82 191  79  95  31  38]\n",
      " [ 46  11 149 112 326  80 103 131  20  22]\n",
      " [ 20  12 102 225  89 335  58 116  20  23]\n",
      " [ 19  26  88 137 113  84 455  39  13  26]\n",
      " [ 36  18  63 120  91 102  26 502   7  35]\n",
      " [169  78  32  25  25  14  15  23 569  50]\n",
      " [ 61 173  24  43  22  37  26  55  51 508]]\n",
      "Average Total Loss over Batches: 0.16184061018537263\n",
      "[[498  51  86  38  50  14  25  22 171  45]\n",
      " [ 61 632  21  13  10  11  28  16  71 137]\n",
      " [111  11 310 103 144  93 105  68  41  14]\n",
      " [ 39  25 101 278  94 184 120  85  32  42]\n",
      " [ 42  16 134  81 357  83 131 109  25  22]\n",
      " [ 21   9 107 172  88 371  75 113  23  21]\n",
      " [ 15  29  82 103 106  74 509  29  22  31]\n",
      " [ 33  20  79 106 100 107  40 458   8  49]\n",
      " [129  92  23  19  18  12  15  14 624  54]\n",
      " [ 62 180  19  36  19  28  30  42  64 520]]\n",
      "Average Total Loss over Batches: 0.16339475291413488\n",
      "[[513  33 108  37  64  16  15  27 152  35]\n",
      " [102 512  40  31  34  10  21  31  78 141]\n",
      " [105   7 317 101 199  78  71  71  40  11]\n",
      " [ 48  15 121 283 121 161  76 106  30  39]\n",
      " [ 41   6 160  82 406  65  95 108  21  16]\n",
      " [ 26   8 141 200 113 301  50 122  25  14]\n",
      " [ 21   9 107 117 170  70 430  42  21  13]\n",
      " [ 38  11  97  98 125  86  29 471  12  33]\n",
      " [167  69  46  24  35  18   9  22 572  38]\n",
      " [ 85 119  32  49  41  27  22  72  70 483]]\n",
      "Average Total Loss over Batches: 0.16414407589905897\n",
      "[[484  46  82  53  72  19  20  22 157  45]\n",
      " [ 51 588  20  34  25  13  31  23  56 159]\n",
      " [ 90  11 265 125 191 127  84  62  28  17]\n",
      " [ 27  25  78 330 102 206  87  86  20  39]\n",
      " [ 38  13 104  92 408 105 103 104  16  17]\n",
      " [ 15   7  90 202  97 387  58 108  17  19]\n",
      " [ 13  19  75 139 124  98 460  30  14  28]\n",
      " [ 24  16  56 106 112 122  34 478   8  44]\n",
      " [149  76  29  35  29  23  15  15 564  65]\n",
      " [ 47 153  15  50  22  40  23  48  49 553]]\n",
      "Average Total Loss over Batches: 0.1668221813706495\n",
      "[[507  36  98  46  48  15  20  18 177  35]\n",
      " [ 81 561  28  28  17  13  32  16  91 133]\n",
      " [117   9 310 115 136 107  94  56  41  15]\n",
      " [ 44  26 103 308 100 185  95  65  38  36]\n",
      " [ 45  15 162  93 341  98 114  95  22  15]\n",
      " [ 26   7 133 183  85 356  73  97  26  14]\n",
      " [ 22  15 103 127 117  90 450  24  26  26]\n",
      " [ 46  15  86 119 102 112  47 429  13  31]\n",
      " [149  75  30  23  19  13  14  17 615  45]\n",
      " [ 75 155  23  43  23  35  23  34  87 502]]\n",
      "1870.4442687034607 sec\n",
      "or roughly 31.0 min\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "startTime = time.time()\n",
    "\n",
    "for epochs in range(100):\n",
    "  total_loss = 0\n",
    "  for batch in range( train_X.shape[0] // batch_size ):\n",
    "    x_batch, y_batch = get_batch(train_X, train_Y, batch_size)\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "    logits = cifar_model( x_batch )\n",
    "    loss = loss_function( logits, y_batch )\n",
    "\n",
    "    loss.backward()\n",
    "    cnn_optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  print( \"Average Total Loss over Batches:\", total_loss / ( train_X.shape[0] // batch_size ) )\n",
    "  print( confusion_matrix( cifar_model, test_X, test_Y ) )\n",
    "endTime = time.time()\n",
    "print(str(endTime - startTime) + \" sec\")\n",
    "print(\"or roughly \" + str((endTime - startTime)//60) + \" min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlMvskNtGWDq"
   },
   "source": [
    "Similar results - but much faster."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
