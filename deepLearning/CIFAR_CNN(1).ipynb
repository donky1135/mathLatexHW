{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3MxHWiEHnYqW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivIpm9qZ3V9y",
    "outputId": "bfaea4f8-83d3-425c-ce61-a5bfa541bbd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaxI5PV_4K_x"
   },
   "source": [
    "In the above, I'm downloading the CIFAR data set. This is a set of 32x32 pixel RGB images, belonging to the 10 classes indicated. The ToTensor() transformation here is necessary to convert the original data from image objects to numerical arrays that we can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMwhxL9bo74s",
    "outputId": "1689a9eb-41e9-4428-8f73-4ab0c8ecf330"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKDU8rSm4ZPN"
   },
   "source": [
    "The training data set consists of 50,000 32x32 images, each pixel consisting of three values (RGB). The three values are generally referred to as the `channels' of the data. A black and white image could be thought of as having a single channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXGkny55pG_1",
    "outputId": "0d9c1894-a56b-4931-9696-c22d72652822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 9, 9, 4, 1, 1, 2, 7, 8, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.targets[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63i86Q2B4nsS"
   },
   "source": [
    "The targets, the first 10 of which are shown here, are class labels, 0 through 9, corresponding to the indices in the classes list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "BgIVy_x1pMhZ",
    "outputId": "b7f1cc3d-f405-41b1-9697-151bcca4c0fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frog'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[ trainset.targets[0] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOH2fPpP4tw4"
   },
   "source": [
    "We see that the first image in the training set is labeled as a frog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "AaUxu6WrpUH1",
    "outputId": "3ed0fce2-709f-4628-d502-3642594d8275"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 59,  62,  63],\n",
       "        [ 43,  46,  45],\n",
       "        [ 50,  48,  43],\n",
       "        ...,\n",
       "        [158, 132, 108],\n",
       "        [152, 125, 102],\n",
       "        [148, 124, 103]],\n",
       "\n",
       "       [[ 16,  20,  20],\n",
       "        [  0,   0,   0],\n",
       "        [ 18,   8,   0],\n",
       "        ...,\n",
       "        [123,  88,  55],\n",
       "        [119,  83,  50],\n",
       "        [122,  87,  57]],\n",
       "\n",
       "       [[ 25,  24,  21],\n",
       "        [ 16,   7,   0],\n",
       "        [ 49,  27,   8],\n",
       "        ...,\n",
       "        [118,  84,  50],\n",
       "        [120,  84,  50],\n",
       "        [109,  73,  42]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[208, 170,  96],\n",
       "        [201, 153,  34],\n",
       "        [198, 161,  26],\n",
       "        ...,\n",
       "        [160, 133,  70],\n",
       "        [ 56,  31,   7],\n",
       "        [ 53,  34,  20]],\n",
       "\n",
       "       [[180, 139,  96],\n",
       "        [173, 123,  42],\n",
       "        [186, 144,  30],\n",
       "        ...,\n",
       "        [184, 148,  94],\n",
       "        [ 97,  62,  34],\n",
       "        [ 83,  53,  34]],\n",
       "\n",
       "       [[177, 144, 116],\n",
       "        [168, 129,  94],\n",
       "        [179, 142,  87],\n",
       "        ...,\n",
       "        [216, 184, 140],\n",
       "        [151, 118,  84],\n",
       "        [123,  92,  72]]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BniQOZ764yx1"
   },
   "source": [
    "COLAB prints out a tiny thumbnail here, which I think is interesting, but more importantly we see that the image data is stored as a numpy array, 32x32x3, where each value is an integer between 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "7AlD1gPHpVZ-",
    "outputId": "97054520-8c90-4491-a57a-1e697db1b0dd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow( trainset.data[0] / 256 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bamkT4q5CBc"
   },
   "source": [
    "In the following code block, I prepare the data for use - the first step is to scale all the pixel values to be between 0 and 1, by dividing by 255. Next, I subtract off 0.5, so that pixel values are centered [-0.5, 0.5] rather than [0, 255] as before.\n",
    "\n",
    "Two important technical notes here:\n",
    "\n",
    "First, the implementation of convolutional layers in PyTorch expects the data to be presented 'channel first', that is a tensor of data should be of the form [ batch_size, num channels, height, width ]. As such, I use the permute function here, to make the channel dimension the second dimension (placing dimension 3 in dimension 1), and bump over the remaining dimensions. The result of this will be, in the case of the training data, a block of size [50000,3,32,32].\n",
    "\n",
    "Second, apparently the crossEntropyLoss method expects class labels to come in the form of long data types, so I transform the data labels as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3l8wu6ZleJo",
    "outputId": "568b6ff1-f51b-42fd-ea27-f36c8a316f0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32])\n",
      "torch.Size([10000, 3, 32, 32])\n",
      "torch.Size([50000])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "train_X = torch.Tensor( trainset.data/255.0 - 0.5 )\n",
    "train_X = train_X.permute( 0, 3, 1, 2 )\n",
    "test_X = torch.Tensor( testset.data/255.0 - 0.5 )\n",
    "test_X = test_X.permute( 0, 3, 1, 2 )\n",
    "\n",
    "train_Y = torch.Tensor( np.asarray( trainset.targets ) ).long()\n",
    "#train_Y = train_Y.reshape( (-1,1) )\n",
    "test_Y = torch.Tensor( np.asarray( testset.targets ) ).long()\n",
    "#test_Y = test_Y.reshape( (-1,1) )\n",
    "\n",
    "print( train_X.shape )\n",
    "print( test_X.shape )\n",
    "print( train_Y.shape )\n",
    "print( test_Y.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDj36Is157TQ"
   },
   "source": [
    "The following code simply serves to remove a random batch from the specified data set, as we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iHA-uU_znf2",
    "outputId": "3bc89dfb-61ff-47b7-fd9b-515af9494ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(x, y, batch_size):\n",
    "  n = x.shape[0]\n",
    "\n",
    "  batch_indices = random.sample( [ i for i in range(n) ], k = batch_size )\n",
    "\n",
    "  x_batch = x[ batch_indices ]\n",
    "  y_batch = y[ batch_indices ]\n",
    "\n",
    "  return x_batch, y_batch\n",
    "\n",
    "batch_x, batch_y = get_batch( train_X, train_Y, batch_size = 4 )\n",
    "print( batch_x.shape )\n",
    "print( batch_y.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-XnMf4s6DCL"
   },
   "source": [
    "To demonstrate a convolutional layer, I'm going to create a 2D convolutional layer object. I'm specifying that the input will have 3 channels (RGB). The output will have 10 channels (that is, I will be calculating 10 features every time the convolutional window is applied). The local feature will be of size 3x3 (it doesn't have to be square, but the default assumption is square), and the stride is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpJ39WOE0Rvj"
   },
   "outputs": [],
   "source": [
    "conv_test_layer = nn.Conv2d(in_channels = 3, out_channels = 10, kernel_size = 3, stride = 1, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EU52U4Er6XIs"
   },
   "source": [
    "Note that applying this convolutional layer to a input batch of 32x32, there are 30 places that a 3x3 feature window can be applied horizontally, at stride 1.\n",
    "\n",
    "Similarly, there are 30 places a 3x3 feature window can be applied horizontally, at stride 1.\n",
    "\n",
    "At each of these 30x30 locations, this layer will be computing 10 features. So the output of this layer should be of size 10x30x30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fo5Fmk-k00mF"
   },
   "outputs": [],
   "source": [
    "convolved_batch = conv_test_layer( batch_x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICnINTiD04Hr",
    "outputId": "52275512-d61a-47fd-bc58-94d1fbb1e75d"
   },
   "outputs": [],
   "source": [
    "convolved_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVnxcxGb6y-l"
   },
   "source": [
    "This is exactly as we expect.\n",
    "\n",
    "What do the internal parameters of a convolutional layer look like?\n",
    "\n",
    "Note that when we compute a feature of a 3x3 window, that is of 9 pixels, each one with three values. So a single feature will require 3\\*3\\*3 weights, with a single bias value.\n",
    "\n",
    "If we want to do this 10 times (for 10 total features), we need an arrangement of 10x3x3x3 weights, and 10 bias values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WZv6YmJ05hM",
    "outputId": "e5086f03-9cc5-483b-b235-f31a3ccd874e"
   },
   "outputs": [],
   "source": [
    "print( conv_test_layer.weight.shape )\n",
    "print( conv_test_layer.bias.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3umk3hDD7XkS"
   },
   "source": [
    "This is exactly as we expect.\n",
    "\n",
    "Note that `under the hood', the convolutional layer is applying this block of weights iteratively, at each location it is computing a local feature (according to the stride)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP6BjfEQ7mGg"
   },
   "source": [
    "We can in fact stack convolutional layers.\n",
    "\n",
    "The output of the first test layer is 10x30x30. We could pass this through another convolutional layer. Here I specify 10 input features or channels at each location, we want to compute 5 output features, and the kernel or feature window will again be 3x3, with a stride of 1.\n",
    "\n",
    "For a 30x30 field of view, and a 3x3 feature window at stride 1, there are 28x28 possible locations to apply that feature window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcYNfIGaGx_m"
   },
   "outputs": [],
   "source": [
    "conv_test_layer_2 = nn.Conv2d(in_channels = 10, out_channels = 5, kernel_size = 3, stride = 1, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vgciZnLG3cw",
    "outputId": "09443b3b-ebbc-4a62-aec0-af76157bfc28"
   },
   "outputs": [],
   "source": [
    "conv_test_layer_2( convolved_batch ).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv1NzsFQ8AF0"
   },
   "source": [
    "We see that for each of the 4 images in the batch, we are computing an output of size 5 (features) x 28 (local field height) x 28 (local field width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GkQclHkF1Dig"
   },
   "outputs": [],
   "source": [
    "class CIFARModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CIFARModel, self).__init__()\n",
    "\n",
    "    self.conv_layer_1 = nn.Conv2d(in_channels = 3, out_channels = 5, kernel_size = 3, stride = 1, bias=True)\n",
    "    self.conv_layer_2 = nn.Conv2d(in_channels = 5, out_channels = 10, kernel_size = 3, stride = 1, bias=True)\n",
    "    self.conv_layer_3 = nn.Conv2d(in_channels = 10, out_channels = 15, kernel_size = 3, stride = 1, bias=True)\n",
    "\n",
    "    self.linear_layer = torch.nn.Linear( in_features = 15*26*26, out_features = 10, bias=True )\n",
    "    # Note that the output of the last convolutional layer will be 15x26x16 - why?\n",
    "    # So we want to input 15*26*26 values into the last layer, and get 10 output values out (for the class probabilities)\n",
    "\n",
    "  def forward(self, input_tensor):\n",
    "    output = self.conv_layer_1( input_tensor )\n",
    "    output = nn.Sigmoid()( output )\n",
    "    output = self.conv_layer_2( output )\n",
    "    output = nn.Sigmoid()( output )\n",
    "    output = self.conv_layer_3( output )\n",
    "    output = nn.Sigmoid()( output )\n",
    "\n",
    "    # At this point, the block of node values from the convolutional layer is flattened\n",
    "    # So that it can be passed into a standard linear layer\n",
    "    output = nn.Flatten()( output )\n",
    "    output = self.linear_layer( output )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8HdWU1B88zo"
   },
   "source": [
    "We can initialize and view this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VgmewIe1lSp",
    "outputId": "67a29238-63af-46ab-e499-63ab6d87430d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFARModel(\n",
      "  (conv_layer_1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv_layer_2): Conv2d(5, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv_layer_3): Conv2d(10, 15, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (linear_layer): Linear(in_features=10140, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cifar_model = CIFARModel()\n",
    "print( cifar_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKYeu3Ri9Meb"
   },
   "source": [
    "We can use the standard confusion matrix to get a sense of how good the initial random model is, and see how that changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "EpGDcyOz9Q-U"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix( model, x, y ):\n",
    "  identification_counts = np.zeros( shape = (10,10), dtype = np.int32 )\n",
    "\n",
    "  logits = model( x )\n",
    "  predicted_classes = torch.argmax( logits, dim = 1 )\n",
    "\n",
    "  n = x.shape[0]\n",
    "\n",
    "  for i in range(n):\n",
    "    actual_class = int( y[i].item() )\n",
    "    predicted_class = predicted_classes[i].item()\n",
    "    identification_counts[actual_class, predicted_class] += 1\n",
    "\n",
    "  return identification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BwNlcq7_-ZSu",
    "outputId": "1e481252-4e0c-4179-bf98-47db88d0573d"
   },
   "outputs": [],
   "source": [
    "confusion_matrix( cifar_model, test_X, test_Y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZdB75lOI1sVg"
   },
   "outputs": [],
   "source": [
    "cnn_optimizer = optim.Adam(cifar_model.parameters(), lr = 0.01 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5_uWsb1V1xZg"
   },
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ya7iliGm136x",
    "outputId": "62fa21e3-ee6c-48c6-fbcf-b7be80cdc656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Test Loss: 2.342569351196289\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Test Loss:\", loss_function( cifar_model( test_X ), test_Y ).item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-dEK9Sr2BL6",
    "outputId": "3d527766-7950-4052-8724-9bb6ddf829fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Total Loss over Batches: 2.1020022275209427\n",
      "[[470 159  16  27  25  11  19  37 136 100]\n",
      " [ 58 569  15  30  30  22  24  29  57 166]\n",
      " [ 89  76  83  72 160  34 340  63  36  47]\n",
      " [ 47  82  40 163 137  58 305  90  29  49]\n",
      " [ 53  42  28  54 173  25 500  70  17  38]\n",
      " [ 40 141  50 119 157  77 266  80  32  38]\n",
      " [ 19  40   8  80 102  18 630  85   4  14]\n",
      " [ 29 118  18  66 145  19 186 306  16  97]\n",
      " [256 170   7  33   9   5  12  26 353 129]\n",
      " [ 63 212  11  49  53  15  27  93  79 398]]\n",
      "Average Total Loss over Batches: 1.7005827948856354\n",
      "[[417  29  54  41  43  19  36  81 128 152]\n",
      " [  7 406   3  27   5  14  44  37  49 408]\n",
      " [ 36   8 137  71 204  76 257 158  18  35]\n",
      " [  4   7  25 192  98 113 340 176   5  40]\n",
      " [ 20   4  30  70 295  45 345 168   3  20]\n",
      " [  4   3  34 124 115 214 303 181   3  19]\n",
      " [  1   4  16  49  36  39 685 145   1  24]\n",
      " [ 11   3  16  69  78  37 181 564   2  39]\n",
      " [181  51  27  30  16  14  16  36 418 211]\n",
      " [ 21  79  13  37   7  16  57  96  39 635]]\n",
      "Average Total Loss over Batches: 1.5079046415948867\n",
      "[[468  39  32  37  30  23  47  34 209  81]\n",
      " [ 20 644   2  23   6   9  35  14  46 201]\n",
      " [ 63  11 154 118 186 144 170  97  39  18]\n",
      " [ 21  13  26 323  52 226 197  86  10  46]\n",
      " [ 34   8  27 122 306 141 213 123   8  18]\n",
      " [ 11  13  38 189  54 441 120 106   8  20]\n",
      " [  2  10  11 120  46  93 645  54   4  15]\n",
      " [ 15   8  13 125  57 142  95 482   7  56]\n",
      " [129  90  11  30  13  13  28  17 583  86]\n",
      " [ 37 168  10  45   3  17  54  51  54 561]]\n",
      "Average Total Loss over Batches: 1.3770299858850241\n",
      "[[523  53  56  27  40  21  25  41 141  73]\n",
      " [ 31 717   1  18   2  12  25  10  50 134]\n",
      " [ 73  15 168 114 271 111 104  95  25  24]\n",
      " [ 12  15  40 327 116 180 137 118  11  44]\n",
      " [ 36  10  34  90 473  63 136 140   9   9]\n",
      " [ 10  19  26 194  97 389  91 149   8  17]\n",
      " [  2  11  16  94 111  80 602  60   4  20]\n",
      " [ 15   9  10  92  97  92  45 593   3  44]\n",
      " [138  98  11  31   8   8  17  26 615  48]\n",
      " [ 45 171   6  35  11  15  31  47  59 580]]\n",
      "Average Total Loss over Batches: 1.2561744954645633\n",
      "[[534  29 127  23  45  25  32  28 125  32]\n",
      " [ 36 674  13  36  13  22  42  16  56  92]\n",
      " [ 56   1 401  74 196  85 114  46  17  10]\n",
      " [ 17   8 107 272 138 197 188  56   6  11]\n",
      " [ 19   9 123  59 511  60 130  81   6   2]\n",
      " [  6   4  78 147 109 420 130  94   5   7]\n",
      " [  1   8  70  78 122  29 661  26   2   3]\n",
      " [ 11   3  36  72 153 111  55 541   6  12]\n",
      " [114  68  45  31  11  11  30  25 638  27]\n",
      " [ 55 153  21  49  30  26  67  60  62 477]]\n",
      "Average Total Loss over Batches: 1.1634012487575411\n",
      "[[511  45  92  28  30  21  14  17 186  56]\n",
      " [ 28 638   3  18   6  14  18  11  69 195]\n",
      " [ 62   7 320  90 217 127  74  40  39  24]\n",
      " [ 17  18  77 352 113 226  73  59  27  38]\n",
      " [ 19   7  85 100 516  83  64  97  18  11]\n",
      " [ 10   7  54 180  68 499  46  92  16  28]\n",
      " [  6  20  64 128 158  62 500  28  11  23]\n",
      " [ 12   7  41  95  89 138  18 542   6  52]\n",
      " [ 73  70  12  20   6  10   6  14 731  58]\n",
      " [ 30 132   7  24  14  25  19  38  69 642]]\n",
      "Average Total Loss over Batches: 1.0816368834769725\n",
      "[[554  41 105  26  18  21  24  11 162  38]\n",
      " [ 41 703  11  25   3   4  15   8  53 137]\n",
      " [ 84  11 480  63  86  89 112  38  24  13]\n",
      " [ 23  24 147 284  75 193 144  61  18  31]\n",
      " [ 34   8 213  69 356  55 137 103  13  12]\n",
      " [ 14  16 110 162  46 420  85 106  11  30]\n",
      " [  7  14 118  67  54  43 658  23   6  10]\n",
      " [ 16   9  75  72  76 100  43 559   7  43]\n",
      " [123  90  28  17   5  10  16  12 656  43]\n",
      " [ 61 166  18  17   4  19  31  37  58 589]]\n",
      "Average Total Loss over Batches: 1.0117603298728168\n",
      "[[630  35  72   9  21  17  15  15 147  39]\n",
      " [ 56 688   9  11   6   3  11   6  74 136]\n",
      " [ 93   9 417  39 146  78 111  55  34  18]\n",
      " [ 37  25 126 204 127 194 158  67  27  35]\n",
      " [ 38  10 159  40 464  55 116  90  18  10]\n",
      " [ 24  14 106 100  95 433  89  95  21  23]\n",
      " [ 11  25  83  41  89  59 635  31   9  17]\n",
      " [ 32  12  54  39 118  93  45 550  11  46]\n",
      " [139  61   9   6  10   7   7  15 717  29]\n",
      " [ 72 145  11  13  19  15  26  41  80 578]]\n",
      "Average Total Loss over Batches: 0.9553982626636326\n",
      "[[606  30  66  24  30  16  18  32 125  53]\n",
      " [ 57 618  11  18   4  11  22  16  45 198]\n",
      " [ 75   6 314  94 171 108 120  72  25  15]\n",
      " [ 23  15  79 328 103 195 114  82  17  44]\n",
      " [ 28   5  79  96 473  56 118 119  12  14]\n",
      " [ 20  11  69 187  74 424  67 114   7  27]\n",
      " [  6   9  49  77 104  59 642  33   5  16]\n",
      " [ 20  10  43  70 109  92  35 570   2  49]\n",
      " [163  69  15  17  13   7  16  22 611  67]\n",
      " [ 50 103  14  28  16  19  26  48  46 650]]\n",
      "Average Total Loss over Batches: 0.9054738923308998\n",
      "[[514  26 106  26  51  13  42  15 168  39]\n",
      " [ 49 623   7  20  12   9  51  12  79 138]\n",
      " [ 53   4 370  60 182  68 173  54  26  10]\n",
      " [ 16  16 116 232 126 144 267  52  12  19]\n",
      " [ 20   6 116  49 515  32 176  68   9   9]\n",
      " [  8   7  95 143  99 385 168  69  13  13]\n",
      " [  3   7  58  54  77  35 738  17   9   2]\n",
      " [ 16   7  57  61 151  71  87 512   8  30]\n",
      " [100  53  24  10  12  10  31  16 702  42]\n",
      " [ 51 116  21  27  20  18  52  47  60 588]]\n",
      "Average Total Loss over Batches: 0.8573772916307301\n",
      "[[539  25  96  31  25  11  14  27 153  79]\n",
      " [ 48 568  11  38   1  11  15  15  55 238]\n",
      " [ 81   7 389 107 149  71  69  86  22  19]\n",
      " [ 29  22 115 350  93 164  80  87  11  49]\n",
      " [ 26   9 145 101 438  45  78 131  11  16]\n",
      " [ 18  10 110 218  58 374  39 130  13  30]\n",
      " [ 12  19 106 133 102  62 489  46   7  24]\n",
      " [ 19   7  53  93  95  88  18 554   3  70]\n",
      " [128  65  24  13  11   8   7  17 645  82]\n",
      " [ 45 109  10  29  12  14  13  53  48 667]]\n",
      "Average Total Loss over Batches: 0.8117678237295524\n",
      "[[571  35  86  40  36  24  12  25 145  26]\n",
      " [ 75 663  11  30   6  12  13  13  51 126]\n",
      " [ 69   8 375 130 131 100  92  50  28  17]\n",
      " [ 19  24  93 395  85 214  75  57  16  22]\n",
      " [ 33   8 140 107 431  69  92  99  14   7]\n",
      " [ 15  11  81 241  64 442  43  79  10  14]\n",
      " [ 11  15  88 150  95  69 526  28   6  12]\n",
      " [ 27   9  57 114  95 120  14 518   7  39]\n",
      " [140  80  21  22  20  10  12  14 646  35]\n",
      " [ 65 141  16  47  16  32  17  39  59 568]]\n",
      "Average Total Loss over Batches: 0.7695668462777976\n",
      "[[501  32 137  20  49  31  22  23 136  49]\n",
      " [ 44 582  21  26  12  18  22  19  56 200]\n",
      " [ 55   5 417  69 156 109 111  46  18  14]\n",
      " [ 16  16 132 256 118 229 136  55   8  34]\n",
      " [ 19   7 155  64 464  68 127  83   7   6]\n",
      " [ 13   7 121 157  83 453  72  72   8  14]\n",
      " [  5  12 105  67  84  59 624  25   4  15]\n",
      " [ 17   8  75  71 136 139  37 468   3  46]\n",
      " [138  79  36  15  24  15  18  14 598  63]\n",
      " [ 47 101  26  33  29  40  31  39  52 602]]\n",
      "Average Total Loss over Batches: 0.7248595244776178\n",
      "[[522  40 151  37  31  26  12  15 139  27]\n",
      " [ 59 664  25  38   9  15  19  19  50 102]\n",
      " [ 59   5 515  87 105  92  69  39  23   6]\n",
      " [ 21  23 186 322  78 203  86  54  10  17]\n",
      " [ 27   6 244  99 384  53  77  96   9   5]\n",
      " [ 11   8 150 185  60 435  48  86   8   9]\n",
      " [  9  13 166 128  73  63 508  29   6   5]\n",
      " [ 24  10 107  87 101 123  28 490   8  22]\n",
      " [122  89  41  29  12  15   9  13 639  31]\n",
      " [ 65 168  31  50  20  33  25  50  60 498]]\n",
      "Average Total Loss over Batches: 0.6968134387747943\n",
      "[[531  25  99  35  22  24  15  27 166  56]\n",
      " [ 62 546  12  37   7  13  20  21  86 196]\n",
      " [ 61   4 381 111 147  98  78  72  28  20]\n",
      " [ 27  17  96 360  86 215  68  79  17  35]\n",
      " [ 33   8 133 116 413  63  81 136  13   4]\n",
      " [ 16   7  95 217  61 418  40 114   9  23]\n",
      " [ 12  10  81 148  85  77 523  41   5  18]\n",
      " [ 23   2  61  93  84 105  20 557   5  50]\n",
      " [134  54  34  19  15  16   8  16 657  47]\n",
      " [ 58  97  19  40  19  29  14  54  64 606]]\n",
      "Average Total Loss over Batches: 0.6649565779502131\n",
      "[[586  51  54  31  35  12  18  21 143  49]\n",
      " [ 44 709   7  22   8   7  16  18  48 121]\n",
      " [ 95   7 346  90 165  84 110  55  31  17]\n",
      " [ 36  30  95 291 112 186 126  73  19  32]\n",
      " [ 39  11 122  81 475  45 102 102  13  10]\n",
      " [ 30  13  90 184  90 390  71  90  16  26]\n",
      " [ 10  25  80 100  83  46 605  25   7  19]\n",
      " [ 35  14  58  63 122 102  32 508   9  57]\n",
      " [130 134  22  16  13  10   9  11 621  34]\n",
      " [ 56 186  13  25  20  17  15  37  62 569]]\n",
      "Average Total Loss over Batches: 0.6367034306448465\n",
      "[[503  36 126  31  24  23  24  22 172  39]\n",
      " [ 46 622  14  38   0  10  29  15  83 143]\n",
      " [ 64   7 437  89  67 101 136  53  33  13]\n",
      " [ 25  20 144 304  48 201 149  67  15  27]\n",
      " [ 28   9 229 107 276  65 156 107  14   9]\n",
      " [ 19   8 127 199  40 417  66  89  11  24]\n",
      " [  9  16 106 112  28  60 635  18   6  10]\n",
      " [ 20   7  70  93  63 121  50 511   9  56]\n",
      " [106  73  39  25   6  12  20  11 676  32]\n",
      " [ 53 140  23  45  11  22  29  48  74 555]]\n",
      "Average Total Loss over Batches: 0.6105062756351568\n",
      "[[608  41  62  41  34  14   7  23 142  28]\n",
      " [ 61 642   7  37   6   9  11  19  73 135]\n",
      " [ 99   7 352 122 151  81  65  78  33  12]\n",
      " [ 32  24 111 341 110 174  65  90  22  31]\n",
      " [ 50  10 140  96 451  47  71 118  12   5]\n",
      " [ 21  11 110 212  68 384  42 120  13  19]\n",
      " [ 19  23 108 148 101  57 480  30  12  22]\n",
      " [ 29   8  56  90  97  99  21 545  11  44]\n",
      " [157  92  19  25  11  11   4  22 624  35]\n",
      " [ 72 143  15  43  19  24  16  48  72 548]]\n",
      "Average Total Loss over Batches: 0.5948342238322086\n",
      "[[555  42  43  19  34  18  19  21 186  63]\n",
      " [ 50 671   6  14   4   7  16   7  59 166]\n",
      " [ 91  11 281  81 162 113  93  79  54  35]\n",
      " [ 46  36  66 247 107 211  89  89  26  83]\n",
      " [ 53  10  88  70 450  63  92 135  15  24]\n",
      " [ 30  18  73 152  71 409  57 109  21  60]\n",
      " [ 18  31  51 106  95  69 511  44  20  55]\n",
      " [ 32  12  45  63  86 108  24 526  11  93]\n",
      " [118  95  11  14   8  11  10  13 657  63]\n",
      " [ 49 165  10  20  11  14  12  28  77 614]]\n",
      "Average Total Loss over Batches: 0.5711300703193061\n",
      "[[461  47 111  40  48  18  12  25 189  49]\n",
      " [ 59 668   6  30   9  11  15  17  63 122]\n",
      " [ 69   5 337  93 203  87  89  71  36  10]\n",
      " [ 29  21  91 310 140 179 112  71  21  26]\n",
      " [ 32   9 101  79 525  50  82 105  10   7]\n",
      " [ 15  16  96 190 105 399  57  88  18  16]\n",
      " [ 10  15  83 124 143  63 516  27   9  10]\n",
      " [ 22   9  59  79 133 107  26 512  12  41]\n",
      " [108 103  29  24  16   8  10  18 645  39]\n",
      " [ 48 176  21  32  28  26  22  45  66 536]]\n",
      "Average Total Loss over Batches: 0.5547337196620554\n",
      "[[620  39  73  17  17  15  13   9 144  53]\n",
      " [ 53 660  10  12   5   9  14   8  65 164]\n",
      " [109   7 382  68 139 107  86  38  36  28]\n",
      " [ 51  26 142 250  95 212  95  48  25  56]\n",
      " [ 63  11 156  74 421  71  86  80  21  17]\n",
      " [ 28  20 130 156  66 428  47  66  19  40]\n",
      " [ 19  20 127  85  83  81 511  20  21  33]\n",
      " [ 35  16  77  65 106 146  22 442  12  79]\n",
      " [186  92  31  15   6   6   6   8 606  44]\n",
      " [ 72 166  20  23  11  20  11  17  71 589]]\n",
      "Average Total Loss over Batches: 0.5299685583325942\n",
      "[[562  42 104  31  36  20  12  18 130  45]\n",
      " [ 65 651   9  30   7  12  17  18  60 131]\n",
      " [ 90   4 368  98 135  96  98  65  29  17]\n",
      " [ 41  23 100 337 103 180 101  62  15  38]\n",
      " [ 38  11 150 114 414  61  93 105   7   7]\n",
      " [ 20  11 117 212  78 384  54  90  11  23]\n",
      " [ 15  10 104 136  77  55 559  25   7  12]\n",
      " [ 21   9  63  95 113 106  31 491   8  63]\n",
      " [158 107  36  24  12  13   9  15 580  46]\n",
      " [ 72 156  20  39  22  27  17  44  54 549]]\n",
      "Average Total Loss over Batches: 0.5213338950159796\n",
      "[[635  28  56  19  14  15   6  14 182  31]\n",
      " [ 84 591  11  27   9   9   7  15 114 133]\n",
      " [142  10 318  89 189  83  39  64  50  16]\n",
      " [ 71  26 107 307 106 187  48  74  36  38]\n",
      " [ 75  13 116  84 483  54  34 114  19   8]\n",
      " [ 44  12 109 192  88 372  24 104  31  24]\n",
      " [ 39  20 108 140 181  73 351  37  27  24]\n",
      " [ 55   8  67  72 122 103  13 492  15  53]\n",
      " [180  57  18  16   5  12   4  10 675  23]\n",
      " [110 138  17  31  24  30  10  43 118 479]]\n",
      "Average Total Loss over Batches: 0.4987684306239116\n",
      "[[443  48 137  48  46  34  15  21 152  56]\n",
      " [ 39 598  19  34   7  16  16  30  58 183]\n",
      " [ 51   6 380 133 117  92  98  79  28  16]\n",
      " [ 28  22 118 324  82 176 113  84  16  37]\n",
      " [ 30   9 136 105 397  68 102 135   8  10]\n",
      " [ 13   8 119 217  66 368  56 120   5  28]\n",
      " [ 11  15 104 134  76  53 545  39   6  17]\n",
      " [ 10   5  72  94  90  96  31 529  10  63]\n",
      " [112  99  61  42  13  13  10  15 572  63]\n",
      " [ 43 146  28  46  21  25  18  43  54 576]]\n",
      "Average Total Loss over Batches: 0.48872933592729734\n",
      "[[603  38  53  43  32  18  12  14 143  44]\n",
      " [ 67 629  10  31   7  13  18  11  59 155]\n",
      " [ 98   7 298 119 145 107 119  55  33  19]\n",
      " [ 51  27  91 322  83 194 126  57  18  31]\n",
      " [ 56  10 101 110 418  68 132  87  12   6]\n",
      " [ 26   9  74 217  79 422  62  68  15  28]\n",
      " [ 18  12  71 117  84  55 597  21  10  15]\n",
      " [ 32  10  63  97 111 143  33 446  14  51]\n",
      " [151  86  26  30   9  13  13   5 622  45]\n",
      " [ 78 156  17  42  21  31  25  31  69 530]]\n",
      "Average Total Loss over Batches: 0.47556244248471224\n",
      "[[512  54  76  25  23  19  16  21 211  43]\n",
      " [ 52 689   9  21   3  14  12  10  74 116]\n",
      " [ 83  13 312  81 155 115 122  51  50  18]\n",
      " [ 35  37  95 268  95 196 128  70  24  52]\n",
      " [ 40  19 103  65 415  74 133 121  19  11]\n",
      " [ 23  20  98 171  79 395  58  95  22  39]\n",
      " [ 10  30  77  97  64  67 596  25  14  20]\n",
      " [ 26  19  59  71  91 140  38 460  18  78]\n",
      " [114  95  18  22   6  12  11   5 679  38]\n",
      " [ 66 215  14  22  11  21  20  32  99 500]]\n",
      "Average Total Loss over Batches: 0.4642906025433587\n",
      "[[489  37  59  31  27  14  14  19 253  57]\n",
      " [ 45 606  13  19   6  13  18  14 111 155]\n",
      " [ 94  10 295  77 170  91 114  72  60  17]\n",
      " [ 42  27  99 260 121 173 125  67  36  50]\n",
      " [ 46   9 100  69 446  57 117 116  26  14]\n",
      " [ 23  11  93 179 105 350  68 105  27  39]\n",
      " [ 17  18  82 113  89  56 562  24  23  16]\n",
      " [ 27  14  59  75 118  96  37 486  23  65]\n",
      " [100  78  19  18  11  12   7  10 703  42]\n",
      " [ 68 160  14  42  19  20  16  33 110 518]]\n",
      "Average Total Loss over Batches: 0.4452956841941178\n",
      "[[530  50  95  33  29  30  19  35 143  36]\n",
      " [ 55 629  16  23  13  17  19  32  57 139]\n",
      " [ 83   6 348  88 129 115 100  88  30  13]\n",
      " [ 29  24  99 264  98 232 107  89  24  34]\n",
      " [ 46  12 136  84 364  85 103 152  10   8]\n",
      " [ 15  10  90 164  74 453  47 118  12  17]\n",
      " [ 10  16  95 120 100  90 507  39   7  16]\n",
      " [ 28  13  60  76  88 121  27 534  10  43]\n",
      " [148 101  40  23  10  23  11  20 593  31]\n",
      " [ 64 180  19  37  22  45  28  58  66 481]]\n",
      "Average Total Loss over Batches: 0.43457080734760034\n",
      "[[475  39  77  23  55  31  15  29 195  61]\n",
      " [ 60 564  18  23  16  17  21  24  73 184]\n",
      " [ 79   6 311  69 198 110  92  77  40  18]\n",
      " [ 38  20 104 236 118 222 115  81  17  49]\n",
      " [ 29   8 109  63 453  72 103 136  12  15]\n",
      " [ 14  10  83 144 108 424  66 110  15  26]\n",
      " [ 11  10  79 100 117  81 522  40  15  25]\n",
      " [ 22   9  66  70 128 114  24 492   8  67]\n",
      " [120  83  31  19  18  13  14  20 628  54]\n",
      " [ 48 123  20  29  36  32  20  44  69 579]]\n",
      "Average Total Loss over Batches: 0.42387411447730616\n",
      "[[583  44  61  51  19  25  14  18 131  54]\n",
      " [ 61 627  12  38   5  14  16  13  44 170]\n",
      " [103   8 354 160  97  98  82  53  28  17]\n",
      " [ 43  23 105 374  76 183  92  52  11  41]\n",
      " [ 50  12 160 153 343  71  96  97   5  13]\n",
      " [ 25  14  90 275  62 372  48  73  11  30]\n",
      " [ 18  15 103 159  65  70 512  26   9  23]\n",
      " [ 37  10  64 141  84 120  29 435   6  74]\n",
      " [200 101  29  30   9  18   8   6 536  63]\n",
      " [ 72 175  18  44  14  32  16  26  55 548]]\n",
      "Average Total Loss over Batches: 0.42503949926845963\n",
      "[[564  45  85  46  35  23  21  18 124  39]\n",
      " [ 76 641  14  25   6  13  16  19  53 137]\n",
      " [ 85  10 363 104 118  92 118  59  32  19]\n",
      " [ 43  28 101 313  83 166 132  67  22  45]\n",
      " [ 59  14 152  97 375  51 123 110  10   9]\n",
      " [ 23  14 117 216  77 343  75  95  12  28]\n",
      " [ 20  17  97 128  70  43 575  28  10  12]\n",
      " [ 44  11  80 103 104 104  47 449   7  51]\n",
      " [180 106  35  22  10  14  16  11 570  36]\n",
      " [ 83 163  20  45  18  29  24  36  67 515]]\n",
      "Average Total Loss over Batches: 0.41437535600284287\n",
      "[[494  41  72  30  22  20  26  25 202  68]\n",
      " [ 36 598  11  19   5  10  22  16  65 218]\n",
      " [ 86   9 320 100 117 105 105  91  42  25]\n",
      " [ 34  27  95 293  83 177 133  75  20  63]\n",
      " [ 41  12 141  97 366  55 106 139  18  25]\n",
      " [ 20  19  98 196  59 368  63 109  24  44]\n",
      " [ 14  20  85 128  73  64 532  39  13  32]\n",
      " [ 30   9  54  85  70 103  45 508   8  88]\n",
      " [130  98  26  19   7   9  13  16 612  70]\n",
      " [ 47 148  14  33  12  20  18  34  64 610]]\n",
      "Average Total Loss over Batches: 0.4012081904428097\n",
      "[[615  33  78  34  34  20  17  16 106  47]\n",
      " [ 68 580  13  23   9  16  15  22  55 199]\n",
      " [100   7 370  94 160  89  74  58  28  20]\n",
      " [ 40  26 113 300 134 154  95  66  21  51]\n",
      " [ 59   8 155  97 417  48  77 120   7  12]\n",
      " [ 29  13 113 210 111 328  66  86  12  32]\n",
      " [ 19  12 113 127 109  56 488  38   9  29]\n",
      " [ 37   9  83 100 128  91  25 447   9  71]\n",
      " [208  99  41  23  18  12   7  11 524  57]\n",
      " [ 73 136  20  30  27  27  23  38  64 562]]\n",
      "Average Total Loss over Batches: 0.39382235941337335\n",
      "[[502  28  64  40  34  21  18  30 205  58]\n",
      " [ 58 599  13  23   6  11  23  24  87 156]\n",
      " [ 87   7 317 109 136  98  97  84  46  19]\n",
      " [ 36  20  97 319  93 171 112  87  22  43]\n",
      " [ 39   9 138 108 380  62  98 131  19  16]\n",
      " [ 20  12  82 203  68 397  48 122  23  25]\n",
      " [ 15  19  95 139  76  60 513  39  16  28]\n",
      " [ 28   7  58 104  88 111  27 496  15  66]\n",
      " [130  73  20  17  10  11   8  25 658  48]\n",
      " [ 61 141  15  32  16  27  25  40  83 560]]\n",
      "Average Total Loss over Batches: 0.3779352024431346\n",
      "[[585  32  83  34  24  23  16  29 140  34]\n",
      " [ 77 610  19  21   7  21  23  24  67 131]\n",
      " [104   7 372  95 119  89  94  80  26  14]\n",
      " [ 39  22 129 322  94 160 115  72  17  30]\n",
      " [ 57  10 177  98 357  54 107 125   6   9]\n",
      " [ 26  10 129 205  72 367  52 102  19  18]\n",
      " [ 15  10 125 124  95  53 530  26  11  11]\n",
      " [ 43   6  83  92  96 101  25 499   9  46]\n",
      " [195  89  34  23  13  18  10  23 559  36]\n",
      " [ 88 142  22  44  21  34  27  58  67 497]]\n",
      "Average Total Loss over Batches: 0.38082278010101817\n",
      "[[575  52  68  31  16  17  16  21 168  36]\n",
      " [ 64 649  16  21   8   9  13  21  67 132]\n",
      " [125  17 336  96 116  95  86  75  35  19]\n",
      " [ 52  33 116 289  94 177  94  68  27  50]\n",
      " [ 69  20 149  91 368  57 101 117  14  14]\n",
      " [ 38  23  96 187  69 378  49 104  26  30]\n",
      " [ 27  20 102 108  90  63 521  29  17  23]\n",
      " [ 42  13  73  88  91  99  23 492  12  67]\n",
      " [176 110  28  18   6  11   7  11 601  32]\n",
      " [ 72 192  18  27  19  29  23  37  82 501]]\n",
      "Average Total Loss over Batches: 0.36432909863229024\n",
      "[[573  42  95  33  30  10  27  19 130  41]\n",
      " [ 72 595  21  36  11  11  32  24  70 128]\n",
      " [ 98  11 363 107 130  56 131  57  30  17]\n",
      " [ 36  15 121 305 106 135 152  65  23  42]\n",
      " [ 59  13 145 101 385  30 143 103  11  10]\n",
      " [ 24  12 125 215  93 286 101 103  19  22]\n",
      " [ 15  10 121  92 108  32 569  30  12  11]\n",
      " [ 38   6  84  90 129  74  50 463   9  57]\n",
      " [169  88  40  25  14   7  16  15 581  45]\n",
      " [ 74 141  28  33  29  20  45  52  74 504]]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m( train_X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size ):\n\u001b[1;32m----> 6\u001b[0m   x_batch, y_batch \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_Y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m   cnn_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      9\u001b[0m   logits \u001b[38;5;241m=\u001b[39m cifar_model( x_batch )\n",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m, in \u001b[0;36mget_batch\u001b[1;34m(x, y, batch_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_batch\u001b[39m(x, y, batch_size):\n\u001b[0;32m      2\u001b[0m   n \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m   batch_indices \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample( [ i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n) ], k \u001b[38;5;241m=\u001b[39m batch_size )\n\u001b[0;32m      6\u001b[0m   x_batch \u001b[38;5;241m=\u001b[39m x[ batch_indices ]\n\u001b[0;32m      7\u001b[0m   y_batch \u001b[38;5;241m=\u001b[39m y[ batch_indices ]\n",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_batch\u001b[39m(x, y, batch_size):\n\u001b[0;32m      2\u001b[0m   n \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m   batch_indices \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample( [ i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n) ], k \u001b[38;5;241m=\u001b[39m batch_size )\n\u001b[0;32m      6\u001b[0m   x_batch \u001b[38;5;241m=\u001b[39m x[ batch_indices ]\n\u001b[0;32m      7\u001b[0m   y_batch \u001b[38;5;241m=\u001b[39m y[ batch_indices ]\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "startTime = time.time()\n",
    "for epochs in range(100):\n",
    "  total_loss = 0\n",
    "  for batch in range( train_X.shape[0] // batch_size ):\n",
    "    x_batch, y_batch = get_batch(train_X, train_Y, batch_size)\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "    logits = cifar_model( x_batch )\n",
    "    loss = loss_function( logits, y_batch )\n",
    "\n",
    "    loss.backward()\n",
    "    cnn_optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  print( \"Average Total Loss over Batches:\", total_loss / ( train_X.shape[0] // batch_size ) )\n",
    "  print( confusion_matrix( cifar_model, test_X, test_Y ) )\n",
    "endTime = time.time()\n",
    "print(str(endTime - startTime) + \" sec\")\n",
    "print(\"or roughly \" + str((endTime - startTime)//60) + \" min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ta4Mwqto9y6s"
   },
   "source": [
    "There are some interesting narratives that develop over the course of training - for instance the back and forth misidentifications between cars and trucks.\n",
    "\n",
    "If your run is anything like mine, you see steady decrease in the loss, and trending of the confusion matrix to focus on the diagonal (i.e., correctly identifying things). There are persistent misclassifications, but this is not the most complext network you could construct here.\n",
    "\n",
    "An interesting thing happens - at least for me - if you try to add another convolutional layer to the network (mine had 20 features). Why?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
