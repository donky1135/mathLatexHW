{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3MxHWiEHnYqW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivIpm9qZ3V9y",
    "outputId": "bfaea4f8-83d3-425c-ce61-a5bfa541bbd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaxI5PV_4K_x"
   },
   "source": [
    "In the above, I'm downloading the CIFAR data set. This is a set of 32x32 pixel RGB images, belonging to the 10 classes indicated. The ToTensor() transformation here is necessary to convert the original data from image objects to numerical arrays that we can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMwhxL9bo74s",
    "outputId": "1689a9eb-41e9-4428-8f73-4ab0c8ecf330"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKDU8rSm4ZPN"
   },
   "source": [
    "The training data set consists of 50,000 32x32 images, each pixel consisting of three values (RGB). The three values are generally referred to as the `channels' of the data. A black and white image could be thought of as having a single channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SXGkny55pG_1",
    "outputId": "0d9c1894-a56b-4931-9696-c22d72652822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 9, 9, 4, 1, 1, 2, 7, 8, 3]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.targets[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63i86Q2B4nsS"
   },
   "source": [
    "The targets, the first 10 of which are shown here, are class labels, 0 through 9, corresponding to the indices in the classes list above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "BgIVy_x1pMhZ",
    "outputId": "b7f1cc3d-f405-41b1-9697-151bcca4c0fe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'frog'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[ trainset.targets[0] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOH2fPpP4tw4"
   },
   "source": [
    "We see that the first image in the training set is labeled as a frog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "AaUxu6WrpUH1",
    "outputId": "3ed0fce2-709f-4628-d502-3642594d8275"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "      .ndarray_repr .ndarray_raw_data {\n",
       "        display: none;\n",
       "      }\n",
       "      .ndarray_repr.show_array .ndarray_raw_data {\n",
       "        display: block;\n",
       "      }\n",
       "      .ndarray_repr.show_array .ndarray_image_preview {\n",
       "        display: none;\n",
       "      }\n",
       "      </style>\n",
       "      <div id=\"id-6b1c95bc-3a74-4231-9eb1-63d322180f81\" class=\"ndarray_repr\"><pre>ndarray (32, 32, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,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\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[ 59,  62,  63],\n",
       "        [ 43,  46,  45],\n",
       "        [ 50,  48,  43],\n",
       "        ...,\n",
       "        [158, 132, 108],\n",
       "        [152, 125, 102],\n",
       "        [148, 124, 103]],\n",
       "\n",
       "       [[ 16,  20,  20],\n",
       "        [  0,   0,   0],\n",
       "        [ 18,   8,   0],\n",
       "        ...,\n",
       "        [123,  88,  55],\n",
       "        [119,  83,  50],\n",
       "        [122,  87,  57]],\n",
       "\n",
       "       [[ 25,  24,  21],\n",
       "        [ 16,   7,   0],\n",
       "        [ 49,  27,   8],\n",
       "        ...,\n",
       "        [118,  84,  50],\n",
       "        [120,  84,  50],\n",
       "        [109,  73,  42]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[208, 170,  96],\n",
       "        [201, 153,  34],\n",
       "        [198, 161,  26],\n",
       "        ...,\n",
       "        [160, 133,  70],\n",
       "        [ 56,  31,   7],\n",
       "        [ 53,  34,  20]],\n",
       "\n",
       "       [[180, 139,  96],\n",
       "        [173, 123,  42],\n",
       "        [186, 144,  30],\n",
       "        ...,\n",
       "        [184, 148,  94],\n",
       "        [ 97,  62,  34],\n",
       "        [ 83,  53,  34]],\n",
       "\n",
       "       [[177, 144, 116],\n",
       "        [168, 129,  94],\n",
       "        [179, 142,  87],\n",
       "        ...,\n",
       "        [216, 184, 140],\n",
       "        [151, 118,  84],\n",
       "        [123,  92,  72]]], dtype=uint8)</pre></div><script>\n",
       "      (() => {\n",
       "      const titles = ['show data', 'hide data'];\n",
       "      let index = 0\n",
       "      document.querySelector('#id-6b1c95bc-3a74-4231-9eb1-63d322180f81 button').onclick = (e) => {\n",
       "        document.querySelector('#id-6b1c95bc-3a74-4231-9eb1-63d322180f81').classList.toggle('show_array');\n",
       "        index = (++index) % 2;\n",
       "        document.querySelector('#id-6b1c95bc-3a74-4231-9eb1-63d322180f81 button').textContent = titles[index];\n",
       "        e.preventDefault();\n",
       "        e.stopPropagation();\n",
       "      }\n",
       "      })();\n",
       "    </script>"
      ],
      "text/plain": [
       "array([[[ 59,  62,  63],\n",
       "        [ 43,  46,  45],\n",
       "        [ 50,  48,  43],\n",
       "        ...,\n",
       "        [158, 132, 108],\n",
       "        [152, 125, 102],\n",
       "        [148, 124, 103]],\n",
       "\n",
       "       [[ 16,  20,  20],\n",
       "        [  0,   0,   0],\n",
       "        [ 18,   8,   0],\n",
       "        ...,\n",
       "        [123,  88,  55],\n",
       "        [119,  83,  50],\n",
       "        [122,  87,  57]],\n",
       "\n",
       "       [[ 25,  24,  21],\n",
       "        [ 16,   7,   0],\n",
       "        [ 49,  27,   8],\n",
       "        ...,\n",
       "        [118,  84,  50],\n",
       "        [120,  84,  50],\n",
       "        [109,  73,  42]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[208, 170,  96],\n",
       "        [201, 153,  34],\n",
       "        [198, 161,  26],\n",
       "        ...,\n",
       "        [160, 133,  70],\n",
       "        [ 56,  31,   7],\n",
       "        [ 53,  34,  20]],\n",
       "\n",
       "       [[180, 139,  96],\n",
       "        [173, 123,  42],\n",
       "        [186, 144,  30],\n",
       "        ...,\n",
       "        [184, 148,  94],\n",
       "        [ 97,  62,  34],\n",
       "        [ 83,  53,  34]],\n",
       "\n",
       "       [[177, 144, 116],\n",
       "        [168, 129,  94],\n",
       "        [179, 142,  87],\n",
       "        ...,\n",
       "        [216, 184, 140],\n",
       "        [151, 118,  84],\n",
       "        [123,  92,  72]]], dtype=uint8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BniQOZ764yx1"
   },
   "source": [
    "COLAB prints out a tiny thumbnail here, which I think is interesting, but more importantly we see that the image data is stored as a numpy array, 32x32x3, where each value is an integer between 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "7AlD1gPHpVZ-",
    "outputId": "97054520-8c90-4491-a57a-1e697db1b0dd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow( trainset.data[0] / 256 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bamkT4q5CBc"
   },
   "source": [
    "In the following code block, I prepare the data for use - the first step is to scale all the pixel values to be between 0 and 1, by dividing by 255. Next, I subtract off 0.5, so that pixel values are centered [-0.5, 0.5] rather than [0, 255] as before.\n",
    "\n",
    "Two important technical notes here:\n",
    "\n",
    "First, the implementation of convolutional layers in PyTorch expects the data to be presented 'channel first', that is a tensor of data should be of the form [ batch_size, num channels, height, width ]. As such, I use the permute function here, to make the channel dimension the second dimension (placing dimension 3 in dimension 1), and bump over the remaining dimensions. The result of this will be, in the case of the training data, a block of size [50000,3,32,32].\n",
    "\n",
    "Second, apparently the crossEntropyLoss method expects class labels to come in the form of long data types, so I transform the data labels as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3l8wu6ZleJo",
    "outputId": "568b6ff1-f51b-42fd-ea27-f36c8a316f0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32])\n",
      "torch.Size([10000, 3, 32, 32])\n",
      "torch.Size([50000])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "train_X = torch.Tensor( trainset.data/255.0 - 0.5 )\n",
    "train_X = train_X.permute( 0, 3, 1, 2 )\n",
    "test_X = torch.Tensor( testset.data/255.0 - 0.5 )\n",
    "test_X = test_X.permute( 0, 3, 1, 2 )\n",
    "\n",
    "train_Y = torch.Tensor( np.asarray( trainset.targets ) ).long()\n",
    "#train_Y = train_Y.reshape( (-1,1) )\n",
    "test_Y = torch.Tensor( np.asarray( testset.targets ) ).long()\n",
    "#test_Y = test_Y.reshape( (-1,1) )\n",
    "\n",
    "print( train_X.shape )\n",
    "print( test_X.shape )\n",
    "print( train_Y.shape )\n",
    "print( test_Y.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDj36Is157TQ"
   },
   "source": [
    "The following code simply serves to remove a random batch from the specified data set, as we've seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iHA-uU_znf2",
    "outputId": "3bc89dfb-61ff-47b7-fd9b-515af9494ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "def get_batch(x, y, batch_size):\n",
    "  n = x.shape[0]\n",
    "\n",
    "  batch_indices = random.sample( [ i for i in range(n) ], k = batch_size )\n",
    "\n",
    "  x_batch = x[ batch_indices ]\n",
    "  y_batch = y[ batch_indices ]\n",
    "\n",
    "  return x_batch, y_batch\n",
    "\n",
    "batch_x, batch_y = get_batch( train_X, train_Y, batch_size = 4 )\n",
    "print( batch_x.shape )\n",
    "print( batch_y.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-XnMf4s6DCL"
   },
   "source": [
    "To demonstrate a convolutional layer, I'm going to create a 2D convolutional layer object. I'm specifying that the input will have 3 channels (RGB). The output will have 10 channels (that is, I will be calculating 10 features every time the convolutional window is applied). The local feature will be of size 3x3 (it doesn't have to be square, but the default assumption is square), and the stride is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IpJ39WOE0Rvj"
   },
   "outputs": [],
   "source": [
    "conv_test_layer = nn.Conv2d(in_channels = 3, out_channels = 10, kernel_size = 3, stride = 1, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EU52U4Er6XIs"
   },
   "source": [
    "Note that applying this convolutional layer to a input batch of 32x32, there are 30 places that a 3x3 feature window can be applied horizontally, at stride 1.\n",
    "\n",
    "Similarly, there are 30 places a 3x3 feature window can be applied horizontally, at stride 1.\n",
    "\n",
    "At each of these 30x30 locations, this layer will be computing 10 features. So the output of this layer should be of size 10x30x30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Fo5Fmk-k00mF"
   },
   "outputs": [],
   "source": [
    "convolved_batch = conv_test_layer( batch_x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICnINTiD04Hr",
    "outputId": "52275512-d61a-47fd-bc58-94d1fbb1e75d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 30, 30])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convolved_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVnxcxGb6y-l"
   },
   "source": [
    "This is exactly as we expect.\n",
    "\n",
    "What do the internal parameters of a convolutional layer look like?\n",
    "\n",
    "Note that when we compute a feature of a 3x3 window, that is of 9 pixels, each one with three values. So a single feature will require 3\\*3\\*3 weights, with a single bias value.\n",
    "\n",
    "If we want to do this 10 times (for 10 total features), we need an arrangement of 10x3x3x3 weights, and 10 bias values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WZv6YmJ05hM",
    "outputId": "e5086f03-9cc5-483b-b235-f31a3ccd874e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 3, 3])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print( conv_test_layer.weight.shape )\n",
    "print( conv_test_layer.bias.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3umk3hDD7XkS"
   },
   "source": [
    "This is exactly as we expect.\n",
    "\n",
    "Note that `under the hood', the convolutional layer is applying this block of weights iteratively, at each location it is computing a local feature (according to the stride)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP6BjfEQ7mGg"
   },
   "source": [
    "We can in fact stack convolutional layers.\n",
    "\n",
    "The output of the first test layer is 10x30x30. We could pass this through another convolutional layer. Here I specify 10 input features or channels at each location, we want to compute 5 output features, and the kernel or feature window will again be 3x3, with a stride of 1.\n",
    "\n",
    "For a 30x30 field of view, and a 3x3 feature window at stride 1, there are 28x28 possible locations to apply that feature window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcYNfIGaGx_m"
   },
   "outputs": [],
   "source": [
    "conv_test_layer_2 = nn.Conv2d(in_channels = 10, out_channels = 5, kernel_size = 3, stride = 1, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vgciZnLG3cw",
    "outputId": "09443b3b-ebbc-4a62-aec0-af76157bfc28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 28, 28])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_test_layer_2( convolved_batch ).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv1NzsFQ8AF0"
   },
   "source": [
    "We see that for each of the 4 images in the batch, we are computing an output of size 5 (features) x 28 (local field height) x 28 (local field width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GkQclHkF1Dig"
   },
   "outputs": [],
   "source": [
    "class CIFARModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CIFARModel, self).__init__()\n",
    "\n",
    "    self.conv_layer_1 = nn.Conv2d(in_channels = 3, out_channels = 5, kernel_size = 3, stride = 1, bias=True)\n",
    "    self.conv_layer_2 = nn.Conv2d(in_channels = 5, out_channels = 10, kernel_size = 3, stride = 1, bias=True)\n",
    "    self.conv_layer_3 = nn.Conv2d(in_channels = 10, out_channels = 15, kernel_size = 3, stride = 1, bias=True)\n",
    "\n",
    "    self.linear_layer = torch.nn.Linear( in_features = 15*26*26, out_features = 10, bias=True )\n",
    "    # Note that the output of the last convolutional layer will be 15x26x16 - why?\n",
    "    # So we want to input 15*26*26 values into the last layer, and get 10 output values out (for the class probabilities)\n",
    "\n",
    "  def forward(self, input_tensor):\n",
    "    output = self.conv_layer_1( input_tensor )\n",
    "    output = nn.Sigmoid()( output )\n",
    "    output = self.conv_layer_2( output )\n",
    "    output = nn.Sigmoid()( output )\n",
    "    output = self.conv_layer_3( output )\n",
    "    output = nn.Sigmoid()( output )\n",
    "\n",
    "    # At this point, the block of node values from the convolutional layer is flattened\n",
    "    # So that it can be passed into a standard linear layer\n",
    "    output = nn.Flatten()( output )\n",
    "    output = self.linear_layer( output )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8HdWU1B88zo"
   },
   "source": [
    "We can initialize and view this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VgmewIe1lSp",
    "outputId": "67a29238-63af-46ab-e499-63ab6d87430d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFARModel(\n",
      "  (conv_layer_1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv_layer_2): Conv2d(5, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv_layer_3): Conv2d(10, 15, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (linear_layer): Linear(in_features=10140, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cifar_model = CIFARModel()\n",
    "print( cifar_model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKYeu3Ri9Meb"
   },
   "source": [
    "We can use the standard confusion matrix to get a sense of how good the initial random model is, and see how that changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EpGDcyOz9Q-U"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix( model, x, y ):\n",
    "  identification_counts = np.zeros( shape = (10,10), dtype = np.int32 )\n",
    "\n",
    "  logits = model( x )\n",
    "  predicted_classes = torch.argmax( logits, dim = 1 )\n",
    "\n",
    "  n = x.shape[0]\n",
    "\n",
    "  for i in range(n):\n",
    "    actual_class = int( y[i].item() )\n",
    "    predicted_class = predicted_classes[i].item()\n",
    "    identification_counts[actual_class, predicted_class] += 1\n",
    "\n",
    "  return identification_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BwNlcq7_-ZSu",
    "outputId": "1e481252-4e0c-4179-bf98-47db88d0573d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 1000,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0, 1000,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0, 1000,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0, 1000,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0, 1000,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0, 1000,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0, 1000,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0, 1000,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0, 1000,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "       [   0, 1000,    0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix( cifar_model, test_X, test_Y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZdB75lOI1sVg"
   },
   "outputs": [],
   "source": [
    "cnn_optimizer = optim.Adam(cifar_model.parameters(), lr = 0.01 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5_uWsb1V1xZg"
   },
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ya7iliGm136x",
    "outputId": "62fa21e3-ee6c-48c6-fbcf-b7be80cdc656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Test Loss: 2.3434672355651855\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Test Loss:\", loss_function( cifar_model( test_X ), test_Y ).item() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-dEK9Sr2BL6",
    "outputId": "3d527766-7950-4052-8724-9bb6ddf829fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Total Loss over Batches: 2.3259323231601714\n",
      "[[1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]]\n",
      "Average Total Loss over Batches: 2.3066025297546386\n",
      "[[1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]]\n",
      "Average Total Loss over Batches: 2.3063114523887633\n",
      "[[   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]]\n",
      "Average Total Loss over Batches: 2.3069899097442628\n",
      "[[1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]]\n",
      "Average Total Loss over Batches: 2.3064217523765564\n",
      "[[   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]\n",
      " [   0    0    0    0 1000    0    0    0    0    0]]\n",
      "Average Total Loss over Batches: 2.3069434652137755\n",
      "[[   0 1000    0    0    0    0    0    0    0    0]\n",
      " [   0 1000    0    0    0    0    0    0    0    0]\n",
      " [   0 1000    0    0    0    0    0    0    0    0]\n",
      " [   0 1000    0    0    0    0    0    0    0    0]\n",
      " [   0 1000    0    0    0    0    0    0    0    0]\n",
      " [   0 1000    0    0    0    0    0    0    0    0]\n",
      " [   0 1000    0    0    0    0    0    0    0    0]\n",
      " [   0 1000    0    0    0    0    0    0    0    0]\n",
      " [   0 1000    0    0    0    0    0    0    0    0]\n",
      " [   0 1000    0    0    0    0    0    0    0    0]]\n",
      "Average Total Loss over Batches: 2.3067281427288053\n",
      "[[1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]]\n",
      "Average Total Loss over Batches: 2.3089410834026336\n",
      "[[1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]]\n",
      "Average Total Loss over Batches: 2.306351573162079\n",
      "[[1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]]\n",
      "Average Total Loss over Batches: 2.3082115967845915\n",
      "[[   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]]\n",
      "Average Total Loss over Batches: 2.3074535938835146\n",
      "[[   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]]\n",
      "Average Total Loss over Batches: 2.306211605453491\n",
      "[[   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]\n",
      " [   0    0    0    0    0 1000    0    0    0    0]]\n",
      "Average Total Loss over Batches: 2.3065798676490785\n",
      "[[   0    0 1000    0    0    0    0    0    0    0]\n",
      " [   0    0 1000    0    0    0    0    0    0    0]\n",
      " [   0    0 1000    0    0    0    0    0    0    0]\n",
      " [   0    0 1000    0    0    0    0    0    0    0]\n",
      " [   0    0 1000    0    0    0    0    0    0    0]\n",
      " [   0    0 1000    0    0    0    0    0    0    0]\n",
      " [   0    0 1000    0    0    0    0    0    0    0]\n",
      " [   0    0 1000    0    0    0    0    0    0    0]\n",
      " [   0    0 1000    0    0    0    0    0    0    0]\n",
      " [   0    0 1000    0    0    0    0    0    0    0]]\n",
      "Average Total Loss over Batches: 2.3059419303989412\n",
      "[[1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]\n",
      " [1000    0    0    0    0    0    0    0    0    0]]\n",
      "Average Total Loss over Batches: 2.2795805739593504\n",
      "[[246   7 105   5 366  13 106  53  76  23]\n",
      " [ 77  90  46  18  51  55 337  24 179 123]\n",
      " [190   4 171   7 357  20 170  39  37   5]\n",
      " [129   3 131  36 186  81 339  35  44  16]\n",
      " [215   3 114   5 343  22 203  49  42   4]\n",
      " [155   8 153  27 190  76 300  40  41  10]\n",
      " [150   0  97  12 265  40 350   9  67  10]\n",
      " [197   8 131  15 144  38 176 247  32  12]\n",
      " [168   6  60   9 202  15 214  48 248  30]\n",
      " [111  62  39   7  50  23 247  38 271 152]]\n",
      "Average Total Loss over Batches: 1.7497910631752014\n",
      "[[590  25  63  27  18  24  23  20 162  48]\n",
      " [ 67 474  13  43   0  27  40  24 157 155]\n",
      " [110   7 364  74  82 148 112  53  28  22]\n",
      " [ 31  16 118 245  52 273 159  51  15  40]\n",
      " [ 50   8 223  88 223 105 177  92  18  16]\n",
      " [ 28   7 152 147  31 438  90  65  19  23]\n",
      " [ 10  15  78  92  61 103 575  25  13  28]\n",
      " [ 35  14  90  82  36 130  46 499  11  57]\n",
      " [230  76  32  21   8  14  21   7 551  40]\n",
      " [ 77 140  27  30   7  34  34  28 151 472]]\n",
      "Average Total Loss over Batches: 1.396765170789361\n",
      "[[681  38  62  13  14   4  16  15 112  45]\n",
      " [ 85 589  12  17   4   8  24  12  98 151]\n",
      " [131  12 452  60  86  70 106  39  29  15]\n",
      " [ 61  24 181 289  64 107 149  67  22  36]\n",
      " [ 78  17 271  56 291  35 136  90  15  11]\n",
      " [ 37  10 205 197  52 292  83  87  18  19]\n",
      " [ 18  13 101  64  63  37 643  26  11  24]\n",
      " [ 49  11 110  58  58  59  34 564  13  44]\n",
      " [271  92  34  13   4   5  12   8 514  47]\n",
      " [101 146  28  23   6   7  38  25  78 548]]\n",
      "Average Total Loss over Batches: 1.2916327869862319\n",
      "[[523  24 106  21  14   8  11  13 198  82]\n",
      " [ 54 505  17  18   4  12  16  10 119 245]\n",
      " [ 85  10 415  68 131  94  69  55  46  27]\n",
      " [ 22  14 170 261  79 228  78  65  32  51]\n",
      " [ 35   6 223  63 384  63  77 100  27  22]\n",
      " [ 13   7 181 156  55 424  37  86  18  23]\n",
      " [  8  11 103  73  90  64 560  43  17  31]\n",
      " [ 17  13 106  40  67  94  10 572  16  65]\n",
      " [106  53  32  14   5  10   3  10 702  65]\n",
      " [ 56  92  28  16   4  18  17  34  80 655]]\n",
      "Average Total Loss over Batches: 1.2232794951063395\n",
      "[[568  22 133  31  27   9  23  21 130  36]\n",
      " [ 72 550  41  28  19  15  64  20  84 107]\n",
      " [ 64   4 476  64 123  70 131  40  24   4]\n",
      " [ 28   8 177 283  77 140 217  38  22  10]\n",
      " [ 28   4 217  66 370  34 215  54  11   1]\n",
      " [  4   2 207 183  57 374 116  43  10   4]\n",
      " [  6   3  62  48  62  22 778  12   3   4]\n",
      " [ 19   9 140  60 112  84  69 489   5  13]\n",
      " [168  50  50  17  11  13  24  11 631  25]\n",
      " [ 77 110  65  60  17  23  72  35  60 481]]\n",
      "Average Total Loss over Batches: 1.1696654924246668\n",
      "[[646  43  44  22  17  11   2  12 156  47]\n",
      " [ 70 657   5  14   5   7   9   9 106 118]\n",
      " [142  13 345  69 116 147  48  38  64  18]\n",
      " [ 69  45  96 259  88 266  57  46  38  36]\n",
      " [ 67  17 155  68 405  98  49 100  25  16]\n",
      " [ 31  12 127 137  50 488  26  79  30  20]\n",
      " [ 27  20  84  89  86  82 533  32  22  25]\n",
      " [ 38  24  66  47  75 121  11 555  17  46]\n",
      " [165  84  17   6   8   6   3   7 661  43]\n",
      " [ 82 196  12  17   6  14  16  18 102 537]]\n",
      "Average Total Loss over Batches: 1.1146516849592327\n",
      "[[650  37  65  27  25   5  10  24 110  47]\n",
      " [ 61 666   9  28   5   5  20  17  82 107]\n",
      " [103  16 378 121  94 108  86  64  20  10]\n",
      " [ 38  21  96 405  59 174 100  74  16  17]\n",
      " [ 54  14 160 111 337  57 117 135   7   8]\n",
      " [ 16  17 131 266  36 383  41  92  13   5]\n",
      " [ 13   9  71 124  61  44 634  24   8  12]\n",
      " [ 32  14  71 100  63  88  18 582   8  24]\n",
      " [199 102  23  26   8   6   6  10 571  49]\n",
      " [ 71 198  21  54  10  11  25  32  69 509]]\n",
      "Average Total Loss over Batches: 1.0578072635844349\n",
      "[[494  74  47  29  17   7   6  30 183 113]\n",
      " [ 18 721   4  10   2   3   9   8  48 177]\n",
      " [ 83  34 305 107 146  84  63  88  43  47]\n",
      " [ 26  63  71 361  92 138  73  85  23  68]\n",
      " [ 27  31  94  95 419  39  86 160  13  36]\n",
      " [ 16  26  83 270  51 326  24 122  29  53]\n",
      " [ 10  35  46 108  88  32 572  39   9  61]\n",
      " [ 12  36  39  77  79  59   9 578   8 103]\n",
      " [ 98 146  16  10   7   3   3  11 601 105]\n",
      " [ 22 199   3  19   6   4  16  21  40 670]]\n",
      "Average Total Loss over Batches: 1.0223260707187654\n",
      "[[547  39  89  26  38  13  11  20 154  63]\n",
      " [ 26 635  13  17  14   9  14  10 100 162]\n",
      " [ 72   8 395  89 142 146  60  45  26  17]\n",
      " [ 24  24  92 346 105 243  73  44  22  27]\n",
      " [ 38  10 149  95 435  96  71  84   9  13]\n",
      " [  8   9 108 182  58 517  27  59  19  13]\n",
      " [ 11   9  73 109  91  80 577  19  11  20]\n",
      " [ 12  13  71  73 119 128  17 526   5  36]\n",
      " [135  80  30  22  12  11   8  10 630  62]\n",
      " [ 35 124  24  30  16  22  18  32  76 623]]\n",
      "Average Total Loss over Batches: 0.9940728587079048\n",
      "[[552  44  98  14  41   7  15  25 146  58]\n",
      " [ 35 675   6   8  15   5  23   9  76 148]\n",
      " [ 78  15 430  56 159  74  98  55  28   7]\n",
      " [ 35  35 136 250 135 164 132  59  23  31]\n",
      " [ 38  19 168  40 461  34 127  95   8  10]\n",
      " [ 10  16 194 160  96 339  72  77  18  18]\n",
      " [  7  10  74  51  90  42 671  17  16  22]\n",
      " [ 29  23  84  42 143  70  21 537   9  42]\n",
      " [152 107  30   7  23   3  13   9 603  53]\n",
      " [ 40 196  15  22  20   8  24  20  76 579]]\n",
      "Average Total Loss over Batches: 0.9588698878654093\n",
      "[[584  47  69   8  28   7  25  20 154  58]\n",
      " [ 36 689  10   5  13   4  23   7  89 124]\n",
      " [ 95  21 373  42 156  72 132  54  41  14]\n",
      " [ 37  49 132 203 120 146 181  64  35  33]\n",
      " [ 42  21 127  27 430  32 165 125  21  10]\n",
      " [ 18  13 185 124  92 326  92  91  40  19]\n",
      " [ 12  22  53  41  84  27 702  21  17  21]\n",
      " [ 27  29  73  44 123  45  46 555  12  46]\n",
      " [155  97  21   8  13   5  14   5 641  41]\n",
      " [ 48 212  17  13   9   8  39  17  86 551]]\n",
      "Average Total Loss over Batches: 0.9257250270975381\n",
      "[[479  41  87  40  28   5  12  18 235  55]\n",
      " [ 20 640  12  25   4   9   9   9 116 156]\n",
      " [ 73  13 395 111 138  91  67  51  47  14]\n",
      " [ 24  31 110 392 103 157  71  51  31  30]\n",
      " [ 26  18 185 103 412  43  78  97  22  16]\n",
      " [  5  11 133 254  69 356  36  77  42  17]\n",
      " [  8  12  71 109  99  48 570  24  31  28]\n",
      " [ 21  18  69  96 103  74  21 534  15  49]\n",
      " [ 92  74  31  19  16   6   9   7 696  50]\n",
      " [ 33 178  20  30   8  11  20  14 109 577]]\n",
      "Average Total Loss over Batches: 0.8953154314296693\n",
      "[[466  41 133  22  79  13  38  31  90  87]\n",
      " [ 22 623  19  22  23  11  40  12  49 179]\n",
      " [ 40   6 359  71 230  78 130  62  13  11]\n",
      " [  9  24 101 306 165 149 152  55   8  31]\n",
      " [ 18   8 105  55 523  30 144 102   6   9]\n",
      " [  0   7 136 183 124 361  86  77   5  21]\n",
      " [  2   4  48  69 115  27 697  17   5  16]\n",
      " [  6   7  62  71 168  63  53 530   1  39]\n",
      " [141  96  46  31  37  16  26   9 498 100]\n",
      " [ 17 130  27  27  30  19  46  28  32 644]]\n",
      "Average Total Loss over Batches: 0.8724069062164799\n",
      "[[507  69  49  28  25  11  10  26 200  75]\n",
      " [ 18 757   2  11   1   3  11   8  56 133]\n",
      " [ 88  30 332  91 120 125  90  57  41  26]\n",
      " [ 26  55  72 326  88 199 102  68  27  37]\n",
      " [ 37  37 128  92 348  74 125 118  16  25]\n",
      " [  7  24 103 205  51 428  49  85  20  28]\n",
      " [  8  39  52 108  53  59 611  17  20  33]\n",
      " [ 23  35  41  74  80  83  22 570  10  62]\n",
      " [ 98 142  17  12   8  12   9   8 627  67]\n",
      " [ 20 239   8  20   4  13  18  12  59 607]]\n",
      "Average Total Loss over Batches: 0.8564081833529472\n",
      "[[629  42  58  18  17   7  10  11 159  49]\n",
      " [ 46 674   9  16   1   4  12   7 108 123]\n",
      " [123  13 422 111  90  60  80  44  43  14]\n",
      " [ 56  40 109 389  64 133  97  55  30  27]\n",
      " [ 62  18 202 101 312  49 119 102  21  14]\n",
      " [ 18  13 154 270  37 339  46  71  34  18]\n",
      " [ 20  19  90 117  54  48 601  11  19  21]\n",
      " [ 42  22  90  99  68  57  31 525  17  49]\n",
      " [167  89  22  10   7   5   7   6 649  38]\n",
      " [ 55 184  16  30   5  11  18  15 101 565]]\n",
      "Average Total Loss over Batches: 0.8221072239068895\n",
      "[[502  33 119  30  61  14  30  27 117  67]\n",
      " [ 25 644  22  25  26  11  32  13  52 150]\n",
      " [ 54  10 398  76 204  92 105  40  15   6]\n",
      " [ 10  25 123 328 137 172 138  43  11  13]\n",
      " [ 24   9 143  54 503  60 115  81   5   6]\n",
      " [  3   4 139 205 104 369  85  69  11  11]\n",
      " [  4   6  64  69 107  39 675  16   8  12]\n",
      " [  8   7  96  81 160  71  43 511   5  18]\n",
      " [140  98  43  28  26  15  24   7 566  53]\n",
      " [ 34 141  25  45  30  24  41  26  48 586]]\n",
      "Average Total Loss over Batches: 0.7944230352087319\n",
      "[[581  43  75  24  35   8  16  19 135  64]\n",
      " [ 40 681   6  20   5   7  15  14  93 119]\n",
      " [ 91  17 389  87 153  71  87  63  31  11]\n",
      " [ 26  37  96 343 114 137 114  86  27  20]\n",
      " [ 37  15 145  74 412  47 104 139  15  12]\n",
      " [ 14  15 124 221  84 327  48 122  26  19]\n",
      " [  7  19  62  89  91  49 617  27  13  26]\n",
      " [ 20  22  76  58 104  48  25 607   7  33]\n",
      " [152 102  28  17  18   6   9   4 617  47]\n",
      " [ 43 179  18  31  10  14  24  28  87 566]]\n",
      "Average Total Loss over Batches: 0.7715786970161693\n",
      "[[612  33  64  11  28  14   8  22 172  36]\n",
      " [ 49 669   9   9  11  11   8  19 108 107]\n",
      " [127  11 420  58 151  94  43  49  40   7]\n",
      " [ 46  36 137 228 138 214  70  74  41  16]\n",
      " [ 66  16 188  41 452  68  38 108  15   8]\n",
      " [ 21  14 162 126  95 424  26  86  36  10]\n",
      " [ 26  21 106  74 136  86 486  19  28  18]\n",
      " [ 33  14  83  43 121  97  11 563  12  23]\n",
      " [154  85  23   6  14   6   4   9 664  35]\n",
      " [ 62 187  23  19  16  21  15  30 119 508]]\n",
      "Average Total Loss over Batches: 0.7464865508164465\n",
      "[[622  42  59  28  18  12  11  14 156  38]\n",
      " [ 54 649   9  25   3  13  10  10  96 131]\n",
      " [128  14 433 117  74  98  49  42  40   5]\n",
      " [ 53  25 123 427  51 167  57  50  34  13]\n",
      " [ 67  14 242 144 268  92  61  89  18   5]\n",
      " [ 20  11 122 291  38 393  31  58  25  11]\n",
      " [ 30  23 111 145  40  75 526  12  26  12]\n",
      " [ 42  18  90 138  67  98  13 494  17  23]\n",
      " [168  96  20  18   5   7   3   7 637  39]\n",
      " [ 64 195  19  57   7  15  13  17  99 514]]\n",
      "Average Total Loss over Batches: 0.7318847767668404\n",
      "[[534  73  23  14  21   9   5  21 236  64]\n",
      " [ 22 740   1   6   2   3   5  11  70 140]\n",
      " [132  32 249  87 136 112  58  91  71  32]\n",
      " [ 55  74  54 276  88 196  54 100  44  59]\n",
      " [ 64  47  91  66 355  80  85 144  33  35]\n",
      " [ 32  33  62 160  68 392  41 117  44  51]\n",
      " [ 23  67  50  98  78  91 498  31  20  44]\n",
      " [ 38  47  35  46  92  74  16 557  14  81]\n",
      " [100 139   4   5   8   5   5   6 658  70]\n",
      " [ 35 254   3   8   4   9   8  12  76 591]]\n",
      "Average Total Loss over Batches: 0.7165303326946497\n",
      "[[586  35 108  27  35  15  23  25 107  39]\n",
      " [ 46 669  22  27  10  11  32  15  60 108]\n",
      " [ 84  11 438  95  86  93 123  51  15   4]\n",
      " [ 36  25 132 305  79 190 142  56  20  15]\n",
      " [ 47   9 220  79 310  76 150  93  10   6]\n",
      " [ 11  11 148 199  47 406  88  75  12   3]\n",
      " [  9  12  89  77  53  48 669  22  12   9]\n",
      " [ 17  16 101  76  89  96  52 532   9  12]\n",
      " [177 103  50  24  14  10  12  12 555  43]\n",
      " [ 54 198  26  34  16  27  52  24  63 506]]\n",
      "Average Total Loss over Batches: 0.6871853229457513\n",
      "[[550  29  84  23  38  15  14  30 160  57]\n",
      " [ 48 565  18  20  23   7  31  16  73 199]\n",
      " [ 91   5 370  81 152  86 108  60  35  12]\n",
      " [ 31  21 118 278 125 177 112  66  39  33]\n",
      " [ 41  13 160  53 401  56 124 126  12  14]\n",
      " [  8  10 142 168  83 347  69 125  28  20]\n",
      " [ 12   9  71  59  98  46 633  28  18  26]\n",
      " [ 16   9  75  53 137  65  39 551  14  41]\n",
      " [145  68  43  18  15   9  17   6 612  67]\n",
      " [ 45 110  21  27  23  16  37  34  62 625]]\n",
      "Average Total Loss over Batches: 0.6663961479744129\n",
      "[[570  37  68  27  50  12  18  32 123  63]\n",
      " [ 44 604   8  24  20   8  17  24  68 183]\n",
      " [ 95  11 331  89 198  92  76  74  22  12]\n",
      " [ 32  27  83 337 137 182  87  74  17  24]\n",
      " [ 43  11 118  71 457  59  87 141   5   8]\n",
      " [  9  10  94 221  99 368  48 123  12  16]\n",
      " [ 13  13  63  92 121  63 571  32   8  24]\n",
      " [ 14  12  59  70 133  64  25 584  10  29]\n",
      " [159  85  39  18  20  10   7  12 588  62]\n",
      " [ 42 132  17  32  25  15  32  27  62 616]]\n",
      "Average Total Loss over Batches: 0.663733970605745\n",
      "[[525  60  61  17  29  12   9  25 181  81]\n",
      " [ 27 672   3  15   4   4   6  12  73 184]\n",
      " [105  19 342  91 129 120  70  60  41  23]\n",
      " [ 34  31  87 303  94 213  62  87  37  52]\n",
      " [ 38  17 141  79 372  91  72 139  23  28]\n",
      " [ 17  10  97 196  64 421  32 105  26  32]\n",
      " [ 14  30  66  98  87 101 501  43  22  38]\n",
      " [ 22  21  47  60  92  86  22 572  16  62]\n",
      " [123 109  23  11  10   7   7   6 627  77]\n",
      " [ 30 181  11  15   6  14  15  23  59 646]]\n",
      "Average Total Loss over Batches: 0.6397618587526959\n",
      "[[553  47  92  18  32   8  24  13 158  55]\n",
      " [ 36 655  15  20   8   6  29  11  76 144]\n",
      " [ 90  15 459  93 110  64  92  36  36   5]\n",
      " [ 37  30 144 319  91 135 142  52  28  22]\n",
      " [ 54  11 243  76 340  48 116  90  17   5]\n",
      " [ 10  12 180 204  62 328  95  66  31  12]\n",
      " [ 13  20 104  85  65  47 621  14  19  12]\n",
      " [ 26  17 122  74 112  65  54 491  14  25]\n",
      " [136 103  34  11  12   3  14   3 641  43]\n",
      " [ 47 196  24  27  17  14  35  20  79 541]]\n",
      "Average Total Loss over Batches: 0.6318671908613295\n",
      "[[637  14  92  24  21  25   5  11 149  22]\n",
      " [108 519  30  34   9  17   8   6 151 118]\n",
      " [128   4 466  93  53 159  35  15  41   6]\n",
      " [ 46  16 169 321  52 275  43  32  35  11]\n",
      " [ 87   6 288  98 240 140  45  69  22   5]\n",
      " [ 22   8 164 205  38 477  19  34  26   7]\n",
      " [ 36  19 143 130  59 135 424  12  31  11]\n",
      " [ 48  18 146 123  76 181  10 364  22  12]\n",
      " [206  51  43  21   7  18   1   5 622  26]\n",
      " [120 148  34  65   8  41  10  16 145 413]]\n",
      "Average Total Loss over Batches: 0.6107241636206489\n",
      "[[519  54  68  29  36  13  16  24 183  58]\n",
      " [ 32 663  11  23   4  12  16  12  81 146]\n",
      " [ 62  15 390 112 114 119  78  53  48   9]\n",
      " [ 27  37  98 359  71 215  70  64  31  28]\n",
      " [ 36  18 197 103 311 101  88 118  15  13]\n",
      " [ 10  14 105 226  53 423  49  77  23  20]\n",
      " [ 10  30  80 127  73  86 541  19  14  20]\n",
      " [ 17  22  80 111  83 107  26 496  16  42]\n",
      " [113 107  25  19  10  11   4  10 657  44]\n",
      " [ 37 196  18  40   9  17  24  21  75 563]]\n",
      "Average Total Loss over Batches: 0.5953238318040455\n",
      "[[620  33  61  30  26  13  13  27 122  55]\n",
      " [ 49 597  14  26  20   9  16  17  82 170]\n",
      " [107  11 345 115 139 105  64  66  38  10]\n",
      " [ 41  25 100 354  99 197  66  65  28  25]\n",
      " [ 58  11 147 108 374  69  79 132  12  10]\n",
      " [ 15   8  87 242  70 390  31 113  28  16]\n",
      " [ 18  19  72 125 107  71 514  33  20  21]\n",
      " [ 26  10  72  88 106  93  18 545  12  30]\n",
      " [198  79  21  23  15  11   6   9 576  62]\n",
      " [ 53 130  17  40  14  21  25  30  62 608]]\n",
      "Average Total Loss over Batches: 0.5918884511178243\n",
      "[[612  28  57  23  47  13   9  31 107  73]\n",
      " [ 52 576  14  19  16  14  19  19  72 199]\n",
      " [118   9 356  79 176 111  60  51  27  13]\n",
      " [ 46  26  96 277 135 200  80  75  28  37]\n",
      " [ 56  12 156  73 423  66  71 118   9  16]\n",
      " [ 12   9 107 181  96 404  37 103  20  31]\n",
      " [ 25  25  76  87 135  73 500  26  24  29]\n",
      " [ 25  12  79  64 137  92  25 511   7  48]\n",
      " [205  79  26  21  22   8   5   9 561  64]\n",
      " [ 54 140  13  27  19  18  15  21  52 641]]\n",
      "Average Total Loss over Batches: 0.5765767895488441\n",
      "[[587  24 108  34  41  19  29  14 109  35]\n",
      " [ 56 541  43  38  23  20  40  10  97 132]\n",
      " [ 79   5 430  94 127 122  98  18  24   3]\n",
      " [ 34   9 144 303  91 222 129  40  18  10]\n",
      " [ 48   4 224  87 332  96 125  68  11   5]\n",
      " [  8   9 156 207  60 432  69  40  11   8]\n",
      " [ 14   9  91  80  73  73 634   8  11   7]\n",
      " [ 23  11 144  93 117 144  61 389   9   9]\n",
      " [186  72  57  23  16  16  18   9 563  40]\n",
      " [ 80 131  46  53  24  36  61  22  73 474]]\n",
      "Average Total Loss over Batches: 0.5753878814354563\n",
      "[[565  26  66  34  46  17  11  38 137  60]\n",
      " [ 46 570  16  23  29  15  17  24  81 179]\n",
      " [ 88   7 358  99 175 114  59  62  30   8]\n",
      " [ 31  20 100 300 122 205  81  78  33  30]\n",
      " [ 40  12 141  77 437  70  62 140   8  13]\n",
      " [ 14   4 114 195  87 399  37 118  14  18]\n",
      " [ 14   8  73 106 149  76 500  34  17  23]\n",
      " [ 17   9  67  68 110 103  23 560   7  36]\n",
      " [161  67  28  29  23  11   6  17 594  64]\n",
      " [ 51 114  17  39  26  23  20  40  58 612]]\n",
      "Average Total Loss over Batches: 0.5639523104619747\n",
      "[[506  57 109  40  38  14  25  37 128  46]\n",
      " [ 37 661  13  32  12  16  31  17  69 112]\n",
      " [ 72  18 389 117 109 115 105  48  21   6]\n",
      " [ 24  34 111 337  76 202 119  67  15  15]\n",
      " [ 36  21 171 116 287  89 145 114  11  10]\n",
      " [  7   9 131 215  45 403  68  93  10  19]\n",
      " [  9  25  76 107  59  62 618  23  11  10]\n",
      " [ 15  21  84  94  82 106  49 517   6  26]\n",
      " [157 115  43  34  14  15  15  13 555  39]\n",
      " [ 45 229  20  44  15  27  43  21  65 491]]\n",
      "Average Total Loss over Batches: 0.5548091029987356\n",
      "[[517  60  57  17  53   8   9  44 178  57]\n",
      " [ 33 659  10  19  13   7   9  24  71 155]\n",
      " [ 90  20 356  98 153  80  60  84  48  11]\n",
      " [ 35  40 109 279 125 159  72 108  34  39]\n",
      " [ 50  24 175  84 354  60  58 162  16  17]\n",
      " [ 17  20 118 195  91 326  34 146  31  22]\n",
      " [ 18  38  94  98 126  72 467  39  25  23]\n",
      " [ 18  21  69  67 113  63  22 570  15  42]\n",
      " [115 120  27  13  22   6   6  15 623  53]\n",
      " [ 34 218  12  17  22  13  17  35  54 578]]\n",
      "Average Total Loss over Batches: 0.5406248609965807\n",
      "[[573  34  69  20  45  10  18  18 166  47]\n",
      " [ 56 595  15  19  12   9  31  11 112 140]\n",
      " [115  15 370  83 136  82  99  49  45   6]\n",
      " [ 48  26 133 279 122 128 124  70  44  26]\n",
      " [ 62  11 200  81 341  47 115 110  21  12]\n",
      " [ 20  14 177 199  72 292  85  93  31  17]\n",
      " [ 20  18  93  81  86  48 593  20  25  16]\n",
      " [ 31  15  97  73 109  72  49 514  17  23]\n",
      " [147  88  32  11  15   6  17   5 635  44]\n",
      " [ 68 179  23  24  17  16  42  24 113 494]]\n",
      "Average Total Loss over Batches: 0.5362867661414807\n",
      "[[528  38  75  37  50  21  15  43 120  73]\n",
      " [ 41 551  12  27  29  12  20  21  68 219]\n",
      " [ 77   7 329 126 178  99  83  69  22  10]\n",
      " [ 27  19  86 323 134 191  96  80  16  28]\n",
      " [ 33   8 119 104 406  61  91 151   7  20]\n",
      " [  6   8  97 232 101 355  44 122   9  26]\n",
      " [  5   7  60 120 149  73 521  32   9  24]\n",
      " [ 11   7  55  81 130  85  27 555  12  37]\n",
      " [157  72  32  36  24  11  13  15 555  85]\n",
      " [ 44 112  13  39  26  19  28  36  45 638]]\n",
      "Average Total Loss over Batches: 0.5247649622170208\n",
      "[[578  40  79  32  35  14  19  31 130  42]\n",
      " [ 54 633  13  31   8  17  26  18  82 118]\n",
      " [ 99  19 378 111  99 124  79  52  34   5]\n",
      " [ 45  27 115 298  81 223 101  73  24  13]\n",
      " [ 56  13 176 116 287 105 104 126  11   6]\n",
      " [ 13  12 126 209  51 423  61  73  17  15]\n",
      " [ 20  17  72 111  84  98 560  17  14   7]\n",
      " [ 24  16  76  93  89 114  36 518  13  21]\n",
      " [190  83  33  25  12  16  13   6 582  40]\n",
      " [ 67 203  18  45  12  26  36  27  90 476]]\n",
      "Average Total Loss over Batches: 0.5366282032387983\n",
      "[[565  26  79  25  36  14  14  29 170  42]\n",
      " [ 67 562  21  27  10  13  20  17 114 149]\n",
      " [ 97   7 406 106 131  77  70  62  37   7]\n",
      " [ 39  18 145 301 112 165  97  77  27  19]\n",
      " [ 59  14 198 103 337  72  86 113   9   9]\n",
      " [ 18   9 160 209  80 338  51  99  23  13]\n",
      " [ 21  10  95 106  95  71 527  39  24  12]\n",
      " [ 27  13 111  70 123  88  23 509  13  23]\n",
      " [168  68  30  22  18   9  12   7 618  48]\n",
      " [ 64 132  30  42  22  22  37  35 103 513]]\n",
      "Average Total Loss over Batches: 0.5051174007552374\n",
      "[[547  49  88  24  47  11  25  38 117  54]\n",
      " [ 41 626  14  23  21  14  26  21  63 151]\n",
      " [ 80  14 384 103 137  73 118  60  24   7]\n",
      " [ 26  24 122 304 103 159 132  94  16  20]\n",
      " [ 43  18 166  89 353  62 125 132   7   5]\n",
      " [  8  11 159 195  91 306  85 111  18  16]\n",
      " [  5  16  76  97  92  57 608  24  10  15]\n",
      " [ 17  13  88  74 114  75  50 541   8  20]\n",
      " [164 105  43  24  19  11  16  11 554  53]\n",
      " [ 52 194  22  36  25  16  40  31  47 537]]\n",
      "Average Total Loss over Batches: 0.4992452917915664\n",
      "[[586  41  73  22  54  21  16  35  88  64]\n",
      " [ 52 596   9  32  22  15  20  20  55 179]\n",
      " [ 97  15 312 105 185 112  75  72  19   8]\n",
      " [ 33  30  88 302 121 204 101  86  11  24]\n",
      " [ 42  13 120  95 410  81  90 134   4  11]\n",
      " [ 13  11 103 192  88 398  51 116   8  20]\n",
      " [ 10  19  68 105 135  78 534  26   8  17]\n",
      " [ 18  10  59  87 127  94  33 541   7  24]\n",
      " [201 100  39  24  21  16  14  16 501  68]\n",
      " [ 49 166  16  41  28  21  29  35  38 577]]\n",
      "Average Total Loss over Batches: 0.49300722409451614\n",
      "[[543  22  86  34  46  24  13  34 163  35]\n",
      " [ 70 548  28  35  22  17  21  21 118 120]\n",
      " [ 87   8 420 103 124 119  53  53  31   2]\n",
      " [ 38  12 135 313  89 248  65  68  24   8]\n",
      " [ 58   6 224  99 326 104  66  97  18   2]\n",
      " [  8   5 142 196  63 446  37  85  15   3]\n",
      " [ 13   9 120 117  96  89 508  25  17   6]\n",
      " [ 17   4 100  78  91 144  19 523   9  15]\n",
      " [150  59  41  28  16  23   3  13 635  32]\n",
      " [ 77 143  33  54  25  42  32  32 103 459]]\n",
      "Average Total Loss over Batches: 0.49663542090039237\n",
      "[[584  47  81  19  43   7  31  26 110  52]\n",
      " [ 54 626  13  22  15  10  36  13  67 144]\n",
      " [106  19 346  75 155  76 146  40  28   9]\n",
      " [ 40  36 122 265 116 149 173  51  24  24]\n",
      " [ 53  11 136  76 390  42 171  90  17  14]\n",
      " [ 15  16 136 183  87 317 124  84  18  20]\n",
      " [ 14  19  55  65  90  42 681  10  11  13]\n",
      " [ 24  22  74  61 141  79  78 479  10  32]\n",
      " [178 113  37  19  20   8  20   8 551  46]\n",
      " [ 67 192  16  34  21  16  52  18  74 510]]\n",
      "Average Total Loss over Batches: 0.47755694652864245\n",
      "[[557  49  63  34  31  16   7  25 151  67]\n",
      " [ 54 622   9  22   4  14  15  18  75 167]\n",
      " [100  22 350 133  85 141  63  56  38  12]\n",
      " [ 40  39  82 332  60 249  63  77  23  35]\n",
      " [ 62  26 156 133 262 121  76 127  16  21]\n",
      " [ 10  15  97 208  45 467  37  74  18  29]\n",
      " [ 16  46  65 140  62 130 471  27  15  28]\n",
      " [ 24  27  79  99  77 130  28 484  10  42]\n",
      " [162 112  35  21   9  19   9  10 572  51]\n",
      " [ 45 216  10  37  12  23  19  13  70 555]]\n",
      "Average Total Loss over Batches: 0.4799489148120827\n",
      "[[515  54  73  37  46  14   9  38 151  63]\n",
      " [ 46 647   9  37   8   9  14  18  62 150]\n",
      " [106  20 340 141 138 105  58  54  33   5]\n",
      " [ 34  41 102 371  90 187  63  73  20  19]\n",
      " [ 53  22 165 145 323  75  69 122  13  13]\n",
      " [ 18  21 121 261  58 356  37  89  18  21]\n",
      " [ 15  42  75 151  99  72 473  32  21  20]\n",
      " [ 23  25  77 109 101  92  26 510   6  31]\n",
      " [152 116  36  32  14  14   7   9 573  47]\n",
      " [ 50 226  19  45  16  20  21  23  55 525]]\n",
      "Average Total Loss over Batches: 0.46493391629046177\n",
      "[[501  53  74  24  50  17  14  29 164  74]\n",
      " [ 44 593  10  18  12  11  21  18  71 202]\n",
      " [ 88  11 332 104 175  95  92  52  39  12]\n",
      " [ 30  36  85 278 110 211 102  79  35  34]\n",
      " [ 40  17 119  86 408  81 109 106  17  17]\n",
      " [ 13  15 105 191  85 391  55  89  20  36]\n",
      " [ 12  30  55  89 116  74 546  30  21  27]\n",
      " [ 13  15  74  74 135  87  33 504  15  50]\n",
      " [141  90  31  20  14  15   8  10 605  66]\n",
      " [ 37 169  14  24  23  16  23  20  62 612]]\n",
      "Average Total Loss over Batches: 0.4666620951811818\n",
      "[[535  51  52  18  41  19   9  26 181  68]\n",
      " [ 54 600  11  18   4  16   7  21 105 164]\n",
      " [111  15 313 105 141 145  53  59  45  13]\n",
      " [ 39  35  88 264  99 268  46  93  37  31]\n",
      " [ 64  17 139  93 331 122  52 143  26  13]\n",
      " [ 19  17 102 169  74 447  27  91  31  23]\n",
      " [ 25  45  77 129 111 139 387  33  30  24]\n",
      " [ 23  28  65  73 103 129  20 503  14  42]\n",
      " [141  93  19  19  15  12   7   7 640  47]\n",
      " [ 48 208  11  25  16  30  11  19  79 553]]\n",
      "Average Total Loss over Batches: 0.4498568708195398\n",
      "[[552  32  82  35  26  24  21  36 156  36]\n",
      " [ 58 564  21  35  10  18  29  19  96 150]\n",
      " [ 97   9 394 131  82 122  80  51  31   3]\n",
      " [ 34  17 138 332  74 218  82  67  25  13]\n",
      " [ 66  10 214 128 252 106  95 108  16   5]\n",
      " [ 15   7 149 254  46 384  48  78  12   7]\n",
      " [ 14  19 114 144  64  92 511  22  13   7]\n",
      " [ 23  12 112 103  76 131  35 483  10  15]\n",
      " [154  76  45  23  14  20  13  10 606  39]\n",
      " [ 58 169  25  57  19  34  33  24  89 492]]\n",
      "Average Total Loss over Batches: 0.46270811772148357\n",
      "[[485  42  83  24  49  19  10  23 205  60]\n",
      " [ 41 629   9  22  10  11  12  20  91 155]\n",
      " [ 98  15 356 116 136 107  58  52  51  11]\n",
      " [ 37  42 112 331  99 193  64  61  29  32]\n",
      " [ 47  16 161 115 353  77  77 105  27  22]\n",
      " [ 14  15 128 238  66 367  39  80  26  27]\n",
      " [ 19  36  77 137 108  89 459  21  27  27]\n",
      " [ 25  24  83  97 110 105  33 469   8  46]\n",
      " [120  91  26  25  22   9   8   7 644  48]\n",
      " [ 45 217  16  29  18  24  16  18  82 535]]\n",
      "Average Total Loss over Batches: 0.43555008067016754\n",
      "[[554  35  82  35  43  12  13  21 153  52]\n",
      " [ 63 583  16  37  11  11  19  15  81 164]\n",
      " [ 97  16 355 145 130  93  74  49  34   7]\n",
      " [ 33  29 108 380  88 165  72  72  19  34]\n",
      " [ 49  13 155 144 347  72  88 100  16  16]\n",
      " [ 13  11 118 263  69 358  35  82  22  29]\n",
      " [ 15  24  80 153  92  87 496  21  15  17]\n",
      " [ 24  15  85 120 110  97  31 468  11  39]\n",
      " [162  84  32  33  20  14  11   7 589  48]\n",
      " [ 56 190  18  40  17  22  27  14  80 536]]\n",
      "Average Total Loss over Batches: 0.446547096844105\n",
      "[[504  40  74  15  67   7  14  29 190  60]\n",
      " [ 56 562  10  12  24  10  12  22 115 177]\n",
      " [100   9 343  83 201  70  71  63  49  11]\n",
      " [ 40  31 128 237 176 141  79  86  44  38]\n",
      " [ 37  15 148  59 435  40  93 130  29  14]\n",
      " [ 21  15 148 170 124 279  56 120  35  32]\n",
      " [ 15  20  82  68 161  57 510  36  26  25]\n",
      " [ 28  14  77  54 169  67  34 496  18  43]\n",
      " [142  85  27  12  24   7  10   8 628  57]\n",
      " [ 51 165  20  20  35  15  25  30 100 539]]\n",
      "Average Total Loss over Batches: 0.4406901703583548\n",
      "[[498  50  68  25  55  26  13  31 163  71]\n",
      " [ 43 592  11  29  10  18  10  21  71 195]\n",
      " [ 81  12 342 106 135 142  65  65  36  16]\n",
      " [ 33  32 101 288  84 261  60  82  19  40]\n",
      " [ 40  22 169  97 328 123  65 125  16  15]\n",
      " [ 14  12 110 180  68 449  33  89  17  28]\n",
      " [ 11  39  94 124 104 127 437  29  16  19]\n",
      " [ 18  21  77  89  90 133  24 491  10  47]\n",
      " [135  95  35  26  23  19   9  12 570  76]\n",
      " [ 38 182  16  27  21  24  12  21  61 598]]\n",
      "Average Total Loss over Batches: 0.4338744670715416\n",
      "[[511  26  64  37  57  17  10  33 201  44]\n",
      " [ 69 491  19  27  25  13  15  20 131 190]\n",
      " [ 83   4 314 121 189 109  70  51  52   7]\n",
      " [ 35  12  98 331 116 212  79  62  34  21]\n",
      " [ 47   4 138  95 418  68  98  95  27  10]\n",
      " [ 13   8 101 228  90 391  37  92  22  18]\n",
      " [ 18  13  67 110 140  73 511  23  27  18]\n",
      " [ 24   6  73  98 144 102  24 484  18  27]\n",
      " [135  56  21  26  18  12   6  12 671  43]\n",
      " [ 63 120  16  46  23  21  24  33  99 555]]\n",
      "Average Total Loss over Batches: 0.4361684382991039\n",
      "[[527  66  41  20  39  18   5  39 153  92]\n",
      " [ 42 634   4  11   6   8   2  20  71 202]\n",
      " [116  25 280  80 157 115  49 108  45  25]\n",
      " [ 38  61  65 265 114 189  46 119  28  75]\n",
      " [ 62  27 124  79 366  72  50 171  19  30]\n",
      " [ 16  26  89 186  82 350  28 148  23  52]\n",
      " [ 25  46  72 112 122  96 402  49  25  51]\n",
      " [ 23  30  50  56  95  70  14 572  15  75]\n",
      " [170 134  20  12  11   9   0  10 552  82]\n",
      " [ 31 203   5  17  14  14   7  20  51 638]]\n",
      "Average Total Loss over Batches: 0.4255832438026904\n",
      "[[578  38  75  25  43  18  15  21 145  42]\n",
      " [ 64 589  19  23  14  13  24  15 104 135]\n",
      " [113  11 344 103 148 111  81  47  38   4]\n",
      " [ 48  32 112 294 110 189 110  62  23  20]\n",
      " [ 67  12 177  87 371  69  97  96  17   7]\n",
      " [ 17  11 132 208  78 383  59  85  16  11]\n",
      " [ 18  24  94  97 100  76 531  24  23  13]\n",
      " [ 40  15  92  84 120 102  38 477  11  21]\n",
      " [173 103  29  25  14  15   9   9 589  34]\n",
      " [ 82 184  22  42  24  23  33  24 101 465]]\n",
      "Average Total Loss over Batches: 0.42898868730157147\n",
      "[[543  46  78  24  45  16  19  39 141  49]\n",
      " [ 45 640  14  24  17  12  26  30  72 120]\n",
      " [ 86  11 356 112 139  95  91  69  34   7]\n",
      " [ 29  32 105 319  97 188 107  87  20  16]\n",
      " [ 45  23 166 101 332  75 110 127  13   8]\n",
      " [ 10  14 117 209  73 365  57 123  13  19]\n",
      " [  7  22  75 111  81  77 564  31  16  16]\n",
      " [ 20  16  80  83  95  79  37 552  14  24]\n",
      " [155 112  35  30  18  15  14  12 566  43]\n",
      " [ 52 212  16  35  24  21  28  24  69 519]]\n",
      "Average Total Loss over Batches: 0.41924814362203006\n",
      "[[510  46  85  39  43  22  19  44 127  65]\n",
      " [ 42 584  12  35   9  19  19  22  64 194]\n",
      " [ 91  13 329 126 117 149  73  61  28  13]\n",
      " [ 28  29  87 314  83 255  77  83  15  29]\n",
      " [ 36  16 163 111 304 121  81 142  11  15]\n",
      " [  7  12  97 205  52 453  42  95  11  26]\n",
      " [  9  27  68 120  89 116 501  34  13  23]\n",
      " [ 17  11  71  82  78 136  32 522  11  40]\n",
      " [162 105  35  32  19  19  10  13 536  69]\n",
      " [ 46 164  19  37  13  28  19  20  49 605]]\n",
      "Average Total Loss over Batches: 0.4166285382078584\n",
      "[[517  39 108  25  48  16  20  28 140  59]\n",
      " [ 53 562  20  25  23  15  24  12  88 178]\n",
      " [ 87  11 386  92 167 103  76  44  25   9]\n",
      " [ 26  26 146 247 136 214  90  61  21  33]\n",
      " [ 38  10 182  71 404  67  99  92  20  17]\n",
      " [  8   8 164 190  92 355  65  87  16  15]\n",
      " [ 13  23 102  80 100  73 552  22  18  17]\n",
      " [ 22  14 108  69 137 108  37 463   8  34]\n",
      " [153  81  46  27  26  11   8   8 580  60]\n",
      " [ 59 165  24  32  25  24  29  15  72 555]]\n",
      "Average Total Loss over Batches: 0.4160808126903372\n",
      "[[552  35 114  25  41  15  25  30 121  42]\n",
      " [ 65 590  28  26  19  14  31  18  78 131]\n",
      " [101   9 395 101 132 107  84  47  21   3]\n",
      " [ 35  26 154 282 101 195 106  69  15  17]\n",
      " [ 58   9 209  97 321  81 103 105  10   7]\n",
      " [ 12   8 166 212  62 375  60  78  16  11]\n",
      " [ 14  18 112  92  88  94 541  18  14   9]\n",
      " [ 25  14 116  86 108 115  41 470   6  19]\n",
      " [198  83  45  25  20  19  15  10 551  34]\n",
      " [ 74 188  29  42  26  29  36  26  74 476]]\n",
      "Average Total Loss over Batches: 0.40731433234345166\n",
      "[[504  53  63  29  41  19  12  28 178  73]\n",
      " [ 47 617   9  18  12  13  14  19  67 184]\n",
      " [ 95  18 317 106 130 145  72  62  39  16]\n",
      " [ 32  38  87 300  85 245  67  84  22  40]\n",
      " [ 47  18 138 103 309 118  84 139  20  24]\n",
      " [ 11  14 103 188  58 431  41 100  20  34]\n",
      " [ 12  36  71 115  84 111 484  35  19  33]\n",
      " [ 17  21  67  81  83 131  32 502  11  55]\n",
      " [135  97  29  24  18  22   8   9 596  62]\n",
      " [ 43 191  12  25  15  26  16  16  71 585]]\n",
      "Average Total Loss over Batches: 0.39460632792858247\n",
      "[[544  58  67  20  50   9  19  42 127  64]\n",
      " [ 43 649   7  14  13   7  17  15  67 168]\n",
      " [105  20 345  92 145  93  93  63  31  13]\n",
      " [ 42  40 109 274 115 177 105  83  25  30]\n",
      " [ 52  18 137  78 387  64 108 124  18  14]\n",
      " [ 14  20 131 217  84 325  57 110  14  28]\n",
      " [ 17  27  85  88  95  63 562  26  18  19]\n",
      " [ 22  21  84  70 112  68  42 524  10  47]\n",
      " [171 121  31  19  19  10  12  13 540  64]\n",
      " [ 49 207  16  30  20  19  27  26  60 546]]\n",
      "Average Total Loss over Batches: 0.4020773823341189\n",
      "[[514  43  82  39  53  16  23  37 134  59]\n",
      " [ 53 584  13  37  16  14  30  25  71 157]\n",
      " [ 79  13 320 114 162 111  92  66  31  12]\n",
      " [ 26  26  95 307 122 199 103  80  16  26]\n",
      " [ 40  10 131 106 358  83 123 129  11   9]\n",
      " [ 12  10 115 211  80 374  55 105  11  27]\n",
      " [ 11  15  76 114 107  69 550  30  11  17]\n",
      " [ 15  12  68 107 120  96  43 499   8  32]\n",
      " [164  91  40  27  23  15  14  13 555  58]\n",
      " [ 46 174  25  34  23  25  36  27  60 550]]\n",
      "Average Total Loss over Batches: 0.3971788391181303\n",
      "[[493  59  99  19  51  18  20  33 133  75]\n",
      " [ 38 624  11  18  15  11  18  15  60 190]\n",
      " [ 81  16 347  87 152 112  94  64  24  23]\n",
      " [ 28  44 122 267 106 200  91  84  14  44]\n",
      " [ 38  21 156  87 355  81 107 129   8  18]\n",
      " [ 10  21 134 190  87 341  66  96  16  39]\n",
      " [  7  31  85  92 103  79 537  29  12  25]\n",
      " [ 19  26  85  68 115 102  47 492   4  42]\n",
      " [147 133  37  27  28  14  11  10 520  73]\n",
      " [ 29 204  19  24  27  18  24  16  47 592]]\n",
      "Average Total Loss over Batches: 0.388300259452702\n",
      "[[555  45  75  31  37  13  18  38 121  67]\n",
      " [ 52 600  11  31  12  14  21  23  80 156]\n",
      " [ 91  15 340 115 122 115  92  71  29  10]\n",
      " [ 36  37 105 315  82 204  89  93  17  22]\n",
      " [ 51  17 159 115 314  81 107 132   9  15]\n",
      " [ 18  12 117 208  60 386  57 111  13  18]\n",
      " [ 10  23  79 122  78  79 545  33  14  17]\n",
      " [ 18  18  69  82  84 102  32 553  10  32]\n",
      " [184 106  47  28  15  14  10  14 523  59]\n",
      " [ 49 176  21  41  14  23  29  25  60 562]]\n",
      "Average Total Loss over Batches: 0.3867254246993977\n",
      "[[504  31 117  44  50  24  31  40 112  47]\n",
      " [ 46 522  26  48  33  21  39  30  72 163]\n",
      " [ 70   4 349 123 153 118  94  63  18   8]\n",
      " [ 20   9 111 345 111 210  95  75  11  13]\n",
      " [ 28   7 153 115 370  84 116 112   8   7]\n",
      " [  9   1 113 237  86 380  57  97  10  10]\n",
      " [  6   6  78 122 115  74 563  19   9   8]\n",
      " [  9   5  92 100 120 120  44 482   5  23]\n",
      " [159  66  56  56  38  21  17  11 520  56]\n",
      " [ 49 127  27  59  35  38  39  42  59 525]]\n",
      "Average Total Loss over Batches: 0.38688016757347066\n",
      "[[498  72  66  20  36  14  20  39 144  91]\n",
      " [ 36 627   6   9   8  12  15  16  71 200]\n",
      " [ 82  18 311 109 119 120 100  75  38  28]\n",
      " [ 30  49  89 290  87 185  95  92  27  56]\n",
      " [ 42  27 138 104 272  78 131 164  19  25]\n",
      " [ 17  23 104 187  55 359  62 130  19  44]\n",
      " [ 12  35  71  98  71  70 560  32  17  34]\n",
      " [ 20  22  56  64  80  97  41 543   9  68]\n",
      " [137 147  29  19  13   9   6  10 547  83]\n",
      " [ 35 188  11  22   8  13  17  27  45 634]]\n",
      "Average Total Loss over Batches: 0.3913247685616667\n",
      "[[577  50  52  20  23  14  10  20 169  65]\n",
      " [ 69 614   6  14   3   9  11   9  87 178]\n",
      " [138  17 311  98 126 105  73  58  57  17]\n",
      " [ 59  49 100 303  84 176  71  76  38  44]\n",
      " [ 74  22 144 104 324  80  93 114  26  19]\n",
      " [ 28  22 120 206  68 358  39  96  31  32]\n",
      " [ 32  38  77 108 103  80 477  23  30  32]\n",
      " [ 46  26  74  73  90  96  34 481  17  63]\n",
      " [177 101  19  15   7   9   4   7 605  56]\n",
      " [ 64 214   9  20  13  16  14  10  72 568]]\n",
      "Average Total Loss over Batches: 0.3752230169450413\n",
      "[[515  50  84  18  36  12  18  26 190  51]\n",
      " [ 54 601  17  15  17  15  14  17 104 146]\n",
      " [ 94  16 397  71 134 111  75  48  47   7]\n",
      " [ 43  36 141 238 124 201  88  62  39  28]\n",
      " [ 54  13 192  66 369  68  90 113  28   7]\n",
      " [ 19  19 163 163  73 379  46  83  31  24]\n",
      " [ 18  27 106  76  99  88 512  24  31  19]\n",
      " [ 24  21 126  45 128 108  40 462  16  30]\n",
      " [135  97  34  15  19  12   5  11 634  38]\n",
      " [ 62 194  29  26  21  25  23  19  97 504]]\n",
      "Average Total Loss over Batches: 0.3903276163454319\n",
      "[[540  48  85  24  47  15  16  34 147  44]\n",
      " [ 68 581  15  24  18  10  22  22  98 142]\n",
      " [104   8 357  98 140 107  77  68  37   4]\n",
      " [ 40  31 146 284 117 179  84  76  23  20]\n",
      " [ 61   9 177 101 363  75  86 110  14   4]\n",
      " [ 16   8 143 211  83 350  52 106  20  11]\n",
      " [ 23  27 105  95 122  75 486  34  21  12]\n",
      " [ 25  13  94  78 125 109  29 493  10  24]\n",
      " [178  95  43  28  16  10  10  11 559  50]\n",
      " [ 65 179  25  37  27  20  31  37  75 504]]\n",
      "Average Total Loss over Batches: 0.37455544157590254\n",
      "[[542  49  97  30  39  11  29  33 120  50]\n",
      " [ 55 619  16  36  13  15  28  15  72 131]\n",
      " [ 96  15 336 121 131 115 105  52  23   6]\n",
      " [ 39  32 111 299  90 212 103  64  19  31]\n",
      " [ 49  16 151 116 315  81 124 121  12  15]\n",
      " [ 16  11 125 223  59 384  69  83  15  15]\n",
      " [ 10  23  77 107  80  80 571  23  14  15]\n",
      " [ 17  18  91  88 103 118  48 481   7  29]\n",
      " [195 107  39  31  15  13  19  13 521  47]\n",
      " [ 61 186  21  38  16  24  43  23  72 516]]\n",
      "Average Total Loss over Batches: 0.37866323666392343\n",
      "[[493  48  83  36  51  22  17  30 161  59]\n",
      " [ 51 570  15  36  19  19  22  21  81 166]\n",
      " [ 90  14 325 123 145 127  78  52  37   9]\n",
      " [ 31  30 110 307  96 233  95  61  19  18]\n",
      " [ 49  13 155 131 323  86 102 114  13  14]\n",
      " [ 10   9 108 239  67 408  50  74  15  20]\n",
      " [ 12  23  76 128  84  98 519  24  21  15]\n",
      " [ 15  17  84 109 110 120  35 464   8  38]\n",
      " [140  97  33  38  19  17  10   9 578  59]\n",
      " [ 48 176  19  40  21  28  29  23  72 544]]\n",
      "Average Total Loss over Batches: 0.3668583920330803\n",
      "[[543  42  78  31  38  17  12  36 149  54]\n",
      " [ 53 562  14  37   8  20  14  30  93 169]\n",
      " [101  11 358 126 110 131  55  71  29   8]\n",
      " [ 33  21 105 320  84 237  54 100  18  28]\n",
      " [ 51   9 170 125 279 109  64 167  13  13]\n",
      " [ 14   9 108 223  60 423  36  95  13  19]\n",
      " [ 13  19  99 143  91 113 442  40  18  22]\n",
      " [ 21  12  83  88  79 120  23 529   9  36]\n",
      " [173  83  38  29  14  18   7  13 577  48]\n",
      " [ 45 160  19  37  17  31  18  27  77 569]]\n",
      "Average Total Loss over Batches: 0.3714048912773363\n",
      "[[598  34  72  30  41   9  18  20 142  36]\n",
      " [101 521  25  34  19  15  28  14 116 127]\n",
      " [125   9 356  89 157 103  87  39  26   9]\n",
      " [ 55  19 150 265 126 176 118  54  26  11]\n",
      " [ 75   6 213  83 364  64 107  69  15   4]\n",
      " [ 35   6 168 177 100 324  77  80  25   8]\n",
      " [ 30  14 119  85 108  58 542  16  22   6]\n",
      " [ 46  12 117  83 137 110  45 411  15  24]\n",
      " [224  76  30  24  25  13  15   9 558  26]\n",
      " [109 158  37  39  29  35  48  29  98 418]]\n",
      "Average Total Loss over Batches: 0.3727726571647118\n",
      "[[503  59  64  23  36  12  14  43 174  72]\n",
      " [ 51 596   6  18   9   8  13  20  85 194]\n",
      " [ 95  11 311  93 175 102  77  74  46  16]\n",
      " [ 33  38  93 261 131 181  87 100  33  43]\n",
      " [ 44  13 131  89 387  62  88 144  22  20]\n",
      " [ 15  16 116 194  90 329  51 124  30  35]\n",
      " [ 14  28  72  92 130  76 490  42  23  33]\n",
      " [ 21  16  78  54 114  84  29 535  14  55]\n",
      " [145  99  29  19  15  11   7  10 598  67]\n",
      " [ 40 168  14  21  18  17  24  26  80 592]]\n",
      "Average Total Loss over Batches: 0.3733322104953093\n",
      "[[532  39 107  28  44  16  26  37 124  47]\n",
      " [ 55 558  26  30  18  16  49  25  74 149]\n",
      " [ 90  11 384  87 138  94 104  63  22   7]\n",
      " [ 28  24 152 232 112 192 148  71  17  24]\n",
      " [ 52   8 197  84 325  65 140 112  10   7]\n",
      " [ 11   9 162 174  77 366  86  92  12  11]\n",
      " [ 11  16  98  74  84  72 588  32  14  11]\n",
      " [ 19  13 113  63 122 108  55 479   6  22]\n",
      " [175  97  48  27  24  18  19  11 528  53]\n",
      " [ 55 156  28  39  31  31  63  30  57 510]]\n",
      "Average Total Loss over Batches: 0.36200028360286307\n",
      "[[577  46  80  22  42   9  19  30 130  45]\n",
      " [ 65 588  16  25  15  14  30  13  93 141]\n",
      " [111  11 372  89 149  84 100  44  32   8]\n",
      " [ 44  35 147 271 114 159 128  59  19  24]\n",
      " [ 59  11 188  84 364  59 133  81  14   7]\n",
      " [ 21  16 162 199  84 318  87  82  17  14]\n",
      " [ 17  17  94  78  92  50 589  22  22  19]\n",
      " [ 36  10 115  78 138  93  58 438   9  25]\n",
      " [198 108  35  20  22   9  15  10 542  41]\n",
      " [ 62 191  26  30  24  21  40  22  78 506]]\n",
      "Average Total Loss over Batches: 0.3704998943585681\n",
      "[[581  62  47  11  27  11   7  17 169  68]\n",
      " [ 68 573   7  11   7   7   8  12  99 208]\n",
      " [152  22 330  80 116 121  46  53  55  25]\n",
      " [ 60  51 119 248  81 208  49  90  39  55]\n",
      " [ 87  22 172  95 294 100  60 121  23  26]\n",
      " [ 31  15 131 162  75 388  31  90  35  42]\n",
      " [ 38  46 105 105 102 100 389  37  35  43]\n",
      " [ 54  30  78  66  85 106  25 464  13  79]\n",
      " [175 104  21  10  10  14   4   7 596  59]\n",
      " [ 64 214  12  19  15  11  12  15  71 567]]\n",
      "Average Total Loss over Batches: 0.3730461059781477\n",
      "[[502  59  75  19  52  21  20  35 153  64]\n",
      " [ 45 594   9  22  15  13  21  21  72 188]\n",
      " [ 89  15 324 101 149 122  91  62  32  15]\n",
      " [ 31  35 101 276 107 198 111  86  22  33]\n",
      " [ 48  18 138  89 350  83 120 121  15  18]\n",
      " [ 10  15 116 194  81 376  67  99  20  22]\n",
      " [ 10  17  87  80  94  75 557  36  22  22]\n",
      " [ 24  18  73  73 108 123  34 496   9  42]\n",
      " [150 113  27  18  19  16  12  11 568  66]\n",
      " [ 46 175  15  22  18  21  30  18  51 604]]\n",
      "Average Total Loss over Batches: 0.35503197510929474\n",
      "[[521  44  84  32  42  23  18  33 153  50]\n",
      " [ 65 542  21  37  18  21  21  31  73 171]\n",
      " [ 86  10 358 112 128 141  67  61  30   7]\n",
      " [ 29  18 121 309 102 228  62  89  21  21]\n",
      " [ 44  13 178 112 317  95  71 146  10  14]\n",
      " [ 14   7 123 215  76 394  41 104  13  13]\n",
      " [  8  14 111 136 108  98 445  41  18  21]\n",
      " [ 17  14  96  91 111 123  20 500   5  23]\n",
      " [164  76  39  36  23  17   9  12 571  53]\n",
      " [ 52 155  28  43  30  34  31  35  57 535]]\n",
      "Average Total Loss over Batches: 0.3552989213971357\n",
      "[[560  49  85  25  48  15  27  39  97  55]\n",
      " [ 67 574  17  26  19  16  31  30  65 155]\n",
      " [ 96  13 336  92 143 128  91  71  21   9]\n",
      " [ 40  28 110 263 110 228 107  78  13  23]\n",
      " [ 52  11 162  83 357  79 109 130   7  10]\n",
      " [ 12   7 126 168  82 409  66 103  14  13]\n",
      " [ 11  19  90  88  93  86 553  36  14  10]\n",
      " [ 18  11  88  68 130 116  45 491   5  28]\n",
      " [204 107  40  22  17  19  14  15 497  65]\n",
      " [ 53 174  27  33  29  29  40  31  46 538]]\n",
      "Average Total Loss over Batches: 0.3552140965425203\n",
      "[[525  51  81  40  44  19  21  20 132  67]\n",
      " [ 61 565   9  30  18  17  30  25  72 173]\n",
      " [ 99  16 298 120 148 121  90  58  36  14]\n",
      " [ 38  36  77 296 102 212 109  74  21  35]\n",
      " [ 51  15 126 125 336  86 114 115  13  19]\n",
      " [ 13  12 103 203  86 385  70  86  17  25]\n",
      " [ 18  16  74 115 101  83 531  23  16  23]\n",
      " [ 24  17  66  93 115 123  43 456  10  53]\n",
      " [188  98  31  25  18  18  10  10 531  71]\n",
      " [ 45 184  15  32  21  29  28  18  65 563]]\n",
      "Average Total Loss over Batches: 0.3492315254324494\n",
      "[[514  56  95  24  43  17  20  38 140  53]\n",
      " [ 52 597  11  25  19  14  20  20  77 165]\n",
      " [ 93   9 328  93 162 120  83  70  31  11]\n",
      " [ 29  32 101 285 118 193  96  92  18  36]\n",
      " [ 45  18 147  96 378  72  96 125  11  12]\n",
      " [ 12  12 110 191  96 371  62 110  17  19]\n",
      " [ 16  16  86  92 119  78 531  28  16  18]\n",
      " [ 25  17  74  70 124 114  37 501   9  29]\n",
      " [181 102  31  21  26  17   9  16 538  59]\n",
      " [ 43 183  20  30  28  28  30  29  56 553]]\n",
      "Average Total Loss over Batches: 0.35228769883674804\n",
      "[[545  40  82  24  39  14  18  29 168  41]\n",
      " [ 67 554  27  25  19  20  17  20  98 153]\n",
      " [ 96  11 383 104 138  97  78  49  39   5]\n",
      " [ 46  22 144 301  97 189 100  62  20  19]\n",
      " [ 60   9 191 124 335  74  86 100  17   4]\n",
      " [ 20   9 157 212  78 351  66  76  19  12]\n",
      " [ 18  13 106 116  91  75 533  21  16  11]\n",
      " [ 34  10 110  91 121 118  37 450   8  21]\n",
      " [166  89  34  21  26  14  11  10 584  45]\n",
      " [ 63 170  26  40  31  28  40  27  75 500]]\n",
      "Average Total Loss over Batches: 0.3601760083333157\n",
      "[[521  54  64  35  44  19  13  44 136  70]\n",
      " [ 52 554  11  35  12  18   7  29  77 205]\n",
      " [ 83  13 287 126 160 124  59  95  36  17]\n",
      " [ 32  29  74 337 110 198  51 105  17  47]\n",
      " [ 38  22 121 131 359  77  61 162  12  17]\n",
      " [ 10  11  97 225  83 354  31 137  17  35]\n",
      " [ 13  25  75 165 115  90 418  47  16  36]\n",
      " [ 21  18  53  91  93 109  19 532   5  59]\n",
      " [158 102  27  32  24  20   4  15 530  88]\n",
      " [ 40 174  13  26  17  28  13  25  60 604]]\n",
      "Average Total Loss over Batches: 0.3444512301013833\n",
      "[[511  35 113  29  62  17  26  30 136  41]\n",
      " [ 59 528  31  30  30  20  34  23  93 152]\n",
      " [ 73  10 392  89 164 101  96  44  24   7]\n",
      " [ 34  18 129 277 128 204 112  63  18  17]\n",
      " [ 41   8 176  89 379  71 118 102  11   5]\n",
      " [ 16   8 155 184  98 351  69  88  13  18]\n",
      " [ 13  14  90  92 103  73 563  27  14  11]\n",
      " [ 22   9 106  64 142 114  44 467   9  23]\n",
      " [163  86  46  33  35  20  16   9 555  37]\n",
      " [ 56 165  33  36  34  35  41  31  80 489]]\n",
      "Average Total Loss over Batches: 0.4289073179284268\n",
      "[[543  56  82  24  50  12  30  26 128  49]\n",
      " [ 58 628  20  20  17  13  36  17  71 120]\n",
      " [ 93  17 335  84 184  70 138  47  27   5]\n",
      " [ 30  39 143 269 133 122 171  57  17  19]\n",
      " [ 49  15 143  71 389  48 181  84  10  10]\n",
      " [ 14  14 160 193 110 279 122  79  16  13]\n",
      " [ 18  14  68  69  95  32 667  16  12   9]\n",
      " [ 25  23  99  61 156  68  83 452  10  23]\n",
      " [181 122  33  25  30   9  21   9 520  50]\n",
      " [ 60 207  24  36  30  20  56  26  66 475]]\n",
      "Average Total Loss over Batches: 0.34237895939284485\n",
      "[[537  50  75  23  42  18  10  27 167  51]\n",
      " [ 64 585  13  21  11  16  13  23  96 158]\n",
      " [105  13 367  97 132 105  69  54  47  11]\n",
      " [ 49  40 115 300 102 176  92  62  31  33]\n",
      " [ 59  23 165  95 343  67 103 115  18  12]\n",
      " [ 21  11 151 200  83 344  48  85  30  27]\n",
      " [ 20  24  92 107 104  77 505  27  26  18]\n",
      " [ 31  25 103  81 125  85  37 454  15  44]\n",
      " [159  90  28  23  22  12   5   7 606  48]\n",
      " [ 58 195  20  29  26  21  22  19  91 519]]\n",
      "Average Total Loss over Batches: 0.3433227022518234\n",
      "[[470  43  89  32  41  23   7  25 219  51]\n",
      " [ 54 542  18  29  18  24   8  16 116 175]\n",
      " [ 77  13 338 111 146 141  41  61  61  11]\n",
      " [ 32  23 109 315 101 237  40  82  32  29]\n",
      " [ 47  10 160 120 346 109  47 127  22  12]\n",
      " [ 11  10 112 212  77 417  22  86  34  19]\n",
      " [ 12  22 101 147 130 126 374  30  32  26]\n",
      " [ 22  16  87  97 116 129  13 468  11  41]\n",
      " [125  82  29  20  21  18   6   8 648  43]\n",
      " [ 48 174  25  33  25  34  17  25 102 517]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "for epochs in range(100):\n",
    "  total_loss = 0\n",
    "  for batch in range( train_X.shape[0] // batch_size ):\n",
    "    x_batch, y_batch = get_batch(train_X, train_Y, batch_size)\n",
    "\n",
    "    cnn_optimizer.zero_grad()\n",
    "    logits = cifar_model( x_batch )\n",
    "    loss = loss_function( logits, y_batch )\n",
    "\n",
    "    loss.backward()\n",
    "    cnn_optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  print( \"Average Total Loss over Batches:\", total_loss / ( train_X.shape[0] // batch_size ) )\n",
    "  print( confusion_matrix( cifar_model, test_X, test_Y ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ta4Mwqto9y6s"
   },
   "source": [
    "There are some interesting narratives that develop over the course of training - for instance the back and forth misidentifications between cars and trucks.\n",
    "\n",
    "If your run is anything like mine, you see steady decrease in the loss, and trending of the confusion matrix to focus on the diagonal (i.e., correctly identifying things). There are persistent misclassifications, but this is not the most complext network you could construct here.\n",
    "\n",
    "An interesting thing happens - at least for me - if you try to add another convolutional layer to the network (mine had 20 features). Why?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
