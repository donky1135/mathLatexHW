\documentclass[12pt, letterpaper]{article}
\date{\today}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{pgfplots}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{amsthm,latexsym,amsfonts,graphicx,epsfig,comment}
\pgfplotsset{compat=1.16}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows.meta,arrows}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Po}{\mathcal{P}}

\author{Alex Valentino}
\title{Homework 6}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\fancyhf{}
\rhead{
	Homework 6\\
	350H	
}
\lhead{
	Alex Valentino\\
}
\begin{document}
	\begin{enumerate}
		\item Let $T: V \to W$ be a linear transformation from a finite-dimensional vector space $V$ to a finite-dimensional vector space $W$.  Let $
		\beta$ and $\beta'$ be ordered bases for $V$, and let $\gamma'$ and $\gamma$ be ordered bases for $W$.  We must show that $[T]^{\gamma'}_{\beta'} = P^{-1}[T]^\gamma_\beta Q$ where $Q$ is the $\beta'$ to $\beta$ change of coordinates matrix and $P$ is the $\gamma'$ to $\gamma$ change of coordinates matrix.  By definition $Q = [\mathbb{I}_V]^{\beta'}_\beta$, $P = [\mathbb{I}_W]^{\gamma}_{\gamma'}$.  Note that the statement we're trying to prove is equivalent to $P [T]^{\gamma'}_{\beta'} = [T]^\gamma_\beta Q$.  Therefore, 
		$$
					P [T]^{\gamma'}_{\beta'} = [\mathbb{I}_W]^{\gamma}_{\gamma'} [T]^{\gamma'}_{\beta'} = [\mathbb{I}_W T]^\gamma_{\beta'} = [T]^\gamma_{\beta'} = [T \mathbb{I}_V]^\gamma_{\beta'} = [T]^{\gamma}_\beta [\mathbb{I}_V]^{\beta}_{\beta'} = [T]^\gamma_\beta Q.
		$$  
		Therefore $[T]^{\gamma'}_{\beta'} = P^{-1}[T]^\gamma_\beta Q$. 
		
		\newpage
		\item Suppose $A,B \in M_{m \times n}(F), P \in GL_m (F), Q \in GL_n (F), B = P^{-1} A Q$.  We must show that there exists an $n$ dimensional vector space $V$ and an $m$ dimensional vector space $W$ over $F$, ordered bases $\beta$ and $\beta'$ for $V$ and $\gamma$ and $\gamma'$ for $W$, and a linear transformation $T: V \to W$ such that $$A = [T]^{\gamma}_\beta \text{ and } B = [T]^{\gamma'}_{\beta'}.$$
		Let $V = F^n, W= F^m, T = L_A$, and $\beta$ and $\gamma$ be the standard ordered bases for $F^n$ and $F^m$ respectively.  Therfore we trivially have $A = [L_A]^{\gamma}_{\beta} = [T]^{\gamma}_{\beta}$.  We must find $\beta', \gamma'$ such that $B = [T]_{\beta'}^{\gamma'}$.  
		
		
		\newpage
		\item Let $V$ be a finite-dimensional vector space with the ordered basis $\beta$.  Prove that $\psi(\beta) = \beta^{**}$.
			Let $\beta = \{x_1,\ldots,x_n\}$.  Therefore $\psi (\beta) = \{\hat{x}_1,\ldots,\hat{x}_n\}$.  By the corollary to theorem 2.26, there exists a basis $\{v_1,\ldots,v_n\}$ of $V^*$ such that $v^*_i = \hat{x}_i$.  Applying the corollary again gives us $\{x_1,\ldots,x_n\}$ as $\delta_{i,j} = \hat{x}_i (v_j) = v_j(x_i)$.  Therefore $x_i^{**} = \hat{x}_i$, giving us $\psi(\beta) = \beta^{**}$.
		\newpage
		
		\item Suppose that $V,W$ are finite dimensional vector spaces over $F$ and that $T: V \to W$ is linear.  We must show that $N(T^t) = (R(T))^0$.  Let $\{x_1,\ldots,x_n\}$ and $\{y_1,\ldots,y_m\}$ be ordered bases $\beta$ and $\gamma$ of $V$ and $W$ respectively, and let $\{f_1,\ldots,f_n\}$ and $\{g_1,\ldots,g_m\}$ be ordered bases $\beta^*$ and $\gamma^*$ of $V^*$ and $W^*$ respectively.    
		\begin{itemize}
			\item Suppose $v \in N(T^t)$.  We must show that $v \in (R(T))^0$.  By definition of being a member in $N(T^t)$, $v \in W^*, T^t (v) = v(T) = 0$.  By theorem 2.24 of $T^t (v) = \sum_{s = 1}^n (v T)(x_s) f_s$.  Therefore $0 = \sum_{s = 1}^n (v T)(x_s) f_s$.  Since $\beta^*$ is a basis for $V^*$, then for all $s \in [n], (v T)(x_s) = 0$.  Therefore for an arbitrary $x \in R(T)$, by the book proof of rank nullity $R(T)$ is spanned by $T(x_1),\ldots, T(x_n)$, thus there exists $a_1,\ldots, a_n$ such that  $x = a_1 T(x_1) + \ldots + a_n T(x_n)$.  Therefore $v(x) = v(\sum_{i=1}^n a_i T(x_i)) = \sum_{i=1}^n a_i v(T(x_i)) = \sum_{i=1}^n a_i 0 = 0$.  Thus $v \in (R(T))^0$.
			\item Suppose $v \in (R(T))^0 $.  We must show that $v \in N(T^t)$.  By definition of $v \in N(T^t)$ we must show that $T^t (v) = v(T) = 0$.  By definition of $v \in (R(T))^0$, for all $x \in R(T), v(x) = 0$.  By definition of being a member of $W^*$ by theorem 2.24, $v = \sum_{l = 1}^m v(y_l)g_l$.  
\end{itemize}		 
	\end{enumerate}
\end{document}